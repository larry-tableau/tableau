{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa8df49",
   "metadata": {},
   "source": [
    "BigQuery to Hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8717fb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -qqq install pandas pantab psutil google-cloud-bigquery google-auth\n",
    "%pip -qqq install ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73d31a0c-0764-41bf-947d-bda4e4b24942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pantab\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import logging\n",
    "import sys\n",
    "import psutil\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "import threading\n",
    "from functools import partial\n",
    "from IPython.display import clear_output\n",
    "import concurrent.futures\n",
    "\n",
    "class ProgressTracker:\n",
    "    def __init__(self, total_rows: int, chunk_size: int, update_interval: float = 0.5):\n",
    "        self.total_rows = total_rows\n",
    "        self.chunk_size = chunk_size\n",
    "        self.processed_rows = 0\n",
    "        self.start_time = time.time()\n",
    "        self.last_update = 0\n",
    "        self.update_interval = update_interval\n",
    "        self.chunks_processed = 0\n",
    "        \n",
    "    def _get_memory_usage(self) -> float:\n",
    "        process = psutil.Process()\n",
    "        return process.memory_info().rss / (1024 * 1024)\n",
    "        \n",
    "    def _format_bar(self, width: int = 50) -> str:\n",
    "        progress = min(1.0, self.processed_rows / self.total_rows)\n",
    "        filled = int(width * progress)\n",
    "        bar = '█' * filled + '░' * (width - filled)\n",
    "        return f'[{bar}]'\n",
    "    \n",
    "    def _format_time(self, seconds: float) -> str:\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        \n",
    "        if hours > 0:\n",
    "            return f\"{hours}h {minutes}m {secs}s\"\n",
    "        elif minutes > 0:\n",
    "            return f\"{minutes}m {secs}s\"\n",
    "        else:\n",
    "            return f\"{secs}s\"\n",
    "    \n",
    "    def _estimate_time_remaining(self, elapsed_time: float, progress: float) -> str:\n",
    "        if progress <= 0:\n",
    "            return \"calculating...\"\n",
    "        \n",
    "        total_estimated_time = elapsed_time / progress\n",
    "        remaining_time = total_estimated_time - elapsed_time\n",
    "        return self._format_time(remaining_time)\n",
    "    \n",
    "    def update(self, rows_processed: int):\n",
    "        self.processed_rows += rows_processed\n",
    "        self.chunks_processed += 1\n",
    "        current_time = time.time()\n",
    "        \n",
    "        if (current_time - self.last_update) >= self.update_interval:\n",
    "            elapsed_time = current_time - self.start_time\n",
    "            processing_speed = self.processed_rows / elapsed_time if elapsed_time > 0 else 0\n",
    "            progress = (self.processed_rows / self.total_rows)\n",
    "            memory_usage = self._get_memory_usage()\n",
    "            \n",
    "            if 'ipykernel' in sys.modules:\n",
    "                clear_output(wait=True)\n",
    "            \n",
    "            eta = self._estimate_time_remaining(elapsed_time, progress)\n",
    "            status = f\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════\n",
    "║ Progress: {self._format_bar()} {progress*100:.1f}%\n",
    "║ Chunks: {self.chunks_processed}/{(self.total_rows + self.chunk_size - 1) // self.chunk_size}\n",
    "║ Rows: {self.processed_rows:,}/{self.total_rows:,}\n",
    "║ Speed: {processing_speed:,.1f} rows/sec\n",
    "║ Memory: {memory_usage:.1f} MB\n",
    "║ Elapsed: {self._format_time(elapsed_time)}\n",
    "║ ETA: {eta}\n",
    "╚══════════════════════════════════════════════════════════════════════════════\"\"\"\n",
    "            print(status)\n",
    "            self.last_update = current_time\n",
    "\n",
    "    def finish(self):\n",
    "        import gc\n",
    "        total_time = time.time() - self.start_time\n",
    "        final_speed = self.processed_rows / total_time if total_time > 0 else 0\n",
    "        \n",
    "        # Memory usage statistics\n",
    "        memory_usage = self._get_memory_usage()\n",
    "        gc.collect()  # Explicitly invoke garbage collection\n",
    "        peak_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "        \n",
    "        if 'ipykernel' in sys.modules:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        status = f\"\"\"\n",
    "    ╔══════════════════════════════════════════════════════════════════════════════\n",
    "    ║ Processing Complete!\n",
    "    ║ Total Rows: {self.processed_rows:,}\n",
    "    ║ Total Time: {self._format_time(total_time)}\n",
    "    ║ Average Speed: {final_speed:,.1f} rows/sec\n",
    "    ║ Final Memory: {memory_usage:.1f} MB\n",
    "    ║ Peak Memory: {peak_memory:.1f} MB\n",
    "    ║ Garbage Collected: {gc.collect()} objects\n",
    "    ╚══════════════════════════════════════════════════════════════════════════════\"\"\"\n",
    "        print(status)\n",
    "\n",
    "class BigQueryToHyperETL:\n",
    "    def __init__(\n",
    "        self,\n",
    "        project_id: str,\n",
    "        source_project: str,\n",
    "        dataset_id: str,\n",
    "        table_id: str,\n",
    "        service_account_path: Optional[str] = None,\n",
    "        chunk_size: int = 100000,\n",
    "        cache_dir: str = \"./cache\",\n",
    "        cleanup_cache: bool = False\n",
    "    ):\n",
    "        self.project_id = project_id\n",
    "        self.source_project = source_project\n",
    "        self.dataset_id = dataset_id\n",
    "        self.table_id = table_id\n",
    "        self.service_account_path = service_account_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cleanup_cache = cleanup_cache\n",
    "        self.setup_logging()\n",
    "        self.schema = None\n",
    "        self.order_by_columns = None\n",
    "        \n",
    "        self.type_mapping = {\n",
    "            'STRING': 'string',\n",
    "            'INTEGER': 'Int32',\n",
    "            'FLOAT': 'float32',\n",
    "            'BOOLEAN': 'boolean',\n",
    "            'DATE': 'datetime64[ns]',\n",
    "            'DATETIME': 'datetime64[ns]',\n",
    "            'TIMESTAMP': 'datetime64[ns]',\n",
    "            'TIME': 'string',\n",
    "            'NUMERIC': 'float64',\n",
    "            'BIGNUMERIC': 'float64',\n",
    "            'BYTES': 'string',\n",
    "            'RECORD': 'string',\n",
    "        }\n",
    "        \n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def setup_logging(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        if not self.logger.handlers:\n",
    "            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "            \n",
    "            console_handler = logging.StreamHandler(sys.stdout)\n",
    "            console_handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(console_handler)\n",
    "            \n",
    "            file_handler = logging.FileHandler('etl_process.log')\n",
    "            file_handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(file_handler)\n",
    "\n",
    "    def get_cache_dir(self) -> Path:\n",
    "        cache_dir = self.cache_dir / f\"{self.source_project}_{self.dataset_id}_{self.table_id}\"\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        return cache_dir\n",
    "\n",
    "    def initialize_auth(self):\n",
    "        try:\n",
    "            if self.service_account_path:\n",
    "                self.logger.info(f\"Authenticating using service account: {self.service_account_path}\")\n",
    "                credentials = service_account.Credentials.from_service_account_file(\n",
    "                    self.service_account_path,\n",
    "                    scopes=['https://www.googleapis.com/auth/bigquery']\n",
    "                )\n",
    "                self.client = bigquery.Client(\n",
    "                    credentials=credentials,\n",
    "                    project=self.project_id\n",
    "                )\n",
    "            else:\n",
    "                self.logger.info(\"Attempting to use default credentials\")\n",
    "                self.client = bigquery.Client(project=self.project_id)\n",
    "            \n",
    "            self._verify_permissions()\n",
    "            self.logger.info(\"Successfully authenticated with Google Cloud\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Authentication failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _verify_permissions(self):\n",
    "        try:\n",
    "            test_query = f\"\"\"\n",
    "            SELECT 1\n",
    "            FROM `{self.source_project}.{self.dataset_id}.{self.table_id}`\n",
    "            LIMIT 1\n",
    "            \"\"\"\n",
    "            self.client.query(test_query).result()\n",
    "            self.logger.info(\"Successfully verified BigQuery permissions\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Permission verification failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_total_rows(self) -> int:\n",
    "        query = f\"\"\"\n",
    "        SELECT COUNT(*) as count\n",
    "        FROM `{self.source_project}.{self.dataset_id}.{self.table_id}`\n",
    "        \"\"\"\n",
    "        query_job = self.client.query(query)\n",
    "        rows = query_job.result()\n",
    "        return next(rows)['count']\n",
    "\n",
    "    def _get_table_schema(self):\n",
    "        \"\"\"Modified schema retrieval without focusing on ordering columns.\"\"\"\n",
    "        table_ref = f\"{self.source_project}.{self.dataset_id}.{self.table_id}\"\n",
    "        table = self.client.get_table(table_ref)\n",
    "        \n",
    "        self.schema = {}\n",
    "        for field in table.schema:\n",
    "            pandas_type = self.type_mapping.get(field.field_type, 'string')\n",
    "            self.schema[field.name] = {\n",
    "                'bq_type': field.field_type,\n",
    "                'pandas_type': pandas_type,\n",
    "                'nullable': field.is_nullable\n",
    "            }\n",
    "        \n",
    "        self.logger.info(f\"Table schema with types: {self.schema}\")\n",
    "\n",
    "    def convert_types(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        for col, type_info in self.schema.items():\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                pandas_type = type_info['pandas_type']\n",
    "                \n",
    "                if pandas_type == 'datetime64[ns]':\n",
    "                    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                elif pandas_type == 'Int64':\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int32')\n",
    "                elif pandas_type == 'float64':\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce').astype('float32')\n",
    "                elif pandas_type == 'boolean':\n",
    "                    df[col] = df[col].astype('boolean')\n",
    "                else:\n",
    "                    df[col] = df[col].astype(str)\n",
    "                \n",
    "                if df[col].isna().any():\n",
    "                    if pandas_type == 'string':\n",
    "                        df[col] = df[col].fillna('')\n",
    "                    elif pandas_type in ['Int32', 'float32']:\n",
    "                        df[col] = df[col].fillna(0)\n",
    "                    elif pandas_type == 'boolean':\n",
    "                        df[col] = df[col].fillna(False)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error converting column {col}: {str(e)}. Converting to string.\")\n",
    "                df[col] = df[col].astype(str)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def process_chunk(self, offset: int) -> pd.DataFrame:\n",
    "        \"\"\"Base chunk processing method without ordering.\"\"\"\n",
    "        try:\n",
    "            query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM `{self.source_project}.{self.dataset_id}.{self.table_id}`\n",
    "            LIMIT {self.chunk_size}\n",
    "            OFFSET {offset}\n",
    "            \"\"\"\n",
    "            \n",
    "            job_config = bigquery.QueryJobConfig(\n",
    "                priority=bigquery.QueryPriority.INTERACTIVE,\n",
    "                use_query_cache=True,\n",
    "                maximum_bytes_billed=1000000000000,\n",
    "                allow_large_results=True,\n",
    "                maximum_bytes_shuffled=None,\n",
    "                use_legacy_sql=False\n",
    "            )\n",
    "            \n",
    "            query_job = self.client.query(query, job_config=job_config)\n",
    "            results = query_job.result()\n",
    "            df = results.to_dataframe()\n",
    "            \n",
    "            return self.convert_types(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing chunk at offset {offset}: {str(e)}\")\n",
    "            if \"Resources exceeded\" in str(e):\n",
    "                original_chunk_size = self.chunk_size\n",
    "                self.chunk_size = max(1000, self.chunk_size // 2)\n",
    "                self.logger.warning(\n",
    "                    f\"Memory limit exceeded. Reducing chunk size from \"\n",
    "                    f\"{original_chunk_size:,} to {self.chunk_size:,}\"\n",
    "                )\n",
    "                return self.process_chunk(offset)\n",
    "            raise\n",
    "        gc.collect()\n",
    "\n",
    "class ParallelETL(BigQueryToHyperETL):\n",
    "    def __init__(\n",
    "        self,\n",
    "        project_id: str,\n",
    "        source_project: str,\n",
    "        dataset_id: str,\n",
    "        table_id: str,\n",
    "        service_account_path: Optional[str] = None,\n",
    "        chunk_size: int = 100000,\n",
    "        cache_dir: str = \"./cache\",\n",
    "        cleanup_cache: bool = False,\n",
    "        max_workers: int = None,\n",
    "        batch_size: int = 10\n",
    "    ):\n",
    "        super().__init__(\n",
    "            project_id=project_id,\n",
    "            source_project=source_project,\n",
    "            dataset_id=dataset_id,\n",
    "            table_id=table_id,\n",
    "            service_account_path=service_account_path,\n",
    "            chunk_size=chunk_size,\n",
    "            cache_dir=cache_dir,\n",
    "            cleanup_cache=cleanup_cache\n",
    "        )\n",
    "        self.max_workers = max_workers or multiprocessing.cpu_count()\n",
    "        self.batch_size = batch_size\n",
    "        self.thread_local = threading.local()\n",
    "\n",
    "    def get_client(self):\n",
    "        if not hasattr(self.thread_local, \"client\"):\n",
    "            if self.service_account_path:\n",
    "                credentials = service_account.Credentials.from_service_account_file(\n",
    "                    self.service_account_path,\n",
    "                    scopes=['https://www.googleapis.com/auth/bigquery']\n",
    "                )\n",
    "                self.thread_local.client = bigquery.Client(\n",
    "                    credentials=credentials,\n",
    "                    project=self.project_id\n",
    "                )\n",
    "            else:\n",
    "                self.thread_local.client = bigquery.Client(project=self.project_id)\n",
    "        return self.thread_local.client\n",
    "\n",
    "    def process_chunk_parallel(self, offset: int, batch_id: int) -> Tuple[bool, int]:\n",
    "        \"\"\"Process a chunk using parallel processing without ordering.\"\"\"\n",
    "        cache_dir = self.get_cache_dir()\n",
    "        cache_file = cache_dir / f\"chunk_{batch_id}_{offset}.parquet\"\n",
    "        \n",
    "        if cache_file.exists():\n",
    "            df = pd.read_parquet(cache_file)\n",
    "            return True, len(df)\n",
    "        \n",
    "        try:\n",
    "            client = self.get_client()\n",
    "            query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM `{self.source_project}.{self.dataset_id}.{self.table_id}`\n",
    "            LIMIT {self.chunk_size}\n",
    "            OFFSET {offset}\n",
    "            \"\"\"\n",
    "            \n",
    "            job_config = bigquery.QueryJobConfig(\n",
    "                priority=bigquery.QueryPriority.INTERACTIVE,\n",
    "                use_query_cache=True,\n",
    "                maximum_bytes_billed=1000000000000,\n",
    "                allow_large_results=True,\n",
    "                use_legacy_sql=False\n",
    "            )\n",
    "            \n",
    "            query_job = client.query(query, job_config=job_config)\n",
    "            df = query_job.result().to_dataframe()\n",
    "            \n",
    "            if not df.empty:\n",
    "                df = self.convert_types(df)\n",
    "                cache_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                df.to_parquet(\n",
    "                    cache_file,\n",
    "                    compression='snappy',\n",
    "                    engine='pyarrow'\n",
    "                )\n",
    "                return True, len(df)\n",
    "            return False, 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in batch {batch_id}, offset {offset}: {str(e)}\")\n",
    "            if \"Resources exceeded\" in str(e):\n",
    "                self.chunk_size = max(1000, self.chunk_size // 2)\n",
    "                self.logger.warning(f\"Reducing chunk size to {self.chunk_size}\")\n",
    "                return self.process_chunk_parallel(offset, batch_id)\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_chunk(file: Path) -> pd.DataFrame:\n",
    "        \"\"\"Static method to convert a parquet file to DataFrame.\"\"\"\n",
    "        return pd.read_parquet(file)\n",
    "\n",
    "    @staticmethod\n",
    "    def _write_hyper_batch(dfs: List[pd.DataFrame], output_file: str, mode: str):\n",
    "        \"\"\"Static method to write DataFrames to hyper file.\"\"\"\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        pantab.frame_to_hyper(\n",
    "            df=combined_df,\n",
    "            table='Extract',\n",
    "            database=output_file,\n",
    "            table_mode=mode,\n",
    "            atomic=False\n",
    "    )\n",
    "\n",
    "    def parallel_convert_to_hyper(self, output_file: str, cached_files: List[Path]):\n",
    "        \"\"\"Convert cached files to hyper format using parallel processing.\"\"\"\n",
    "        batch_size = min(len(cached_files), self.batch_size)\n",
    "        \n",
    "        try:\n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                for i in range(0, len(cached_files), batch_size):\n",
    "                    batch_files = cached_files[i:i + batch_size]\n",
    "                    \n",
    "                    # Read parquet files in parallel\n",
    "                    dfs = list(executor.map(self._convert_chunk, batch_files))\n",
    "                    \n",
    "                    # Write to hyper file\n",
    "                    mode = 'w' if i == 0 else 'a'\n",
    "                    self._write_hyper_batch(dfs, output_file, mode)\n",
    "                    \n",
    "                    del dfs\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    # Log progress\n",
    "                    self.logger.info(f\"Converted batch {i//batch_size + 1}/{(len(cached_files) + batch_size - 1)//batch_size}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during parallel conversion: {str(e)}\")\n",
    "            self.logger.info(\"Falling back to sequential processing...\")\n",
    "            \n",
    "            # Fallback to sequential processing\n",
    "            for i, file in enumerate(cached_files):\n",
    "                try:\n",
    "                    df = self._convert_chunk(file)\n",
    "                    mode = 'w' if i == 0 else 'a'\n",
    "                    self._write_hyper_batch([df], output_file, mode)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing file {file}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "    def execute(self, output_file: str):\n",
    "        \"\"\"Execute the ETL process with parallel processing.\"\"\"\n",
    "        try:\n",
    "            self.initialize_auth()\n",
    "            self._get_table_schema()\n",
    "            total_rows = self.get_total_rows()\n",
    "            \n",
    "            # Initialize progress tracker\n",
    "            progress = ProgressTracker(\n",
    "                total_rows=total_rows,\n",
    "                chunk_size=self.chunk_size\n",
    "            )\n",
    "            \n",
    "            cache_dir = self.get_cache_dir()\n",
    "            checkpoint_file = cache_dir / \"checkpoint.json\"\n",
    "            processed_rows = 0\n",
    "            \n",
    "            # Calculate batch offsets\n",
    "            offsets = list(range(0, total_rows, self.chunk_size))\n",
    "            batches = [\n",
    "                offsets[i:i + self.batch_size] \n",
    "                for i in range(0, len(offsets), self.batch_size)\n",
    "            ]\n",
    "            \n",
    "            self.logger.info(f\"Starting ETL process with {len(batches)} batches\")\n",
    "            self.logger.info(f\"Each batch will process up to {self.batch_size} chunks\")\n",
    "            self.logger.info(f\"Using {self.max_workers} workers for parallel processing\")\n",
    "            \n",
    "            # Phase 1: Download and cache chunks in parallel\n",
    "            failed_chunks = []\n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                for batch_id, batch_offsets in enumerate(batches):\n",
    "                    # Submit all chunks in the batch\n",
    "                    future_to_offset = {\n",
    "                        executor.submit(self.process_chunk_parallel, offset, batch_id): offset \n",
    "                        for offset in batch_offsets\n",
    "                    }\n",
    "                    \n",
    "                    # Process completed chunks\n",
    "                    for future in concurrent.futures.as_completed(future_to_offset):\n",
    "                        try:\n",
    "                            success, rows = future.result()\n",
    "                            if success:\n",
    "                                processed_rows += rows\n",
    "                                progress.update(rows)\n",
    "                                \n",
    "                                # Save checkpoint\n",
    "                                with open(checkpoint_file, 'w') as f:\n",
    "                                    json.dump({\n",
    "                                        'processed_rows': processed_rows,\n",
    "                                        'last_batch': batch_id\n",
    "                                    }, f)\n",
    "                            else:\n",
    "                                offset = future_to_offset[future]\n",
    "                                failed_chunks.append((batch_id, offset))\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            self.logger.error(f\"Error processing chunk: {str(e)}\")\n",
    "                            offset = future_to_offset[future]\n",
    "                            failed_chunks.append((batch_id, offset))\n",
    "                            continue\n",
    "            \n",
    "            if failed_chunks:\n",
    "                self.logger.warning(f\"Failed to process {len(failed_chunks)} chunks\")\n",
    "            \n",
    "            # Phase 2: Convert cached files to hyper format\n",
    "            self.logger.info(\"Converting cached files to hyper format...\")\n",
    "            cached_files = sorted(\n",
    "                cache_dir.glob(\"chunk_*.parquet\"),\n",
    "                key=lambda x: int(x.stem.split('_')[1])\n",
    "            )\n",
    "            \n",
    "            if not cached_files:\n",
    "                raise ValueError(f\"No cached files found in {cache_dir}\")\n",
    "            \n",
    "            self.logger.info(f\"Found {len(cached_files)} cached files to convert\")\n",
    "            self.parallel_convert_to_hyper(output_file, cached_files)\n",
    "            \n",
    "            progress.finish()\n",
    "            \n",
    "            # Cleanup if requested\n",
    "            if self.cleanup_cache:\n",
    "                self.logger.info(\"Cleaning up cache...\")\n",
    "                for file in cache_dir.glob(\"*\"):\n",
    "                    file.unlink()\n",
    "                cache_dir.rmdir()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Critical error in ETL process: {str(e)}\")\n",
    "            self.logger.error(f\"Cache directory contents: {list(cache_dir.glob('*'))}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46407253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔══════════════════════════════════════════════════════════╗\n",
      "║                ETL Configuration Summary                 ║\n",
      "╠══════════════════════════════════════════════════════════╣\n",
      "║ Memory Usage Estimation:                                 ║\n",
      "║   • Row Size Used:    611.27 bytes                       ║\n",
      "║   • Chunk Memory:     0.06 GB per chunk                  ║\n",
      "║   • Total Memory:     0.40 GB (1.3% RAM)                 ║\n",
      "║                                                          ║\n",
      "║ System Resources:                                        ║\n",
      "║   • Total Memory:     31.35 GB                           ║\n",
      "║   • Available CPUs:   16 cores                           ║\n",
      "║                                                          ║\n",
      "║ Table Statistics:                                        ║\n",
      "║   • Avg Row Size:     611.27 bytes                       ║\n",
      "║   • Method Used:      table metadata                     ║\n",
      "║                                                          ║\n",
      "║ Recommended Parameters:                                  ║\n",
      "║   • Chunk Size:       100,000 rows                       ║\n",
      "║   • Max Workers:      7 workers                          ║\n",
      "║   • Batch Size:       10 chunks per batch                ║\n",
      "╚══════════════════════════════════════════════════════════╝\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.api_core import exceptions\n",
    "import psutil\n",
    "import multiprocessing\n",
    "\n",
    "def get_system_resources(avg_row_size=None):\n",
    "    \"\"\"\n",
    "    Get system resources and calculate optimal ETL parameters.\n",
    "    \n",
    "    Args:\n",
    "        avg_row_size (float): Average row size in bytes from BigQuery table\n",
    "    \"\"\"\n",
    "    total_memory_gb = psutil.virtual_memory().total / (1024 ** 3)\n",
    "    available_cpus = multiprocessing.cpu_count()\n",
    "    \n",
    "    # Use actual row size if provided, otherwise use conservative estimate\n",
    "    row_size = avg_row_size if avg_row_size is not None else 300\n",
    "    \n",
    "    # Calculate optimal parameters with actual row size\n",
    "    optimal_chunk_size = int((total_memory_gb * 0.45 * 1024 * 1024 * 1024) / (row_size * 2))\n",
    "    chunk_size = min(100000, optimal_chunk_size)\n",
    "    \n",
    "    optimal_workers = min(\n",
    "        available_cpus - 2,\n",
    "        int(total_memory_gb / 4),\n",
    "        8\n",
    "    )\n",
    "    \n",
    "    optimal_batch_size = min(\n",
    "        max(1, int(optimal_workers * 1.5)),\n",
    "        12\n",
    "    )\n",
    "    \n",
    "    # Calculate actual memory usage for this configuration\n",
    "    estimated_memory_usage_gb = (chunk_size * row_size * optimal_workers) / (1024 * 1024 * 1024)\n",
    "    \n",
    "    return {\n",
    "        'total_memory_gb': total_memory_gb,\n",
    "        'available_cpus': available_cpus,\n",
    "        'chunk_size': chunk_size,\n",
    "        'max_workers': optimal_workers,\n",
    "        'batch_size': optimal_batch_size,\n",
    "        'row_size': row_size,\n",
    "        'estimated_memory_gb': estimated_memory_usage_gb\n",
    "    }\n",
    "\n",
    "\n",
    "def estimate_row_size(project_id, source_project, dataset_id, table_id, credentials_path=None):\n",
    "    \"\"\"\n",
    "    Estimates the average row size of a BigQuery table and suggests optimal ETL parameters.\n",
    "    \n",
    "    Args:\n",
    "        project_id (str): The GCP project ID where the query job will run.\n",
    "        source_project (str): The GCP project ID where the dataset and table reside.\n",
    "        dataset_id (str): The dataset ID containing the table.\n",
    "        table_id (str): The name of the table.\n",
    "        credentials_path (str, optional): Path to the service account JSON file for authentication.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (average row size in bytes, system resources dict)\n",
    "    \"\"\"\n",
    "    # Get system resources first\n",
    "    system_resources = get_system_resources()\n",
    "    \n",
    "    # Initialize the BigQuery client\n",
    "    try:\n",
    "        if credentials_path:\n",
    "            credentials = service_account.Credentials.from_service_account_file(\n",
    "                credentials_path, \n",
    "                scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "            )\n",
    "            client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "        else:\n",
    "            client = bigquery.Client(project=project_id)\n",
    "        \n",
    "        # Try table metadata first\n",
    "        try:\n",
    "            table_ref = client.dataset(dataset_id, project=source_project).table(table_id)\n",
    "            table = client.get_table(table_ref)\n",
    "            \n",
    "            if table.num_bytes is not None and table.num_rows is not None and table.num_rows > 0:\n",
    "                avg_row_size = table.num_bytes / table.num_rows\n",
    "                # Pass the actual row size to get_system_resources\n",
    "                system_resources = get_system_resources(avg_row_size)\n",
    "                print_configuration(avg_row_size, system_resources, \"table metadata\")\n",
    "                return avg_row_size, system_resources\n",
    "                \n",
    "        except exceptions.NotFound:\n",
    "            print(\"\\nTable metadata not directly accessible, trying alternative methods...\")\n",
    "        \n",
    "        # Try INFORMATION_SCHEMA.TABLES\n",
    "        size_query = f\"\"\"\n",
    "            SELECT\n",
    "                total_bytes,\n",
    "                row_count\n",
    "            FROM `{source_project}.{dataset_id}.INFORMATION_SCHEMA.TABLES`\n",
    "            WHERE table_name = '{table_id}'\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            query_job = client.query(size_query)\n",
    "            rows = list(query_job.result())\n",
    "            \n",
    "            if rows and len(rows) > 0:\n",
    "                total_bytes = rows[0].total_bytes or 0\n",
    "                row_count = rows[0].row_count or 1\n",
    "                \n",
    "                if row_count > 0:\n",
    "                    avg_row_size = total_bytes / row_count\n",
    "                    print_configuration(avg_row_size, system_resources, \"INFORMATION_SCHEMA\")\n",
    "                    return avg_row_size, system_resources\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError querying INFORMATION_SCHEMA.TABLES: {str(e)}\")\n",
    "        \n",
    "        # Final fallback method\n",
    "        print(\"\\nUsing fallback method with direct table query...\")\n",
    "        storage_query = f\"\"\"\n",
    "            SELECT APPROXIMATE_STORAGE_USAGE AS total_bytes\n",
    "            FROM `{source_project}.{dataset_id}.INFORMATION_SCHEMA.TABLE_STORAGE_STATS`\n",
    "            WHERE table_name = '{table_id}'\n",
    "        \"\"\"\n",
    "        count_query = f\"\"\"\n",
    "            SELECT COUNT(*) AS row_count \n",
    "            FROM `{source_project}.{dataset_id}.{table_id}`\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            storage_job = client.query(storage_query)\n",
    "            storage_result = list(storage_job.result())\n",
    "            total_bytes = storage_result[0].total_bytes if storage_result else 0\n",
    "            \n",
    "            count_job = client.query(count_query)\n",
    "            count_result = list(count_job.result())\n",
    "            row_count = count_result[0].row_count if count_result else 1\n",
    "            \n",
    "            if row_count > 0:\n",
    "                avg_row_size = total_bytes / row_count\n",
    "                print_configuration(avg_row_size, system_resources, \"fallback method\")\n",
    "                return avg_row_size, system_resources\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in fallback method: {str(e)}\")\n",
    "        \n",
    "        return None, system_resources\n",
    "        \n",
    "    except exceptions.Forbidden as e:\n",
    "        print(f\"\\nPermission error: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred: {str(e)}\")\n",
    "    return None, system_resources\n",
    "\n",
    "def print_configuration(avg_row_size, resources, method):\n",
    "    \"\"\"Print a formatted configuration summary.\"\"\"\n",
    "    width = 60\n",
    "    \n",
    "    def center_text(text, width=width):\n",
    "        return f\"║ {text.center(width-4)} ║\"\n",
    "    \n",
    "    def left_text(text, width=width):\n",
    "        return f\"║ {text:<{width-4}} ║\"\n",
    "    \n",
    "    # Calculate memory estimations\n",
    "    chunk_memory = (resources['chunk_size'] * resources['row_size']) / (1024 * 1024 * 1024)\n",
    "    total_memory_usage = resources['estimated_memory_gb']\n",
    "    memory_percentage = (total_memory_usage/resources['total_memory_gb']*100)\n",
    "    \n",
    "    # Single unified output\n",
    "    print(\"╔\" + \"═\" * (width-2) + \"╗\")\n",
    "    print(center_text(\"ETL Configuration Summary\"))\n",
    "    print(\"╠\" + \"═\" * (width-2) + \"╣\")\n",
    "    \n",
    "    # Memory Usage Estimation\n",
    "    print(left_text(\"Memory Usage Estimation:\"))\n",
    "    print(left_text(f\"  • Row Size Used:    {resources['row_size']:.2f} bytes\"))\n",
    "    print(left_text(f\"  • Chunk Memory:     {chunk_memory:.2f} GB per chunk\"))\n",
    "    print(left_text(f\"  • Total Memory:     {total_memory_usage:.2f} GB ({memory_percentage:.1f}% RAM)\"))\n",
    "    print(left_text(\"\"))\n",
    "    \n",
    "    # System Resources\n",
    "    print(left_text(\"System Resources:\"))\n",
    "    print(left_text(f\"  • Total Memory:     {resources['total_memory_gb']:.2f} GB\"))\n",
    "    print(left_text(f\"  • Available CPUs:   {resources['available_cpus']} cores\"))\n",
    "    print(left_text(\"\"))\n",
    "    \n",
    "    # Table Statistics\n",
    "    print(left_text(\"Table Statistics:\"))\n",
    "    print(left_text(f\"  • Avg Row Size:     {avg_row_size:.2f} bytes\"))\n",
    "    print(left_text(f\"  • Method Used:      {method}\"))\n",
    "    print(left_text(\"\"))\n",
    "    \n",
    "    # Recommended Parameters\n",
    "    print(left_text(\"Recommended Parameters:\"))\n",
    "    print(left_text(f\"  • Chunk Size:       {resources['chunk_size']:,} rows\"))\n",
    "    print(left_text(f\"  • Max Workers:      {resources['max_workers']} workers\"))\n",
    "    print(left_text(f\"  • Batch Size:       {resources['batch_size']} chunks per batch\"))\n",
    "    \n",
    "    print(\"╚\" + \"═\" * (width-2) + \"╝\")\n",
    "\n",
    "# Example usage\n",
    "avg_row_size, resources = estimate_row_size(\n",
    "    project_id='your_project_id',\n",
    "    source_project='bigquery-public-data',\n",
    "    dataset_id='samples',\n",
    "    table_id='github_timeline',\n",
    "    credentials_path='/path/to/credentials.json'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b2ae047-e835-4b2a-b9d2-950efbad287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ╔══════════════════════════════════════════════════════════════════════════════\n",
      "    ║ Processing Complete!\n",
      "    ║ Total Rows: 6,219,749\n",
      "    ║ Total Time: 14m 51s\n",
      "    ║ Average Speed: 6,974.5 rows/sec\n",
      "    ║ Final Memory: 6593.0 MB\n",
      "    ║ Peak Memory: 6592.0 MB\n",
      "    ║ Garbage Collected: 0 objects\n",
      "    ╚══════════════════════════════════════════════════════════════════════════════\n"
     ]
    }
   ],
   "source": [
    "# Initialize the parallel ETL process\n",
    "etl = ParallelETL(\n",
    "    project_id='your_project_id',\n",
    "    source_project='bigquery-public-data',\n",
    "    dataset_id='samples',\n",
    "    table_id='github_timeline',\n",
    "    service_account_path='/path/to/credentials.json',\n",
    "    chunk_size=100000,  # Start with a moderate chunk size\n",
    "    cache_dir=\"./cache\",\n",
    "    max_workers=6,\n",
    "    batch_size=6,  # Process x chunks at a time\n",
    "    cleanup_cache=False  # Keep cache files for debugging\n",
    ")\n",
    "\n",
    "# Execute with proper error handling\n",
    "try:\n",
    "    etl.execute('github_timeline_v2.hyper')\n",
    "except Exception as e:\n",
    "    print(f\"Error during execution: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
