{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN59OJTMXHkLFNLG2FR4eHJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larry-tableau/tableau/blob/main/Querying_Tableau's_Metadata_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Example GraphQL Query using Python to query Tableau's Metadata API\n",
        "\n",
        "---\n",
        "\n",
        "This document discusses the benefits of implementing certain calculations in Tableau to compliment BigQuery/DBT, emphasizing a balanced approach to data analysis, reporting, visual creativity and impactful story telling.\n",
        "\n",
        "Key points include:\n",
        "\n",
        "1.   Enhanced user accessibility and flexibility for self-service analytics\n",
        "2.   Improved visibility and governance through certified metrics and data sources\n",
        "3.   Flexibility by leveraging Tableau Extracts vs Live Connections\n",
        "4.   Context-aware calculations for dynamic, interactive visualizations\n",
        "5.   Managing responsibilities between data engineering and business logic vs use cases\n",
        "6.   Potential cost savings in development and maintenance\n",
        "\n",
        "The following Python code will demonstrate how to integrate BigQuery and Tableau-like functionalities, showcasing the synergy between backend data processing and frontend visualization capabilities.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "knIDu4vIsR_e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T234DeyxsQm5"
      },
      "outputs": [],
      "source": [
        "!pip install requests\n",
        "!pip install pantab==4.1.0\n",
        "!pip install tableauserverclient"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Tableau Cloud URL and authentication details\n",
        "tableau_cloud_url = 'https://prod-apsoutheast-a.online.tableau.com'\n",
        "pat_name = userdata.get('PAT_NAME')\n",
        "pat_secret = userdata.get('PAT_SECRET')\n",
        "site_content_url = 'tableauanzpresalesdemositesydney'  # e.g., 'my_site'"
      ],
      "metadata": {
        "id": "cdjrH0lBueXO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import pantab\n",
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "from google.colab import data_table\n",
        "\n",
        "# Authenticate with Google Cloud\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Constants\n",
        "AUTH_URL = f\"{tableau_cloud_url}/api/3.22/auth/signin\"\n",
        "GRAPHQL_URL = f\"{tableau_cloud_url}/api/metadata/graphql\"\n",
        "QUERY = \"\"\"\n",
        "query published_datasources {\n",
        "  publishedDatasources(filter: {projectNameWithin: [\"Data Sources (LD)\",\"Larry\"]})  {\n",
        "    id\n",
        "    luid\n",
        "    name\n",
        "    hasUserReference\n",
        "    hasExtracts\n",
        "    extractLastRefreshTime\n",
        "    site {\n",
        "      luid\n",
        "    }\n",
        "    fields {\n",
        "      name\n",
        "      ... on CalculatedField {\n",
        "        formula\n",
        "      }\n",
        "    }\n",
        "    projectName\n",
        "    projectVizportalUrlId\n",
        "    owner {\n",
        "      luid\n",
        "    }\n",
        "    isCertified\n",
        "    certifier {\n",
        "      luid\n",
        "    }\n",
        "    certificationNote\n",
        "    certifierDisplayName\n",
        "    description\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def get_auth_token(pat_name, pat_secret, site_content_url):\n",
        "    \"\"\"Authenticate with Tableau Server and return an auth token.\"\"\"\n",
        "    headers = {'Content-Type': 'application/json', 'Accept': 'application/json'}\n",
        "    payload = {\n",
        "        \"credentials\": {\n",
        "            \"personalAccessTokenName\": pat_name,\n",
        "            \"personalAccessTokenSecret\": pat_secret,\n",
        "            \"site\": {\"contentUrl\": site_content_url}\n",
        "        }\n",
        "    }\n",
        "    response = requests.post(AUTH_URL, headers=headers, json=payload)\n",
        "    response.raise_for_status()  # Will raise an HTTPError for bad responses\n",
        "    return response.json()['credentials']['token']\n",
        "\n",
        "def query_metadata_api(token):\n",
        "    \"\"\"Query Tableau Metadata API and return the result.\"\"\"\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json',\n",
        "        'Accept': 'application/json',\n",
        "        'X-Tableau-Auth': token\n",
        "    }\n",
        "    payload = {\"query\": QUERY}\n",
        "    response = requests.post(GRAPHQL_URL, headers=headers, json=payload)\n",
        "    try:\n",
        "        response.raise_for_status()\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"HTTP Error: {e}\")\n",
        "        print(f\"Response content: {response.content}\")\n",
        "        raise\n",
        "\n",
        "    result = response.json()\n",
        "\n",
        "    if 'errors' in result:\n",
        "        errors = \"\\n\".join(\n",
        "            f\"Error: {error.get('message')}, Path: {error.get('path')}, \"\n",
        "            f\"Classification: {error.get('extensions', {}).get('classification')}\"\n",
        "            for error in result['errors']\n",
        "        )\n",
        "        raise Exception(f\"GraphQL query returned errors:\\n{errors}\")\n",
        "\n",
        "    if result.get('data') is None:\n",
        "        print(f\"Full API response: {result}\")\n",
        "        raise Exception(\"No data returned from GraphQL API.\")\n",
        "\n",
        "    return result['data']\n",
        "\n",
        "def flatten_json(nested_json):\n",
        "    \"\"\"Flatten the nested JSON structure from the API response.\"\"\"\n",
        "    flattened_data = []\n",
        "\n",
        "    if nested_json is None:\n",
        "        print(\"Error: nested_json is None\")\n",
        "        return flattened_data\n",
        "\n",
        "    published_datasources = nested_json.get('publishedDatasources')\n",
        "\n",
        "    if published_datasources is None:\n",
        "        print(\"Error: 'publishedDatasources' key not found in the response\")\n",
        "        print(f\"Keys in nested_json: {nested_json.keys()}\")\n",
        "        return flattened_data\n",
        "\n",
        "    if not published_datasources:\n",
        "        print(\"No published datasources found in the response.\")\n",
        "        return flattened_data\n",
        "\n",
        "    for datasource in published_datasources:\n",
        "        base_datasource = {}\n",
        "        for key, value in datasource.items():\n",
        "            if isinstance(value, dict):\n",
        "                # Handle nested dictionaries\n",
        "                for nested_key, nested_value in value.items():\n",
        "                    base_datasource[f\"{key}_{nested_key}\"] = nested_value\n",
        "            elif not isinstance(value, list):\n",
        "                # Directly add non-list values\n",
        "                base_datasource[key] = value\n",
        "\n",
        "        # Handle fields separately\n",
        "        fields = datasource.get('fields', [])\n",
        "        has_calculated_fields = False\n",
        "        for field in fields:\n",
        "            if isinstance(field, dict) and 'formula' in field:\n",
        "                has_calculated_fields = True\n",
        "                flattened_data.append({\n",
        "                    **base_datasource,\n",
        "                    'field_name': field.get('name'),\n",
        "                    'formula': field.get('formula')\n",
        "                })\n",
        "\n",
        "        if not has_calculated_fields:\n",
        "            flattened_data.append({\n",
        "                **base_datasource,\n",
        "                'field_name': None,\n",
        "                'formula': None\n",
        "            })\n",
        "\n",
        "    return flattened_data\n",
        "\n",
        "def display_dataframe(df, title):\n",
        "    \"\"\"Display a DataFrame as an HTML table.\"\"\"\n",
        "    print(f\"\\n{title}:\")\n",
        "    display(data_table.DataTable(df, include_index=False))\n",
        "\n",
        "def write_to_bigquery(df, project_id, dataset_id, table_id):\n",
        "    \"\"\"Write the DataFrame to BigQuery.\"\"\"\n",
        "    client = bigquery.Client(project=project_id)\n",
        "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        autodetect=True,\n",
        "        write_disposition=\"WRITE_TRUNCATE\",\n",
        "    )\n",
        "\n",
        "    job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
        "    job.result()  # Wait for the job to complete\n",
        "\n",
        "    print(f\"Loaded {len(df)} rows into {table_ref}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution flow.\"\"\"\n",
        "    try:\n",
        "        # Authenticate and get token\n",
        "        auth_token = get_auth_token(pat_name, pat_secret, site_content_url)\n",
        "\n",
        "        # Query the Metadata API\n",
        "        result = query_metadata_api(auth_token)\n",
        "        print(f\"Raw API result: {result}\")  # Add this line\n",
        "\n",
        "        # Flatten the JSON structure\n",
        "        flattened_data = flatten_json(result)\n",
        "\n",
        "        if not flattened_data:\n",
        "            print(\"No data to process. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Create and display DataFrame\n",
        "        df = pd.DataFrame(flattened_data)\n",
        "\n",
        "        # Convert problematic columns to strings\n",
        "        problematic_columns = ['certifier', 'certificationNote', 'certifierDisplayName']\n",
        "        for col in problematic_columns:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].astype(str)\n",
        "\n",
        "        display_dataframe(df, \"Flattened Datasources and Calculated Fields\")\n",
        "\n",
        "        # Save to CSV\n",
        "        df.to_csv('tableau_metadata_output.csv', index=False)\n",
        "        print(\"\\nData saved to 'tableau_metadata_output.csv'\")\n",
        "\n",
        "        # Write to BigQuery\n",
        "        project_id = '<Google_Cloud_Project_ID>'  # Replace with your Google Cloud project ID\n",
        "        dataset_id = '<Google_BQ_Dataset_ID>'  # Replace with your BigQuery dataset ID\n",
        "        table_id = '<Google_BQ_Table_ID'  # Replace with your desired table name\n",
        "\n",
        "        # write_to_bigquery(df, project_id, dataset_id, table_id)\n",
        "\n",
        "        # Save as Hyper file\n",
        "        pantab.frame_to_hyper(df, '/content/tableau_metadata_output.hyper', table='Extract')\n",
        "        print(\"\\nData saved to 'tableau_metadata_output.hyper'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "# Execute the main function\n",
        "main()\n"
      ],
      "metadata": {
        "id": "RBexfTrLJGL9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}