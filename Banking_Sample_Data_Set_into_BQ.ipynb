{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxC9tbbtvJyqG4dicIuDeQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larry-tableau/tableau/blob/main/Banking_Sample_Data_Set_into_BQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aYrjOTNZgakT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pantab==4.1.0\n",
        "!pip --quiet install pantab\n",
        "!pip --quiet install tableauhyperapi\n",
        "!pip --quiet install pandas faker\n",
        "!pip --quiet install --upgrade Faker\n",
        "!pip --quiet install Mimesis\n",
        "!pip --quiet install --upgrade numpy\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import pantab\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJyhDGv6gk8o",
        "outputId": "127dff09-2de9-44b7-e501-2aa8a79e5242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NBdupXJ9gZuO",
        "outputId": "f57bc6d2-f117-47ad-bd6b-0d521caa5c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "BENDIGO AND ADELAIDE BANK PORTFOLIO ANALYTICS DATASET\n",
            "Google Colab Version\n",
            "================================================================================\n",
            "Setting up Google Colab authentication...\n",
            "🔑 Attempting user authentication...\n",
            "✅ User authentication successful!\n",
            "Generating 150,000 rows of Bendigo and Adelaide Bank portfolio data...\n",
            "Generating date fields...\n",
            "Generating customer identification...\n",
            "Generating customer names...\n",
            "Generating address information...\n",
            "Generating relationship manager data...\n",
            "Generating account information...\n",
            "Generating product hierarchy...\n",
            "Generating financial measures...\n",
            "Generating transaction data...\n",
            "Generating officer and management data...\n",
            "Generating KPI measures...\n",
            "Creating DataFrame...\n",
            "Applying outliers to recent data...\n",
            "Dataset generation complete! Generated 150,000 rows with 96 columns.\n",
            "Date range: 2022-01-29 to 2025-12-31\n",
            "Outliers applied to 218 recent records (0.1%)\n",
            "\n",
            "================================================================================\n",
            "DATASET SUMMARY\n",
            "================================================================================\n",
            "Total Rows: 150,000\n",
            "Total Columns: 96\n",
            "Memory Usage: 762.2 MB\n",
            "\n",
            "Column Information:\n",
            "--------------------------------------------------\n",
            " 1. evaluate_date                       object          (0 nulls)\n",
            " 2. customer_id                         object          (0 nulls)\n",
            " 3. relation_type                       object          (0 nulls)\n",
            " 4. base_customer                       object          (0 nulls)\n",
            " 5. date_added                          object          (0 nulls)\n",
            " 6. delete_flag                         bool            (0 nulls)\n",
            " 7. cust_type                           object          (0 nulls)\n",
            " 8. cust_name                           object          (0 nulls)\n",
            " 9. address                             object          (0 nulls)\n",
            "10. city                                object          (0 nulls)\n",
            "11. state                               object          (0 nulls)\n",
            "12. postcode                            object          (0 nulls)\n",
            "13. branch                              object          (0 nulls)\n",
            "14. operating_branch                    object          (0 nulls)\n",
            "15. salutation                          object          (0 nulls)\n",
            "16. class_type                          object          (0 nulls)\n",
            "17. sic_code                            object          (105,058 nulls)\n",
            "18. customer_category                   object          (0 nulls)\n",
            "19. charge_officer1                     object          (0 nulls)\n",
            "20. account_id                          object          (0 nulls)\n",
            "21. institution                         object          (0 nulls)\n",
            "22. folio                               object          (0 nulls)\n",
            "23. ledger                              object          (0 nulls)\n",
            "24. account                             object          (0 nulls)\n",
            "25. bank_code                           object          (0 nulls)\n",
            "26. account_status                      object          (0 nulls)\n",
            "27. product                             object          (0 nulls)\n",
            "28. prod_lvl1                           object          (0 nulls)\n",
            "29. prod_lvl2                           object          (0 nulls)\n",
            "30. prod_lvl3                           object          (0 nulls)\n",
            "31. prod_lvl4                           object          (0 nulls)\n",
            "32. prod_grp                            object          (0 nulls)\n",
            "33. type                                object          (0 nulls)\n",
            "34. bgod                                object          (0 nulls)\n",
            "35. custacc                             object          (0 nulls)\n",
            "36. oic_report                          object          (0 nulls)\n",
            "37. business_date                       object          (0 nulls)\n",
            "38. footing_type                        object          (0 nulls)\n",
            "39. balance                             float64         (0 nulls)\n",
            "40. footing                             float64         (0 nulls)\n",
            "41. current_limit                       float64         (0 nulls)\n",
            "42. maturity_date                       object          (120,569 nulls)\n",
            "43. tran_type                           object          (0 nulls)\n",
            "44. tran_method                         object          (0 nulls)\n",
            "45. transaction_description             object          (0 nulls)\n",
            "46. actual_date                         object          (0 nulls)\n",
            "47. tran_eff_date                       object          (0 nulls)\n",
            "48. tran_amount                         float64         (0 nulls)\n",
            "49. amount                              float64         (0 nulls)\n",
            "50. dr_cr                               object          (0 nulls)\n",
            "51. terminal_type                       object          (0 nulls)\n",
            "52. card_bin_no                         object          (0 nulls)\n",
            "53. reference_number                    object          (0 nulls)\n",
            "54. reference_data                      object          (0 nulls)\n",
            "55. reference                           object          (0 nulls)\n",
            "56. oic1_new                            object          (0 nulls)\n",
            "57. name_new                            object          (0 nulls)\n",
            "58. oic_name                            object          (0 nulls)\n",
            "59. oic1                                object          (0 nulls)\n",
            "60. name                                object          (0 nulls)\n",
            "61. adm_new                             object          (0 nulls)\n",
            "62. adm                                 object          (0 nulls)\n",
            "63. gl_util_new                         object          (0 nulls)\n",
            "64. title                               object          (0 nulls)\n",
            "65. gl_cost_code                        object          (0 nulls)\n",
            "66. gl_utility_code                     object          (0 nulls)\n",
            "67. effective_from                      object          (0 nulls)\n",
            "68. effective_to                        object          (120,017 nulls)\n",
            "69. gl_ml1_id                           object          (0 nulls)\n",
            "70. gl_ml1_name                         object          (0 nulls)\n",
            "71. gl_ml2_id                           object          (0 nulls)\n",
            "72. gl_ml2_name                         object          (0 nulls)\n",
            "73. gl_ml3_id                           object          (0 nulls)\n",
            "74. gl_ml3_name                         object          (0 nulls)\n",
            "75. gl_ml4_id                           object          (0 nulls)\n",
            "76. gl_ml4_name                         object          (0 nulls)\n",
            "77. gl_ml5_id                           object          (0 nulls)\n",
            "78. gl_ml5_name                         object          (0 nulls)\n",
            "79. rel_mgt_class                       object          (0 nulls)\n",
            "80. direct_report_adm                   object          (0 nulls)\n",
            "81. direct_report_name                  object          (0 nulls)\n",
            "82. direct_report_title                 object          (0 nulls)\n",
            "83. direct_report_adm_overwrite         object          (0 nulls)\n",
            "84. curr_flag                           bool            (0 nulls)\n",
            "85. oic_and_name                        object          (0 nulls)\n",
            "86. event_timestamp_utc                 datetime64[ns]  (0 nulls)\n",
            "87. avg_products_per_customer           float64         (0 nulls)\n",
            "88. proactive_opportunity_win_rate      float64         (0 nulls)\n",
            "89. key_product_portfolio_growth        float64         (0 nulls)\n",
            "90. competitor_win_back_rate            float64         (0 nulls)\n",
            "91. opportunity_source                  object          (0 nulls)\n",
            "92. displaced_competitor                object          (0 nulls)\n",
            "93. strategic_product_group             object          (0 nulls)\n",
            "94. time_granularity                    object          (0 nulls)\n",
            "95. reporting_period_start              object          (0 nulls)\n",
            "96. reporting_period_end                object          (0 nulls)\n",
            "\n",
            "Sample Data (first 5 rows):\n",
            "--------------------------------------------------\n",
            "  evaluate_date customer_id relation_type base_customer  date_added  \\\n",
            "0    2022-07-28  CUST000001       Primary             Y  2024-09-21   \n",
            "1    2023-10-21  CUST000002         Joint             Y  2020-12-14   \n",
            "2    2022-11-30  CUST000003       Primary             Y  2023-01-04   \n",
            "3    2022-12-02  CUST000004       Primary             Y  2024-03-05   \n",
            "4    2022-08-11  CUST000005       Primary             Y  2020-08-29   \n",
            "\n",
            "   delete_flag   cust_type                      cust_name  \\\n",
            "0        False    Business         Quality Holdings Trust   \n",
            "1        False  Individual                     Peter Hall   \n",
            "2        False  Individual                   James Carter   \n",
            "3        False  Individual                   Sharon Perez   \n",
            "4        False    Business  Elite Development Pty Limited   \n",
            "\n",
            "                address            city  ... avg_products_per_customer  \\\n",
            "0    161 Flinders Place       Melbourne  ...                      3.46   \n",
            "1   406 La Trobe Avenue       Melbourne  ...                      2.63   \n",
            "2       297 King Avenue  Port Macquarie  ...                      1.48   \n",
            "3  842 A'Beckett Street          Sydney  ...                      1.65   \n",
            "4     691 William Drive       Melbourne  ...                      2.14   \n",
            "\n",
            "  proactive_opportunity_win_rate key_product_portfolio_growth  \\\n",
            "0                          85.52                          4.0   \n",
            "1                          66.76                          8.0   \n",
            "2                          52.05                         13.0   \n",
            "3                          71.64                         15.0   \n",
            "4                          77.12                         10.0   \n",
            "\n",
            "  competitor_win_back_rate opportunity_source displaced_competitor  \\\n",
            "0                    28.78    LinkedIn Signal              Suncorp   \n",
            "1                     8.40    LinkedIn Signal                  NAB   \n",
            "2                    19.39    Analytics Alert              Westpac   \n",
            "3                    22.54   Customer Inquiry                  ING   \n",
            "4                    29.21    Analytics Alert       Macquarie Bank   \n",
            "\n",
            "  strategic_product_group time_granularity reporting_period_start  \\\n",
            "0          Business Loans          Monthly             2022-07-01   \n",
            "1     Transaction Banking          Monthly             2023-10-01   \n",
            "2              Home Loans            Daily             2022-12-04   \n",
            "3    Agribusiness Finance          Monthly             2022-11-01   \n",
            "4          Business Loans            Daily             2022-08-08   \n",
            "\n",
            "  reporting_period_end  \n",
            "0           2022-07-31  \n",
            "1           2023-10-31  \n",
            "2           2022-12-04  \n",
            "3           2022-11-30  \n",
            "4           2022-08-08  \n",
            "\n",
            "[5 rows x 96 columns]\n",
            "\n",
            "Financial Measures Summary:\n",
            "--------------------------------------------------\n",
            "            balance       footing  current_limit    tran_amount\n",
            "count  1.500000e+05  1.500000e+05  150000.000000  150000.000000\n",
            "mean   1.520693e+04  1.514912e+04  167859.954111    1008.606826\n",
            "std    4.239597e+04  4.241296e+04  288868.748095     753.954656\n",
            "min    6.250000e+00  7.090000e+00       0.000000       0.540000\n",
            "25%    1.794335e+03  1.774547e+03       0.000000     480.760000\n",
            "50%    4.932740e+03  4.887060e+03       0.000000     838.630000\n",
            "75%    1.367203e+04  1.357701e+04  257422.292500    1348.935000\n",
            "max    3.783823e+06  3.136977e+06  999987.990000   31836.800000\n",
            "\n",
            "================================================================================\n",
            "BIGQUERY UPLOAD\n",
            "================================================================================\n",
            "Setting up BigQuery client for project: ehc-ldioneda-1d3210\n",
            "✅ Successfully connected to dataset: larry_demo_datasets\n",
            "Creating table ehc-ldioneda-1d3210.larry_demo_datasets.bb_banking_dataset_sample...\n",
            "Table ehc-ldioneda-1d3210.larry_demo_datasets.bb_banking_dataset_sample created successfully.\n",
            "Starting upload to ehc-ldioneda-1d3210.larry_demo_datasets.bb_banking_dataset_sample...\n",
            "Upload mode: WRITE_TRUNCATE\n",
            "Rows to upload: 150,000\n",
            "✅ Successfully uploaded 150,000 rows to ehc-ldioneda-1d3210.larry_demo_datasets.bb_banking_dataset_sample\n",
            "📊 Table now contains 150,000 total rows\n",
            "\n",
            "✅ SUCCESS! Data uploaded to BigQuery\n",
            "📊 Table: ehc-ldioneda-1d3210.larry_demo_datasets.bb_banking_dataset_sample\n",
            "📈 Rows: 150,000\n",
            "📋 Columns: 96\n",
            "\n",
            "💾 Creating CSV backup...\n",
            "\n",
            "📁 Saving CSV backup: bendigo_adelaide_bank_portfolio_analytics_150k.csv\n",
            "✅ CSV saved successfully: bendigo_adelaide_bank_portfolio_analytics_150k.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_48628a71-addb-46c9-9c70-a9dac383bace\", \"bendigo_adelaide_bank_portfolio_analytics_150k.csv\", 135733702)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 File download initiated in Colab\n",
            "\n",
            "================================================================================\n",
            "PROCESSING COMPLETE\n",
            "Ready for Analytics and Executive Reporting\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime, timedelta, date\n",
        "import string\n",
        "import warnings\n",
        "import os\n",
        "import json\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Configuration parameters - easily adjustable\n",
        "NUM_ROWS = 150000\n",
        "START_DATE = datetime(2022, 6, 7)  # 3 years before reference date\n",
        "END_DATE = datetime(2025, 6, 7)    # Reference date\n",
        "OUTLIER_FRACTION = 0.02  # 2% of records will have outliers\n",
        "OUTLIER_MAGNITUDE = 5.0  # Outliers will be 5x normal values\n",
        "\n",
        "# BigQuery configuration\n",
        "PROJECT_ID = \"ehc-ldioneda-1d3210\"\n",
        "DATASET_ID = \"larry_demo_datasets\"\n",
        "TABLE_ID = \"bb_banking_dataset_sample\"\n",
        "FULL_TABLE_ID = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
        "\n",
        "def setup_colab_authentication():\n",
        "    \"\"\"\n",
        "    Setup authentication in Google Colab environment\n",
        "    \"\"\"\n",
        "    print(\"Setting up Google Colab authentication...\")\n",
        "\n",
        "    try:\n",
        "        # Method 1: Try user authentication first\n",
        "        from google.colab import auth\n",
        "        print(\"🔑 Attempting user authentication...\")\n",
        "        auth.authenticate_user()\n",
        "        print(\"✅ User authentication successful!\")\n",
        "        return \"user\"\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  User authentication failed: {e}\")\n",
        "\n",
        "    # Method 2: Service account upload\n",
        "    print(\"\\n📁 Please upload your service account JSON file using the method below:\")\n",
        "    print(\"1. Click the 'Choose Files' button that appears\")\n",
        "    print(\"2. Select your service account JSON file\")\n",
        "    print(\"3. Wait for upload to complete\")\n",
        "\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        if uploaded:\n",
        "            # Get the first uploaded file\n",
        "            filename = list(uploaded.keys())[0]\n",
        "            print(f\"📄 Uploaded file: {filename}\")\n",
        "\n",
        "            # Set environment variable\n",
        "            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = filename\n",
        "            print(\"✅ Service account credentials configured!\")\n",
        "            return \"service_account\"\n",
        "        else:\n",
        "            print(\"❌ No file uploaded\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ File upload failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def setup_bigquery_client():\n",
        "    \"\"\"\n",
        "    Setup BigQuery client for Google Colab\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from google.cloud import bigquery\n",
        "        from google.cloud.exceptions import NotFound\n",
        "        from google.auth.exceptions import DefaultCredentialsError\n",
        "\n",
        "        print(f\"Setting up BigQuery client for project: {PROJECT_ID}\")\n",
        "\n",
        "        # Try to create client\n",
        "        client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "        # Test the connection\n",
        "        try:\n",
        "            dataset = client.get_dataset(DATASET_ID)\n",
        "            print(f\"✅ Successfully connected to dataset: {DATASET_ID}\")\n",
        "            return client, bigquery, NotFound\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Dataset access test failed: {e}\")\n",
        "            print(\"Note: This might be normal if the dataset exists but has restricted access\")\n",
        "            return client, bigquery, NotFound\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"❌ google-cloud-bigquery not installed. Installing now...\")\n",
        "        !pip install google-cloud-bigquery\n",
        "\n",
        "        from google.cloud import bigquery\n",
        "        from google.cloud.exceptions import NotFound\n",
        "        client = bigquery.Client(project=PROJECT_ID)\n",
        "        return client, bigquery, NotFound\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error setting up BigQuery client: {e}\")\n",
        "        print(\"\\nTrying alternative authentication...\")\n",
        "        return None, None, None\n",
        "\n",
        "def create_bigquery_table_schema():\n",
        "    \"\"\"\n",
        "    Define BigQuery table schema based on the generated dataset\n",
        "    \"\"\"\n",
        "    # Import here in case it's not available at module level\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"evaluate_date\", \"DATE\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"customer_id\", \"STRING\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"relation_type\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"base_customer\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"date_added\", \"DATE\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"delete_flag\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"cust_type\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"cust_name\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"address\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"city\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"state\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"postcode\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"branch\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"operating_branch\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"salutation\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"class_type\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"sic_code\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"customer_category\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"charge_officer1\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"account_id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"institution\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"folio\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"ledger\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"account\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"bank_code\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"account_status\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"product\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"prod_lvl1\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"prod_lvl2\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"prod_lvl3\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"prod_lvl4\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"prod_grp\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"type\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"bgod\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"custacc\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"oic_report\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"business_date\", \"DATE\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"footing_type\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"balance\", \"FLOAT64\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"footing\", \"FLOAT64\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"current_limit\", \"FLOAT64\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"maturity_date\", \"DATE\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"tran_type\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"tran_method\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"transaction_description\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"actual_date\", \"DATE\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"tran_eff_date\", \"DATE\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"tran_amount\", \"FLOAT64\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"amount\", \"FLOAT64\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"dr_cr\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"terminal_type\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"card_bin_no\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"reference_number\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"reference_data\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"reference\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"oic1_new\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"name_new\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"oic_name\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"oic1\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"name\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"adm_new\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"adm\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"gl_util_new\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"title\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"gl_cost_code\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"gl_utility_code\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"effective_from\", \"DATE\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"effective_to\", \"DATE\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"gl_ml1_id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"gl_ml1_name\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"gl_ml2_id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"gl_ml2_name\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"gl_ml3_id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"gl_ml3_name\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"gl_ml4_id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"gl_ml4_name\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"gl_ml5_id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"gl_ml5_name\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"rel_mgt_class\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"direct_report_adm\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"direct_report_name\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"direct_report_title\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"direct_report_adm_overwrite\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"curr_flag\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"oic_and_name\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"event_timestamp_utc\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"avg_products_per_customer\", \"FLOAT64\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"proactive_opportunity_win_rate\", \"FLOAT64\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"key_product_portfolio_growth\", \"FLOAT64\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"competitor_win_back_rate\", \"FLOAT64\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"opportunity_source\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"displaced_competitor\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"strategic_product_group\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"time_granularity\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"reporting_period_start\", \"DATE\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"reporting_period_end\", \"DATE\", mode=\"NULLABLE\"),\n",
        "    ]\n",
        "    return schema\n",
        "\n",
        "def create_or_update_bigquery_table(client, table_id, schema, bigquery, NotFound):\n",
        "    \"\"\"\n",
        "    Create BigQuery table if it doesn't exist, or update schema if it does\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if table exists\n",
        "        table = client.get_table(table_id)\n",
        "        print(f\"Table {table_id} already exists. Checking schema...\")\n",
        "\n",
        "        # Check if schema needs updating\n",
        "        current_schema = table.schema\n",
        "        if len(current_schema) != len(schema):\n",
        "            print(\"Schema differs. Updating table schema...\")\n",
        "            table.schema = schema\n",
        "            table = client.update_table(table, [\"schema\"])\n",
        "            print(\"Table schema updated successfully.\")\n",
        "        else:\n",
        "            print(\"Table schema is up to date.\")\n",
        "\n",
        "    except NotFound:\n",
        "        # Table doesn't exist, create it\n",
        "        print(f\"Creating table {table_id}...\")\n",
        "        table = bigquery.Table(table_id, schema=schema)\n",
        "        table = client.create_table(table)\n",
        "        print(f\"Table {table_id} created successfully.\")\n",
        "\n",
        "    return table\n",
        "\n",
        "def upload_to_bigquery(df, client, table_id, bigquery, write_disposition=\"WRITE_TRUNCATE\"):\n",
        "    \"\"\"\n",
        "    Upload DataFrame to BigQuery table\n",
        "    \"\"\"\n",
        "    # Configure the job\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        write_disposition=write_disposition,\n",
        "        autodetect=False,  # Use our explicit schema\n",
        "        schema=create_bigquery_table_schema()\n",
        "    )\n",
        "\n",
        "    print(f\"Starting upload to {table_id}...\")\n",
        "    print(f\"Upload mode: {write_disposition}\")\n",
        "    print(f\"Rows to upload: {len(df):,}\")\n",
        "\n",
        "    # Start the job\n",
        "    job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
        "\n",
        "    # Wait for the job to complete\n",
        "    try:\n",
        "        job.result()  # Wait for job to complete\n",
        "        print(f\"✅ Successfully uploaded {len(df):,} rows to {table_id}\")\n",
        "\n",
        "        # Get table info\n",
        "        table = client.get_table(table_id)\n",
        "        print(f\"📊 Table now contains {table.num_rows:,} total rows\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error uploading to BigQuery: {e}\")\n",
        "        if hasattr(job, 'errors') and job.errors:\n",
        "            for error in job.errors:\n",
        "                print(f\"Job error: {error}\")\n",
        "        return False\n",
        "\n",
        "def generate_bendigo_adelaide_banking_data():\n",
        "    \"\"\"\n",
        "    Generate comprehensive banking dataset for Bendigo and Adelaide Bank\n",
        "    Returns: pandas DataFrame with 150,000 rows and 87 columns\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Generating {NUM_ROWS:,} rows of Bendigo and Adelaide Bank portfolio data...\")\n",
        "\n",
        "    # Australian states and territories\n",
        "    australian_states = ['VIC', 'NSW', 'QLD', 'WA', 'SA', 'TAS', 'NT', 'ACT']\n",
        "\n",
        "    # Australian cities weighted by population\n",
        "    australian_cities = [\n",
        "        'Melbourne', 'Sydney', 'Brisbane', 'Perth', 'Adelaide', 'Gold Coast',\n",
        "        'Newcastle', 'Canberra', 'Sunshine Coast', 'Wollongong', 'Hobart',\n",
        "        'Geelong', 'Townsville', 'Cairns', 'Darwin', 'Toowoomba', 'Ballarat',\n",
        "        'Bendigo', 'Albury', 'Launceston', 'Mackay', 'Rockhampton', 'Bunbury',\n",
        "        'Bundaberg', 'Coffs Harbour', 'Wagga Wagga', 'Hervey Bay', 'Mildura',\n",
        "        'Shepparton', 'Port Macquarie', 'Gladstone', 'Tamworth', 'Traralgon',\n",
        "        'Orange', 'Bowral', 'Geraldton', 'Dubbo', 'Nowra', 'Warrnambool'\n",
        "    ]\n",
        "\n",
        "    # Bendigo and Adelaide Bank branch names (realistic Australian locations)\n",
        "    branch_names = [\n",
        "        'Melbourne CBD', 'Sydney CBD', 'Brisbane CBD', 'Perth CBD', 'Adelaide CBD',\n",
        "        'Bendigo Central', 'Ballarat', 'Geelong', 'Shepparton', 'Mildura',\n",
        "        'Echuca', 'Swan Hill', 'Horsham', 'Warrnambool', 'Hamilton',\n",
        "        'Castlemaine', 'Kyneton', 'Woodend', 'Gisborne', 'Sunbury',\n",
        "        'Melton', 'Bacchus Marsh', 'Ararat', 'Stawell', 'Nhill',\n",
        "        'Adelaide Hills', 'Mount Gambier', 'Port Augusta', 'Whyalla', 'Port Pirie',\n",
        "        'Murray Bridge', 'Victor Harbor', 'Gawler', 'Mount Barker', 'Berri',\n",
        "        'Renmark', 'Loxton', 'Naracoorte', 'Millicent', 'Bordertown',\n",
        "        'Kadina', 'Clare', 'Jamestown', 'Peterborough', 'Leigh Creek',\n",
        "        'Coober Pedy', 'Roxby Downs', 'Wudinna', 'Streaky Bay', 'Ceduna',\n",
        "        'Broken Hill', 'Mildura West', 'Swan Hill East', 'Echuca South',\n",
        "        'Bendigo North', 'Bendigo South', 'Bendigo East', 'Bendigo West',\n",
        "        'Kangaroo Flat', 'Eaglehawk', 'Strathfieldsaye', 'Golden Square',\n",
        "        'White Hills', 'Quarry Hill', 'Flora Hill', 'Kennington',\n",
        "        'Huntly', 'Marong', 'Heathcote', 'Elmore', 'Rochester', 'Lockington'\n",
        "    ]\n",
        "\n",
        "    # Relationship Manager names (realistic Australian names)\n",
        "    rm_first_names = [\n",
        "        'Andrew', 'Sarah', 'Michael', 'Emma', 'David', 'Jessica', 'James', 'Rebecca',\n",
        "        'Matthew', 'Lisa', 'Daniel', 'Michelle', 'Christopher', 'Amanda', 'Anthony',\n",
        "        'Nicole', 'Mark', 'Samantha', 'Paul', 'Rachel', 'Steven', 'Catherine',\n",
        "        'Peter', 'Jennifer', 'Robert', 'Melissa', 'John', 'Stephanie', 'Gary',\n",
        "        'Natalie', 'Simon', 'Vanessa', 'Luke', 'Tanya', 'Adam', 'Joanne',\n",
        "        'Benjamin', 'Kylie', 'Nathan', 'Tracey', 'Ryan', 'Leanne', 'Scott',\n",
        "        'Donna', 'Timothy', 'Sharon', 'Jason', 'Karen', 'Brett', 'Susan'\n",
        "    ]\n",
        "\n",
        "    rm_last_names = [\n",
        "        'Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller',\n",
        "        'Davis', 'Rodriguez', 'Martinez', 'Hernandez', 'Lopez', 'Gonzalez',\n",
        "        'Wilson', 'Anderson', 'Thomas', 'Taylor', 'Moore', 'Jackson', 'Martin',\n",
        "        'Lee', 'Perez', 'Thompson', 'White', 'Harris', 'Sanchez', 'Clark',\n",
        "        'Ramirez', 'Lewis', 'Robinson', 'Walker', 'Young', 'Allen', 'King',\n",
        "        'Wright', 'Scott', 'Torres', 'Nguyen', 'Hill', 'Flores', 'Green',\n",
        "        'Adams', 'Nelson', 'Baker', 'Hall', 'Rivera', 'Campbell', 'Mitchell',\n",
        "        'Carter', 'Roberts', 'Gomez', 'Phillips', 'Evans', 'Turner', 'Diaz'\n",
        "    ]\n",
        "\n",
        "    # Product hierarchy data\n",
        "    product_hierarchy = {\n",
        "        'Loans': {\n",
        "            'Home Loans': {\n",
        "                'Variable Rate': ['Standard Variable', 'Premium Variable', 'Introductory Variable'],\n",
        "                'Fixed Rate': ['1 Year Fixed', '2 Year Fixed', '3 Year Fixed', '5 Year Fixed']\n",
        "            },\n",
        "            'Business Loans': {\n",
        "                'Term Loans': ['Equipment Finance', 'Commercial Property', 'Working Capital'],\n",
        "                'Line of Credit': ['Business LOC', 'Commercial LOC', 'Agribusiness LOC']\n",
        "            },\n",
        "            'Personal Loans': {\n",
        "                'Secured': ['Car Loan', 'Personal Secured'],\n",
        "                'Unsecured': ['Personal Unsecured', 'Debt Consolidation']\n",
        "            }\n",
        "        },\n",
        "        'Deposits': {\n",
        "            'Transaction Accounts': {\n",
        "                'Everyday Banking': ['Complete Access', 'Bendigo Access', 'Student Access'],\n",
        "                'Business Banking': ['Business Cheque', 'Business Access', 'Not-for-Profit']\n",
        "            },\n",
        "            'Savings': {\n",
        "                'Term Deposits': ['3 Month TD', '6 Month TD', '12 Month TD', '24 Month TD'],\n",
        "                'Online Savings': ['eSaver', 'Goal Saver', 'Bonus Saver']\n",
        "            }\n",
        "        },\n",
        "        'Cards': {\n",
        "            'Credit Cards': {\n",
        "                'Rewards': ['Platinum Rewards', 'Gold Rewards', 'Classic Rewards'],\n",
        "                'Low Rate': ['Low Rate Platinum', 'Low Rate Gold']\n",
        "            },\n",
        "            'Debit Cards': {\n",
        "                'EFTPOS': ['Bendigo Debit', 'Business Debit'],\n",
        "                'Visa Debit': ['Visa Debit', 'Business Visa Debit']\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Generate base data structure\n",
        "    data = {}\n",
        "\n",
        "    # Date fields - evaluate_date with clustering at month-ends\n",
        "    print(\"Generating date fields...\")\n",
        "    evaluate_dates = []\n",
        "    for _ in range(NUM_ROWS):\n",
        "        if random.random() < 0.3:  # 30% chance of month-end clustering\n",
        "            # Generate month-end dates\n",
        "            year = random.randint(2022, 2025)\n",
        "            month = random.randint(1, 12)\n",
        "            if month == 12:\n",
        "                last_day = 31\n",
        "            elif month in [4, 6, 9, 11]:\n",
        "                last_day = 30\n",
        "            elif month == 2:\n",
        "                last_day = 28 if year % 4 != 0 else 29\n",
        "            else:\n",
        "                last_day = 31\n",
        "            day = random.randint(max(1, last_day-2), last_day)\n",
        "            evaluate_dates.append(date(year, month, day))\n",
        "        else:\n",
        "            # Random dates within range\n",
        "            days_diff = (END_DATE.date() - START_DATE.date()).days\n",
        "            random_days = random.randint(0, days_diff)\n",
        "            evaluate_dates.append(START_DATE.date() + timedelta(days=random_days))\n",
        "\n",
        "    data['evaluate_date'] = evaluate_dates\n",
        "\n",
        "    # Customer identification fields\n",
        "    print(\"Generating customer identification...\")\n",
        "    data['customer_id'] = [f\"CUST{str(i).zfill(6)}\" for i in range(1, NUM_ROWS + 1)]\n",
        "    data['relation_type'] = np.random.choice(['Primary', 'Joint'], NUM_ROWS, p=[0.8, 0.2])\n",
        "    data['base_customer'] = np.random.choice(['Y', 'N'], NUM_ROWS, p=[0.9, 0.1])\n",
        "\n",
        "    # Date added - uniform distribution over past 5 years\n",
        "    date_added_start = datetime(2020, 6, 5)\n",
        "    date_added_list = []\n",
        "    for _ in range(NUM_ROWS):\n",
        "        days_diff = (END_DATE - date_added_start).days\n",
        "        random_days = random.randint(0, days_diff)\n",
        "        date_added_list.append((date_added_start + timedelta(days=random_days)).date())\n",
        "    data['date_added'] = date_added_list\n",
        "\n",
        "    data['delete_flag'] = np.random.choice([False, True], NUM_ROWS, p=[0.98, 0.02])\n",
        "    data['cust_type'] = np.random.choice(['Individual', 'Business'], NUM_ROWS, p=[0.7, 0.3])\n",
        "\n",
        "    # Customer names based on type\n",
        "    print(\"Generating customer names...\")\n",
        "    customer_names = []\n",
        "    for cust_type in data['cust_type']:\n",
        "        if cust_type == 'Individual':\n",
        "            first_name = random.choice(rm_first_names)\n",
        "            last_name = random.choice(rm_last_names)\n",
        "            customer_names.append(f\"{first_name} {last_name}\")\n",
        "        else:\n",
        "            business_types = ['Pty Ltd', 'Pty Limited', 'Limited', 'P/L', 'Trust', 'Partnership']\n",
        "            business_names = [\n",
        "                'Sunrise', 'Golden', 'Premier', 'Elite', 'Advanced', 'Professional',\n",
        "                'Quality', 'Superior', 'Excellence', 'Innovation', 'Dynamic', 'Strategic',\n",
        "                'Progressive', 'Modern', 'Classic', 'Heritage', 'Future', 'Vision'\n",
        "            ]\n",
        "            business_sectors = [\n",
        "                'Consulting', 'Services', 'Solutions', 'Industries', 'Enterprises',\n",
        "                'Holdings', 'Group', 'Partners', 'Associates', 'Ventures', 'Capital',\n",
        "                'Investments', 'Development', 'Construction', 'Manufacturing', 'Trading'\n",
        "            ]\n",
        "            name = f\"{random.choice(business_names)} {random.choice(business_sectors)} {random.choice(business_types)}\"\n",
        "            customer_names.append(name)\n",
        "    data['cust_name'] = customer_names\n",
        "\n",
        "    # Address information\n",
        "    print(\"Generating address information...\")\n",
        "    street_numbers = [str(random.randint(1, 999)) for _ in range(NUM_ROWS)]\n",
        "    street_names = [\n",
        "        'Collins', 'Bourke', 'Flinders', 'Elizabeth', 'Queen', 'King', 'William',\n",
        "        'Spencer', 'Russell', 'Swanston', 'Exhibition', 'Spring', 'Lonsdale',\n",
        "        'Little Collins', 'Little Bourke', 'La Trobe', 'Franklin', 'A\\'Beckett',\n",
        "        'Flagstaff', 'Dudley', 'Hardware', 'Rundle', 'Hindley', 'Currie',\n",
        "        'Grenfell', 'Pirie', 'Waymouth', 'Gouger', 'Grote', 'Angas'\n",
        "    ]\n",
        "    street_types = ['Street', 'Road', 'Avenue', 'Drive', 'Lane', 'Court', 'Place', 'Way']\n",
        "\n",
        "    addresses = []\n",
        "    for i in range(NUM_ROWS):\n",
        "        street_num = street_numbers[i]\n",
        "        street_name = random.choice(street_names)\n",
        "        street_type = random.choice(street_types)\n",
        "        addresses.append(f\"{street_num} {street_name} {street_type}\")\n",
        "    data['address'] = addresses\n",
        "\n",
        "    # Cities weighted by population - FIXED: 39 cities with 39 weights (5 + 34)\n",
        "    city_weights = [0.25, 0.20, 0.15, 0.10, 0.08] + [0.22/34] * 34  # Top 5 cities get higher weights\n",
        "    data['city'] = np.random.choice(australian_cities, NUM_ROWS, p=city_weights)\n",
        "\n",
        "    # States based on cities (simplified mapping)\n",
        "    state_mapping = {\n",
        "        'Melbourne': 'VIC', 'Geelong': 'VIC', 'Ballarat': 'VIC', 'Bendigo': 'VIC',\n",
        "        'Sydney': 'NSW', 'Newcastle': 'NSW', 'Wollongong': 'NSW', 'Albury': 'NSW',\n",
        "        'Brisbane': 'QLD', 'Gold Coast': 'QLD', 'Sunshine Coast': 'QLD', 'Townsville': 'QLD',\n",
        "        'Perth': 'WA', 'Bunbury': 'WA', 'Geraldton': 'WA',\n",
        "        'Adelaide': 'SA', 'Mount Gambier': 'SA',\n",
        "        'Hobart': 'TAS', 'Launceston': 'TAS',\n",
        "        'Darwin': 'NT',\n",
        "        'Canberra': 'ACT'\n",
        "    }\n",
        "\n",
        "    states = []\n",
        "    for city in data['city']:\n",
        "        if city in state_mapping:\n",
        "            states.append(state_mapping[city])\n",
        "        else:\n",
        "            states.append(random.choice(australian_states))\n",
        "    data['state'] = states\n",
        "\n",
        "    # Australian postcodes (simplified - using realistic ranges)\n",
        "    postcode_ranges = {\n",
        "        'VIC': (3000, 3999), 'NSW': (2000, 2999), 'QLD': (4000, 4999),\n",
        "        'WA': (6000, 6999), 'SA': (5000, 5999), 'TAS': (7000, 7999),\n",
        "        'NT': (800, 999), 'ACT': (2600, 2699)\n",
        "    }\n",
        "\n",
        "    postcodes = []\n",
        "    for state in data['state']:\n",
        "        min_pc, max_pc = postcode_ranges[state]\n",
        "        postcodes.append(str(random.randint(min_pc, max_pc)))\n",
        "    data['postcode'] = postcodes\n",
        "\n",
        "    # Branch assignment\n",
        "    data['branch'] = np.random.choice(branch_names, NUM_ROWS)\n",
        "    data['operating_branch'] = [branch if random.random() < 0.9 else random.choice(branch_names)\n",
        "                               for branch in data['branch']]\n",
        "\n",
        "    # Salutations\n",
        "    salutations = ['Mr', 'Ms', 'Mrs', 'Dr', 'Prof', 'Miss']\n",
        "    data['salutation'] = np.random.choice(salutations, NUM_ROWS)\n",
        "\n",
        "    # Customer classification\n",
        "    data['class_type'] = np.random.choice(['Platinum', 'Gold', 'Standard'], NUM_ROWS, p=[0.1, 0.2, 0.7])\n",
        "\n",
        "    # SIC codes for business customers only\n",
        "    sic_codes = ['0111', '0112', '0115', '1311', '1389', '2211', '2212', '2213', '3311', '4111']\n",
        "    sic_code_list = []\n",
        "    for cust_type in data['cust_type']:\n",
        "        if cust_type == 'Business':\n",
        "            sic_code_list.append(random.choice(sic_codes))\n",
        "        else:\n",
        "            sic_code_list.append(None)\n",
        "    data['sic_code'] = sic_code_list\n",
        "\n",
        "    # Customer category\n",
        "    data['customer_category'] = np.random.choice(['Retail', 'Business', 'Agribusiness'], NUM_ROWS, p=[0.6, 0.3, 0.1])\n",
        "\n",
        "    # Relationship Managers\n",
        "    print(\"Generating relationship manager data...\")\n",
        "    rm_names = [f\"{random.choice(rm_first_names)} {random.choice(rm_last_names)}\" for _ in range(150)]\n",
        "    data['charge_officer1'] = np.random.choice(rm_names, NUM_ROWS)\n",
        "\n",
        "    # Account information\n",
        "    print(\"Generating account information...\")\n",
        "    data['account_id'] = [f\"ACC{str(i).zfill(6)}\" for i in range(1, NUM_ROWS + 1)]\n",
        "    data['institution'] = np.random.choice(['BENDIGO', 'ADELAIDE', 'RURAL'], NUM_ROWS, p=[0.6, 0.3, 0.1])\n",
        "    data['folio'] = [str(random.randint(100000, 999999)) for _ in range(NUM_ROWS)]\n",
        "    data['ledger'] = [''.join(random.choices(string.ascii_uppercase, k=3)) for _ in range(NUM_ROWS)]\n",
        "    data['account'] = [str(random.randint(10000000, 999999999)) for _ in range(NUM_ROWS)]\n",
        "\n",
        "    # Bank codes\n",
        "    bank_code_mapping = {'BENDIGO': 'BEND', 'ADELAIDE': 'ADLB', 'RURAL': 'RUR'}\n",
        "    data['bank_code'] = [bank_code_mapping[inst] for inst in data['institution']]\n",
        "\n",
        "    # Account status\n",
        "    data['account_status'] = np.random.choice(['Active', 'Closed', 'Dormant'], NUM_ROWS, p=[0.85, 0.10, 0.05])\n",
        "\n",
        "    # Product hierarchy generation\n",
        "    print(\"Generating product hierarchy...\")\n",
        "    products = []\n",
        "    prod_lvl1s = []\n",
        "    prod_lvl2s = []\n",
        "    prod_lvl3s = []\n",
        "    prod_lvl4s = []\n",
        "    prod_grps = []\n",
        "\n",
        "    for _ in range(NUM_ROWS):\n",
        "        lvl1 = random.choice(list(product_hierarchy.keys()))\n",
        "        lvl2 = random.choice(list(product_hierarchy[lvl1].keys()))\n",
        "        lvl3 = random.choice(list(product_hierarchy[lvl1][lvl2].keys()))\n",
        "        lvl4 = random.choice(product_hierarchy[lvl1][lvl2][lvl3])\n",
        "\n",
        "        products.append(f\"{lvl1} - {lvl2} - {lvl3} - {lvl4}\")\n",
        "        prod_lvl1s.append(lvl1)\n",
        "        prod_lvl2s.append(lvl2)\n",
        "        prod_lvl3s.append(lvl3)\n",
        "        prod_lvl4s.append(lvl4)\n",
        "        prod_grps.append(f\"{lvl1[:2]}{lvl2[:2]}\".upper())\n",
        "\n",
        "    data['product'] = products\n",
        "    data['prod_lvl1'] = prod_lvl1s\n",
        "    data['prod_lvl2'] = prod_lvl2s\n",
        "    data['prod_lvl3'] = prod_lvl3s\n",
        "    data['prod_lvl4'] = prod_lvl4s\n",
        "    data['prod_grp'] = prod_grps\n",
        "    data['type'] = np.random.choice(['Retail', 'Business'], NUM_ROWS, p=[0.7, 0.3])\n",
        "\n",
        "    # Internal codes\n",
        "    data['bgod'] = [''.join(random.choices(string.ascii_uppercase, k=4)) for _ in range(NUM_ROWS)]\n",
        "    data['custacc'] = [f\"CA{random.randint(10000, 99999)}\" for _ in range(NUM_ROWS)]\n",
        "    data['oic_report'] = [f\"OIC{random.randint(100, 999)}\" for _ in range(NUM_ROWS)]\n",
        "\n",
        "    # Business dates\n",
        "    business_dates = []\n",
        "    for eval_date in data['evaluate_date']:\n",
        "        # Business date should be close to evaluate date\n",
        "        days_offset = random.randint(-5, 5)\n",
        "        business_dates.append(eval_date + timedelta(days=days_offset))\n",
        "    data['business_date'] = business_dates\n",
        "\n",
        "    # Footing types\n",
        "    data['footing_type'] = np.random.choice(['Book', 'Available'], NUM_ROWS, p=[0.7, 0.3])\n",
        "\n",
        "    # Financial measures\n",
        "    print(\"Generating financial measures...\")\n",
        "\n",
        "    # Balance - log-normal distribution with outliers\n",
        "    base_balances = np.random.lognormal(mean=8.5, sigma=1.5, size=NUM_ROWS)\n",
        "\n",
        "    # Add outliers for recent dates (last 30 days)\n",
        "    recent_date_threshold = END_DATE.date() - timedelta(days=30)\n",
        "    outlier_indices = []\n",
        "\n",
        "    for i, eval_date in enumerate(data['evaluate_date']):\n",
        "        if eval_date >= recent_date_threshold and random.random() < OUTLIER_FRACTION:\n",
        "            outlier_indices.append(i)\n",
        "\n",
        "    balances = base_balances.copy()\n",
        "    for idx in outlier_indices:\n",
        "        balances[idx] *= OUTLIER_MAGNITUDE\n",
        "\n",
        "    data['balance'] = np.round(balances, 2)\n",
        "\n",
        "    # Footing - correlated with balance but can differ\n",
        "    footings = []\n",
        "    for balance in data['balance']:\n",
        "        correlation_factor = random.uniform(0.8, 1.2)\n",
        "        footings.append(round(balance * correlation_factor, 2))\n",
        "    data['footing'] = footings\n",
        "\n",
        "    # Current limit - 0 for deposits, positive for loans\n",
        "    current_limits = []\n",
        "    for prod_lvl1 in data['prod_lvl1']:\n",
        "        if prod_lvl1 == 'Loans':\n",
        "            current_limits.append(round(random.uniform(10000, 1000000), 2))\n",
        "        else:\n",
        "            current_limits.append(0.0)\n",
        "    data['current_limit'] = current_limits\n",
        "\n",
        "    # Maturity dates for term products\n",
        "    maturity_dates = []\n",
        "    for i, (date_added, prod_lvl3) in enumerate(zip(data['date_added'], data['prod_lvl3'])):\n",
        "        if 'Term' in prod_lvl3 or 'Fixed' in prod_lvl3:\n",
        "            years_to_add = random.randint(1, 5)\n",
        "            maturity_dates.append(date_added + timedelta(days=years_to_add*365))\n",
        "        else:\n",
        "            maturity_dates.append(None)\n",
        "    data['maturity_date'] = maturity_dates\n",
        "\n",
        "    # Transaction information\n",
        "    print(\"Generating transaction data...\")\n",
        "    data['tran_type'] = np.random.choice(['Deposit', 'Withdrawal', 'Payment'], NUM_ROWS, p=[0.6, 0.3, 0.1])\n",
        "    data['tran_method'] = np.random.choice(['Online', 'Branch', 'ATM'], NUM_ROWS, p=[0.5, 0.3, 0.2])\n",
        "\n",
        "    # Transaction descriptions\n",
        "    transaction_descriptions = [\n",
        "        'Direct Debit Payment', 'EFTPOS Purchase', 'ATM Withdrawal', 'Online Transfer',\n",
        "        'Salary Credit', 'Interest Payment', 'Fee Debit', 'Cheque Deposit',\n",
        "        'Card Payment', 'Bill Payment', 'Loan Repayment', 'Term Deposit Maturity'\n",
        "    ]\n",
        "    data['transaction_description'] = np.random.choice(transaction_descriptions, NUM_ROWS)\n",
        "\n",
        "    # Transaction dates\n",
        "    actual_dates = []\n",
        "    tran_eff_dates = []\n",
        "    for business_date in data['business_date']:\n",
        "        actual_date = business_date + timedelta(days=random.randint(-1, 1))\n",
        "        eff_date = actual_date + timedelta(days=random.randint(0, 2))\n",
        "        actual_dates.append(actual_date)\n",
        "        tran_eff_dates.append(eff_date)\n",
        "\n",
        "    data['actual_date'] = actual_dates\n",
        "    data['tran_eff_date'] = tran_eff_dates\n",
        "\n",
        "    # Transaction amounts - skewed distribution with outliers\n",
        "    base_tran_amounts = np.random.gamma(2, 500, NUM_ROWS)\n",
        "\n",
        "    # Add outliers for recent transactions\n",
        "    tran_amounts = base_tran_amounts.copy()\n",
        "    for idx in outlier_indices:\n",
        "        if random.random() < 0.5:  # 50% chance for transaction outliers\n",
        "            tran_amounts[idx] *= OUTLIER_MAGNITUDE\n",
        "\n",
        "    data['tran_amount'] = np.round(tran_amounts, 2)\n",
        "    data['amount'] = data['tran_amount']  # Duplicate field\n",
        "\n",
        "    # Debit/Credit indicator\n",
        "    data['dr_cr'] = np.random.choice(['DR', 'CR'], NUM_ROWS, p=[0.5, 0.5])\n",
        "\n",
        "    # Terminal and card information\n",
        "    data['terminal_type'] = np.random.choice(['ATM', 'POS', 'Online'], NUM_ROWS, p=[0.3, 0.4, 0.3])\n",
        "    data['card_bin_no'] = [str(random.randint(400000, 599999)) for _ in range(NUM_ROWS)]\n",
        "\n",
        "    # Reference fields\n",
        "    data['reference_number'] = [f\"REF{random.randint(1000000, 9999999)}\" for _ in range(NUM_ROWS)]\n",
        "    data['reference_data'] = [f\"RD{random.randint(10000, 99999)}\" for _ in range(NUM_ROWS)]\n",
        "    data['reference'] = [f\"R{random.randint(1000, 9999)}\" for _ in range(NUM_ROWS)]\n",
        "\n",
        "    # Officer information\n",
        "    print(\"Generating officer and management data...\")\n",
        "\n",
        "    # Generate unique RM codes and names\n",
        "    unique_rms = list(set(data['charge_officer1']))\n",
        "    rm_codes = {name: f\"RM{str(i).zfill(3)}\" for i, name in enumerate(unique_rms)}\n",
        "\n",
        "    data['oic1_new'] = [rm_codes[rm] for rm in data['charge_officer1']]\n",
        "    data['name_new'] = data['charge_officer1']\n",
        "    data['oic_name'] = data['charge_officer1']  # Legacy field\n",
        "    data['oic1'] = data['oic1_new']  # Legacy field\n",
        "    data['name'] = data['charge_officer1']  # Legacy field\n",
        "\n",
        "    # Administrative codes\n",
        "    data['adm_new'] = [f\"ADM{random.randint(100, 999)}\" for _ in range(NUM_ROWS)]\n",
        "    data['adm'] = data['adm_new']  # Legacy field\n",
        "    data['gl_util_new'] = [f\"GL{random.randint(1000, 9999)}\" for _ in range(NUM_ROWS)]\n",
        "\n",
        "    # Titles\n",
        "    titles = ['Relationship Manager', 'Senior Relationship Manager', 'Team Leader',\n",
        "              'Regional Manager', 'Area Manager', 'Business Banking Manager']\n",
        "    data['title'] = np.random.choice(titles, NUM_ROWS)\n",
        "\n",
        "    # GL codes\n",
        "    data['gl_cost_code'] = [f\"CC{random.randint(1000, 9999)}\" for _ in range(NUM_ROWS)]\n",
        "    data['gl_utility_code'] = [f\"GU{random.randint(1000, 9999)}\" for _ in range(NUM_ROWS)]\n",
        "\n",
        "    # Effective dates for RM assignments\n",
        "    effective_from_dates = []\n",
        "    effective_to_dates = []\n",
        "\n",
        "    for _ in range(NUM_ROWS):\n",
        "        from_date = START_DATE.date() + timedelta(days=random.randint(0, 1000))\n",
        "        effective_from_dates.append(from_date)\n",
        "\n",
        "        # 80% current assignments (NULL to date), 20% past assignments\n",
        "        if random.random() < 0.8:\n",
        "            effective_to_dates.append(None)\n",
        "        else:\n",
        "            to_date = from_date + timedelta(days=random.randint(30, 730))\n",
        "            effective_to_dates.append(to_date)\n",
        "\n",
        "    data['effective_from'] = effective_from_dates\n",
        "    data['effective_to'] = effective_to_dates\n",
        "\n",
        "    # GL multi-level hierarchy\n",
        "    gl_ml1_options = ['Retail Banking', 'Business Banking', 'Agribusiness', 'Wealth Management']\n",
        "    gl_ml2_options = ['Metro', 'Regional', 'Rural', 'Corporate']\n",
        "    gl_ml3_options = ['Victoria', 'NSW', 'Queensland', 'South Australia', 'Western Australia']\n",
        "    gl_ml4_options = ['Melbourne', 'Sydney', 'Brisbane', 'Adelaide', 'Perth', 'Bendigo']\n",
        "    gl_ml5_options = ['CBD', 'Suburban', 'Rural', 'Industrial']\n",
        "\n",
        "    data['gl_ml1_id'] = [f\"ML1{random.randint(100, 999)}\" for _ in range(NUM_ROWS)]\n",
        "    data['gl_ml1_name'] = np.random.choice(gl_ml1_options, NUM_ROWS)\n",
        "    data['gl_ml2_id'] = [f\"ML2{random.randint(100, 999)}\" for _ in range(NUM_ROWS)]\n",
        "    data['gl_ml2_name'] = np.random.choice(gl_ml2_options, NUM_ROWS)\n",
        "    data['gl_ml3_id'] = [f\"ML3{random.randint(100, 999)}\" for _ in range(NUM_ROWS)]\n",
        "    data['gl_ml3_name'] = np.random.choice(gl_ml3_options, NUM_ROWS)\n",
        "    data['gl_ml4_id'] = [f\"ML4{random.randint(100, 999)}\" for _ in range(NUM_ROWS)]\n",
        "    data['gl_ml4_name'] = np.random.choice(gl_ml4_options, NUM_ROWS)\n",
        "    data['gl_ml5_id'] = [f\"ML5{random.randint(100, 999)}\" for _ in range(NUM_ROWS)]\n",
        "    data['gl_ml5_name'] = np.random.choice(gl_ml5_options, NUM_ROWS)\n",
        "\n",
        "    # Management hierarchy\n",
        "    data['rel_mgt_class'] = np.random.choice(['Senior RM', 'Junior RM', 'Graduate RM'], NUM_ROWS, p=[0.4, 0.5, 0.1])\n",
        "    data['direct_report_adm'] = [f\"DRA{random.randint(100, 999)}\" for _ in range(NUM_ROWS)]\n",
        "    data['direct_report_name'] = np.random.choice(rm_names[:50], NUM_ROWS)  # Subset for managers\n",
        "    data['direct_report_title'] = np.random.choice(['Regional Manager', 'Area Manager', 'General Manager'], NUM_ROWS)\n",
        "    data['direct_report_adm_overwrite'] = [f\"DRO{random.randint(100, 999)}\" for _ in range(NUM_ROWS)]\n",
        "\n",
        "    # Current flags\n",
        "    data['curr_flag'] = np.random.choice([True, False], NUM_ROWS, p=[0.95, 0.05])\n",
        "\n",
        "    # Combined OIC and name field\n",
        "    data['oic_and_name'] = [f\"{oic} - {name}\" for oic, name in zip(data['oic1_new'], data['name_new'])]\n",
        "\n",
        "    # Event timestamps\n",
        "    event_timestamps = []\n",
        "    for business_date in data['business_date']:\n",
        "        # Random time within business hours (9 AM to 5 PM)\n",
        "        hour = random.randint(9, 17)\n",
        "        minute = random.randint(0, 59)\n",
        "        second = random.randint(0, 59)\n",
        "        timestamp = datetime.combine(business_date, datetime.min.time().replace(hour=hour, minute=minute, second=second))\n",
        "        event_timestamps.append(timestamp)\n",
        "\n",
        "    data['event_timestamp_utc'] = event_timestamps\n",
        "\n",
        "    # KPI measures\n",
        "    print(\"Generating KPI measures...\")\n",
        "\n",
        "    # Average products per customer - normal distribution\n",
        "    avg_products = np.random.normal(2.5, 1.0, NUM_ROWS)\n",
        "    avg_products = np.clip(avg_products, 1.0, 6.0)  # Cap at 6\n",
        "    data['avg_products_per_customer'] = np.round(avg_products, 2)\n",
        "\n",
        "    # Proactive opportunity win rate - beta distribution skewed high\n",
        "    win_rates = np.random.beta(3, 2, NUM_ROWS) * 50 + 40  # 40-90% range\n",
        "    data['proactive_opportunity_win_rate'] = np.round(win_rates, 2)\n",
        "\n",
        "    # Key product portfolio growth - positive skewed integers\n",
        "    portfolio_growth = np.random.gamma(2, 5, NUM_ROWS)\n",
        "    portfolio_growth = np.clip(portfolio_growth, 0, 50)\n",
        "    data['key_product_portfolio_growth'] = np.round(portfolio_growth, 0)\n",
        "\n",
        "    # Competitor win back rate\n",
        "    win_back_rates = np.random.uniform(0, 30, NUM_ROWS)\n",
        "    # Add spikes for campaign periods (recent dates)\n",
        "    for i, eval_date in enumerate(data['evaluate_date']):\n",
        "        if eval_date >= recent_date_threshold and random.random() < 0.1:\n",
        "            win_back_rates[i] *= 2  # Double the rate during campaigns\n",
        "\n",
        "    data['competitor_win_back_rate'] = np.round(win_back_rates, 2)\n",
        "\n",
        "    # Opportunity sources\n",
        "    opportunity_sources = ['Analytics Alert', 'LinkedIn Signal', 'Customer Inquiry', 'Referral', 'Campaign Response']\n",
        "    data['opportunity_source'] = np.random.choice(opportunity_sources, NUM_ROWS, p=[0.6, 0.3, 0.05, 0.03, 0.02])\n",
        "\n",
        "    # Displaced competitors\n",
        "    competitors = ['Commonwealth Bank', 'ANZ', 'Westpac', 'NAB', 'ING', 'Macquarie Bank', 'Suncorp', 'Bank of Queensland']\n",
        "    data['displaced_competitor'] = np.random.choice(competitors, NUM_ROWS)\n",
        "\n",
        "    # Strategic product groups\n",
        "    strategic_groups = ['Business Loans', 'Wealth Solutions', 'Agribusiness Finance', 'Home Loans', 'Transaction Banking']\n",
        "    data['strategic_product_group'] = np.random.choice(strategic_groups, NUM_ROWS)\n",
        "\n",
        "    # Time granularity and reporting periods\n",
        "    time_granularities = ['Daily', 'Weekly', 'Monthly', 'Quarterly']\n",
        "    data['time_granularity'] = np.random.choice(time_granularities, NUM_ROWS, p=[0.1, 0.2, 0.5, 0.2])\n",
        "\n",
        "    # Reporting period start and end dates\n",
        "    reporting_start_dates = []\n",
        "    reporting_end_dates = []\n",
        "\n",
        "    for granularity, business_date in zip(data['time_granularity'], data['business_date']):\n",
        "        if granularity == 'Daily':\n",
        "            start_date = business_date\n",
        "            end_date = business_date\n",
        "        elif granularity == 'Weekly':\n",
        "            # Start of week (Monday)\n",
        "            days_since_monday = business_date.weekday()\n",
        "            start_date = business_date - timedelta(days=days_since_monday)\n",
        "            end_date = start_date + timedelta(days=6)\n",
        "        elif granularity == 'Monthly':\n",
        "            # Start of month\n",
        "            start_date = business_date.replace(day=1)\n",
        "            if start_date.month == 12:\n",
        "                end_date = start_date.replace(year=start_date.year + 1, month=1) - timedelta(days=1)\n",
        "            else:\n",
        "                end_date = start_date.replace(month=start_date.month + 1) - timedelta(days=1)\n",
        "        else:  # Quarterly\n",
        "            # Start of quarter\n",
        "            quarter_start_month = ((business_date.month - 1) // 3) * 3 + 1\n",
        "            start_date = business_date.replace(month=quarter_start_month, day=1)\n",
        "            if quarter_start_month == 10:\n",
        "                end_date = start_date.replace(year=start_date.year + 1, month=1) - timedelta(days=1)\n",
        "            else:\n",
        "                end_date = start_date.replace(month=quarter_start_month + 3) - timedelta(days=1)\n",
        "\n",
        "        reporting_start_dates.append(start_date)\n",
        "        reporting_end_dates.append(end_date)\n",
        "\n",
        "    data['reporting_period_start'] = reporting_start_dates\n",
        "    data['reporting_period_end'] = reporting_end_dates\n",
        "\n",
        "    # Create DataFrame\n",
        "    print(\"Creating DataFrame...\")\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Apply outliers to recent data for key financial measures\n",
        "    print(\"Applying outliers to recent data...\")\n",
        "    recent_mask = df['evaluate_date'] >= recent_date_threshold\n",
        "\n",
        "    # Apply outliers to balance and transaction amounts for recent records\n",
        "    outlier_mask = recent_mask & (np.random.random(NUM_ROWS) < OUTLIER_FRACTION)\n",
        "\n",
        "    df.loc[outlier_mask, 'balance'] *= OUTLIER_MAGNITUDE\n",
        "    df.loc[outlier_mask, 'tran_amount'] *= OUTLIER_MAGNITUDE\n",
        "    df.loc[outlier_mask, 'amount'] = df.loc[outlier_mask, 'tran_amount']\n",
        "    df.loc[outlier_mask, 'proactive_opportunity_win_rate'] = np.minimum(\n",
        "        df.loc[outlier_mask, 'proactive_opportunity_win_rate'] * 1.5, 95\n",
        "    )\n",
        "\n",
        "    # Round financial columns\n",
        "    financial_columns = ['balance', 'footing', 'current_limit', 'tran_amount', 'amount']\n",
        "    for col in financial_columns:\n",
        "        df[col] = df[col].round(2)\n",
        "\n",
        "    print(f\"Dataset generation complete! Generated {len(df):,} rows with {len(df.columns)} columns.\")\n",
        "    print(f\"Date range: {df['evaluate_date'].min()} to {df['evaluate_date'].max()}\")\n",
        "    print(f\"Outliers applied to {outlier_mask.sum():,} recent records ({(outlier_mask.sum()/NUM_ROWS)*100:.1f}%)\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def save_csv_backup(df, filename='bendigo_adelaide_bank_portfolio_analytics_150k.csv'):\n",
        "    \"\"\"\n",
        "    Save DataFrame as CSV with Australian date formatting\n",
        "    \"\"\"\n",
        "    print(f\"\\n📁 Saving CSV backup: {filename}\")\n",
        "    df_export = df.copy()\n",
        "\n",
        "    # Convert date columns to Australian format (DD/MM/YYYY) for CSV export\n",
        "    date_columns = ['evaluate_date', 'date_added', 'maturity_date', 'business_date',\n",
        "                   'actual_date', 'tran_eff_date', 'effective_from', 'effective_to',\n",
        "                   'reporting_period_start', 'reporting_period_end']\n",
        "\n",
        "    for col in date_columns:\n",
        "        if col in df_export.columns:\n",
        "            df_export[col] = df_export[col].apply(lambda x: x.strftime('%d/%m/%Y') if pd.notnull(x) else '')\n",
        "\n",
        "    # Convert timestamp to Australian format\n",
        "    if 'event_timestamp_utc' in df_export.columns:\n",
        "        df_export['event_timestamp_utc'] = df_export['event_timestamp_utc'].apply(\n",
        "            lambda x: x.strftime('%d/%m/%Y %H:%M:%S') if pd.notnull(x) else ''\n",
        "        )\n",
        "\n",
        "    df_export.to_csv(filename, index=False)\n",
        "    print(f\"✅ CSV saved successfully: {filename}\")\n",
        "\n",
        "    # Download file in Colab\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(filename)\n",
        "        print(\"📥 File download initiated in Colab\")\n",
        "    except:\n",
        "        print(\"Note: Not running in Colab, file saved locally\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function optimized for Google Colab\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"BENDIGO AND ADELAIDE BANK PORTFOLIO ANALYTICS DATASET\")\n",
        "    print(\"Google Colab Version\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Setup authentication first\n",
        "    auth_method = setup_colab_authentication()\n",
        "\n",
        "    if not auth_method:\n",
        "        print(\"❌ Authentication failed. Generating dataset and saving CSV only...\")\n",
        "        df = generate_bendigo_adelaide_banking_data()\n",
        "        save_csv_backup(df)\n",
        "        return\n",
        "\n",
        "    # Generate the dataset\n",
        "    df = generate_bendigo_adelaide_banking_data()\n",
        "\n",
        "    # Display basic information about the dataset\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"DATASET SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Total Rows: {len(df):,}\")\n",
        "    print(f\"Total Columns: {len(df.columns)}\")\n",
        "    print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "\n",
        "    print(\"\\nColumn Information:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, col in enumerate(df.columns, 1):\n",
        "        dtype = str(df[col].dtype)\n",
        "        null_count = df[col].isnull().sum()\n",
        "        print(f\"{i:2d}. {col:<35} {dtype:<15} ({null_count:,} nulls)\")\n",
        "\n",
        "    print(\"\\nSample Data (first 5 rows):\")\n",
        "    print(\"-\" * 50)\n",
        "    print(df.head())\n",
        "\n",
        "    print(\"\\nFinancial Measures Summary:\")\n",
        "    print(\"-\" * 50)\n",
        "    financial_cols = ['balance', 'footing', 'current_limit', 'tran_amount']\n",
        "    print(df[financial_cols].describe())\n",
        "\n",
        "    # Setup BigQuery and upload\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BIGQUERY UPLOAD\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Try BigQuery upload\n",
        "    client, bigquery, NotFound = setup_bigquery_client()\n",
        "\n",
        "    if client and bigquery and NotFound:\n",
        "        try:\n",
        "            # Create table schema\n",
        "            schema = create_bigquery_table_schema()\n",
        "\n",
        "            # Create or update table\n",
        "            create_or_update_bigquery_table(client, FULL_TABLE_ID, schema, bigquery, NotFound)\n",
        "\n",
        "            # Upload data to BigQuery\n",
        "            upload_success = upload_to_bigquery(df, client, FULL_TABLE_ID, bigquery, write_disposition=\"WRITE_TRUNCATE\")\n",
        "\n",
        "            if upload_success:\n",
        "                print(f\"\\n✅ SUCCESS! Data uploaded to BigQuery\")\n",
        "                print(f\"📊 Table: {FULL_TABLE_ID}\")\n",
        "                print(f\"📈 Rows: {len(df):,}\")\n",
        "                print(f\"📋 Columns: {len(df.columns)}\")\n",
        "\n",
        "                # Also save CSV for backup\n",
        "                print(\"\\n💾 Creating CSV backup...\")\n",
        "                save_csv_backup(df)\n",
        "            else:\n",
        "                print(\"\\n❌ BigQuery upload failed. Saving CSV instead...\")\n",
        "                save_csv_backup(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Unexpected error during BigQuery operations: {e}\")\n",
        "            print(\"Saving CSV backup...\")\n",
        "            save_csv_backup(df)\n",
        "    else:\n",
        "        print(\"\\n⚠️  BigQuery setup failed. Saving CSV instead...\")\n",
        "        save_csv_backup(df)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PROCESSING COMPLETE\")\n",
        "    print(\"Ready for Analytics and Executive Reporting\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime, timedelta, date\n",
        "import string\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Configuration parameters\n",
        "NUM_ROWS = 150000\n",
        "START_DATE = datetime(2023, 1, 1)\n",
        "END_DATE = datetime(2024, 6, 8)\n",
        "\n",
        "def generate_linkedin_navigator_data():\n",
        "    \"\"\"\n",
        "    Generate comprehensive LinkedIn Sales Navigator dataset\n",
        "    Returns: pandas DataFrame with 150,000 rows and 36 columns\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Generating {NUM_ROWS:,} rows of LinkedIn Sales Navigator data...\")\n",
        "\n",
        "    # Australian business context data\n",
        "    australian_states = ['VIC', 'NSW', 'QLD', 'WA', 'SA', 'TAS', 'NT', 'ACT']\n",
        "\n",
        "    australian_cities = [\n",
        "        'Melbourne', 'Sydney', 'Brisbane', 'Perth', 'Adelaide', 'Gold Coast',\n",
        "        'Newcastle', 'Canberra', 'Sunshine Coast', 'Wollongong', 'Hobart',\n",
        "        'Geelong', 'Townsville', 'Cairns', 'Darwin', 'Toowoomba', 'Ballarat',\n",
        "        'Bendigo', 'Albury', 'Launceston', 'Mackay', 'Rockhampton', 'Bunbury'\n",
        "    ]\n",
        "\n",
        "    # Business naming components\n",
        "    business_names = [\n",
        "        'Superior', 'Golden', 'Progressive', 'Innovation', 'Elite', 'Quality',\n",
        "        'Modern', 'Heritage', 'Excellence', 'Future', 'Advanced', 'Strategic',\n",
        "        'Dynamic', 'Professional', 'Premium', 'Global', 'National', 'Regional',\n",
        "        'Premier', 'Leading', 'Innovative', 'Specialist', 'Expert', 'Prime',\n",
        "        'Apex', 'Pinnacle', 'Summit', 'Crown', 'Royal', 'Imperial'\n",
        "    ]\n",
        "\n",
        "    business_types = [\n",
        "        'Consulting', 'Manufacturing', 'Services', 'Industries', 'Holdings',\n",
        "        'Group', 'Enterprises', 'Solutions', 'Capital', 'Ventures',\n",
        "        'Development', 'Associates', 'Partners', 'Corporation', 'Company'\n",
        "    ]\n",
        "\n",
        "    business_structures = ['Pty Ltd', 'Limited', 'Partnership', 'Trust', 'Corporation']\n",
        "\n",
        "    # Professional names (Australian context)\n",
        "    first_names = [\n",
        "        'Michael', 'Sarah', 'David', 'Emma', 'James', 'Lisa', 'Andrew', 'Rachel',\n",
        "        'Daniel', 'Jennifer', 'Matthew', 'Amanda', 'Christopher', 'Nicole', 'Ryan',\n",
        "        'Rebecca', 'Mark', 'Jessica', 'Peter', 'Michelle', 'Anthony', 'Stephanie',\n",
        "        'Paul', 'Laura', 'Steven', 'Catherine', 'John', 'Karen', 'Adam', 'Susan',\n",
        "        'Kevin', 'Helen', 'Robert', 'Anna', 'Timothy', 'Natalie', 'Jason', 'Claire',\n",
        "        'Benjamin', 'Samantha', 'Nathan', 'Louise', 'Simon', 'Victoria', 'Luke', 'Tracy'\n",
        "    ]\n",
        "\n",
        "    last_names = [\n",
        "        'Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Miller', 'Davis',\n",
        "        'Garcia', 'Rodriguez', 'Wilson', 'Martinez', 'Anderson', 'Taylor',\n",
        "        'Thomas', 'Hernandez', 'Moore', 'Martin', 'Jackson', 'Thompson', 'White',\n",
        "        'Lopez', 'Lee', 'Gonzalez', 'Harris', 'Clark', 'Lewis', 'Robinson',\n",
        "        'Walker', 'Perez', 'Hall', 'Young', 'Allen', 'Sanchez', 'Wright',\n",
        "        'King', 'Scott', 'Green', 'Baker', 'Adams', 'Nelson', 'Hill', 'Ramirez',\n",
        "        'Campbell', 'Mitchell', 'Roberts', 'Carter', 'Phillips', 'Evans', 'Turner'\n",
        "    ]\n",
        "\n",
        "    # Professional titles\n",
        "    executive_titles = [\n",
        "        'Chief Executive Officer', 'Chief Financial Officer', 'Chief Technology Officer',\n",
        "        'Chief Operating Officer', 'Managing Director', 'Executive Director',\n",
        "        'General Manager', 'Head of Finance', 'Head of Operations', 'Head of Sales',\n",
        "        'Director of Strategy', 'VP Technology', 'VP Business Development',\n",
        "        'Finance Director', 'Operations Director', 'Business Development Manager'\n",
        "    ]\n",
        "\n",
        "    senior_titles = [\n",
        "        'Senior Manager', 'Senior Business Analyst', 'Senior Project Manager',\n",
        "        'Head of Marketing', 'Head of HR', 'Head of Procurement', 'Head of Treasury',\n",
        "        'Senior Financial Analyst', 'Senior Operations Manager', 'Business Manager',\n",
        "        'Regional Manager', 'Area Manager', 'Branch Manager', 'Team Leader'\n",
        "    ]\n",
        "\n",
        "    mid_titles = [\n",
        "        'Business Analyst', 'Project Manager', 'Financial Analyst', 'Operations Manager',\n",
        "        'Marketing Manager', 'Sales Manager', 'Account Manager', 'Product Manager',\n",
        "        'Business Development Officer', 'Finance Manager', 'Operations Coordinator',\n",
        "        'Strategy Analyst', 'Investment Analyst', 'Risk Analyst', 'Compliance Officer'\n",
        "    ]\n",
        "\n",
        "    # Industries (business banking focus)\n",
        "    industries = [\n",
        "        'Management Consulting', 'Manufacturing', 'Professional Services',\n",
        "        'Investment Management', 'Construction', 'Technology', 'Real Estate',\n",
        "        'Real Estate Development', 'Venture Capital', 'Mining Services',\n",
        "        'Engineering Services', 'Financial Services', 'Healthcare Services',\n",
        "        'Transport & Logistics', 'Retail Trade', 'Wholesale Trade',\n",
        "        'Agriculture', 'Energy & Utilities', 'Telecommunications', 'Media'\n",
        "    ]\n",
        "\n",
        "    # Company sizes\n",
        "    company_sizes = ['11-50', '51-200', '101-250', '201-500', '251-500', '501-1000', '1001-5000', '5001-10000', '10001+']\n",
        "    company_size_weights = [0.25, 0.20, 0.15, 0.15, 0.10, 0.08, 0.05, 0.015, 0.005]\n",
        "\n",
        "    # Technologies\n",
        "    technologies = [\n",
        "        'Microsoft 365, Salesforce, QuickBooks', 'SAP, Oracle, Microsoft Teams',\n",
        "        'AWS, Kubernetes, Python', 'Bloomberg Terminal, Salesforce Financial',\n",
        "        'Procore, AutoCAD, SAP', 'Microsoft Azure, Power BI, Teams',\n",
        "        'Property management software, CRM', 'Salesforce, Tableau, Microsoft Office',\n",
        "        'ERP systems, supply chain management', 'Investment management platforms',\n",
        "        'Manufacturing execution systems', 'Treasury management systems'\n",
        "    ]\n",
        "\n",
        "    # Banking relationships\n",
        "    australian_banks = [\n",
        "        'Commonwealth Bank', 'Westpac', 'ANZ', 'NAB', 'Bendigo Bank',\n",
        "        'Macquarie Bank', 'Suncorp', 'Bank of Queensland', 'ING',\n",
        "        'Heritage Bank', 'Adelaide Bank', 'Great Southern Bank'\n",
        "    ]\n",
        "\n",
        "    # Generate base data structure\n",
        "    data = {}\n",
        "\n",
        "    print(\"Generating names and titles...\")\n",
        "\n",
        "    # Names and titles\n",
        "    data['First Name'] = np.random.choice(first_names, NUM_ROWS)\n",
        "    data['Last Name'] = np.random.choice(last_names, NUM_ROWS)\n",
        "\n",
        "    # Title generation based on seniority levels\n",
        "    title_categories = np.random.choice(['executive', 'senior', 'mid'], NUM_ROWS, p=[0.15, 0.35, 0.50])\n",
        "    titles = []\n",
        "    for category in title_categories:\n",
        "        if category == 'executive':\n",
        "            titles.append(random.choice(executive_titles))\n",
        "        elif category == 'senior':\n",
        "            titles.append(random.choice(senior_titles))\n",
        "        else:\n",
        "            titles.append(random.choice(mid_titles))\n",
        "    data['Title'] = titles\n",
        "\n",
        "    print(\"Generating company information...\")\n",
        "\n",
        "    # Company names\n",
        "    company_names = []\n",
        "    for _ in range(NUM_ROWS):\n",
        "        name1 = random.choice(business_names)\n",
        "        name2 = random.choice(business_types)\n",
        "        structure = random.choice(business_structures)\n",
        "        company_names.append(f\"{name1} {name2} {structure}\")\n",
        "    data['Company'] = company_names\n",
        "\n",
        "    # Contact information (limited availability as per LinkedIn reality)\n",
        "    emails = []\n",
        "    phones = []\n",
        "    for i in range(NUM_ROWS):\n",
        "        # Only 15% have email, 25% have phone (realistic LinkedIn limitation)\n",
        "        if random.random() < 0.15:\n",
        "            first = data['First Name'][i].lower()\n",
        "            last = data['Last Name'][i].lower()\n",
        "            domain = random.choice(['gmail.com', 'company.com.au', 'business.com.au'])\n",
        "            emails.append(f\"{first}.{last}@{domain}\")\n",
        "        else:\n",
        "            emails.append(\"\")\n",
        "\n",
        "        if random.random() < 0.25:\n",
        "            area_code = random.choice(['02', '03', '07', '08'])\n",
        "            number = ''.join([str(random.randint(0, 9)) for _ in range(8)])\n",
        "            phones.append(f\"+61 {area_code} {number[:4]} {number[4:]}\")\n",
        "        else:\n",
        "            phones.append(\"\")\n",
        "\n",
        "    data['Email'] = emails\n",
        "    data['Phone'] = phones\n",
        "\n",
        "    # LinkedIn URLs\n",
        "    linkedin_profiles = []\n",
        "    company_urls = []\n",
        "    sales_navigator_urls = []\n",
        "\n",
        "    for i in range(NUM_ROWS):\n",
        "        first = data['First Name'][i].lower().replace(' ', '-')\n",
        "        last = data['Last Name'][i].lower().replace(' ', '-')\n",
        "        company = data['Company'][i].lower().replace(' ', '-').replace('&', 'and')\n",
        "\n",
        "        linkedin_profiles.append(f\"https://www.linkedin.com/in/{first}-{last}-{random.randint(100, 999)}\")\n",
        "        company_urls.append(f\"https://www.linkedin.com/company/{company}\")\n",
        "        sales_navigator_urls.append(f\"https://www.linkedin.com/sales/lead/ACwAAB{random.choice(string.ascii_uppercase)}{random.randint(100, 999)}\")\n",
        "\n",
        "    data['LinkedIn Profile URL'] = linkedin_profiles\n",
        "    data['Current Company LinkedIn URL'] = company_urls\n",
        "    data['Sales Navigator Lead URL'] = sales_navigator_urls\n",
        "\n",
        "    print(\"Generating location and company details...\")\n",
        "\n",
        "    # Location data\n",
        "    # Calculate correct number of remaining cities for weights\n",
        "    num_remaining_cities = len(australian_cities) - 5\n",
        "    remaining_weight = 0.22 / num_remaining_cities\n",
        "    city_weights = [0.25, 0.20, 0.15, 0.10, 0.08] + [remaining_weight] * num_remaining_cities\n",
        "    data['Location'] = [f\"{city}, {random.choice(australian_states)}, Australia\"\n",
        "                       for city in np.random.choice(australian_cities, NUM_ROWS, p=city_weights)]\n",
        "\n",
        "    # Industry and company size\n",
        "    data['Industry'] = np.random.choice(industries, NUM_ROWS)\n",
        "    data['Company Size'] = np.random.choice(company_sizes, NUM_ROWS, p=company_size_weights)\n",
        "\n",
        "    print(\"Generating LinkedIn engagement metrics...\")\n",
        "\n",
        "    # LinkedIn engagement metrics\n",
        "    data['Connection Degree'] = np.random.choice(['1st', '2nd', '3rd'], NUM_ROWS, p=[0.20, 0.50, 0.30])\n",
        "    data['Shared Connections'] = np.random.randint(0, 50, NUM_ROWS)\n",
        "    data['Profile Views'] = np.random.randint(5, 100, NUM_ROWS)\n",
        "    data['Lead Score'] = np.random.randint(60, 100, NUM_ROWS)  # LinkedIn scores typically 60-100\n",
        "    data['Premium Account'] = np.random.choice(['Yes', 'No'], NUM_ROWS, p=[0.30, 0.70])\n",
        "\n",
        "    # Professional experience\n",
        "    data['Years at Company'] = np.round(np.random.exponential(2.5, NUM_ROWS), 1)\n",
        "    data['Total Experience'] = data['Years at Company'] + np.round(np.random.exponential(8, NUM_ROWS), 1)\n",
        "\n",
        "    # Education (simplified)\n",
        "    universities = [\n",
        "        'University of Melbourne MBA', 'University of Sydney Business', 'UNSW Engineering',\n",
        "        'Monash University Commerce', 'University of Queensland Business', 'UTS Finance',\n",
        "        'Griffith University Business', 'Deakin University MBA', 'Curtin University Commerce',\n",
        "        'Macquarie University Finance', 'Adelaide University Business', 'ANU Economics'\n",
        "    ]\n",
        "    data['Education'] = np.random.choice(universities, NUM_ROWS)\n",
        "\n",
        "    print(\"Generating business intelligence and activity data...\")\n",
        "\n",
        "    # Recent activity (key for banking opportunities)\n",
        "    activities = [\n",
        "        'Posted about cash flow challenges, seeking working capital solutions',\n",
        "        'Shared article about factory expansion plans, mentioned equipment upgrades',\n",
        "        'Updated experience with recent acquisition completion, hinting at next targets',\n",
        "        'Posted about merchant service fees hurting margins, comparing banking providers',\n",
        "        'Announced new fund launch, seeking banking partners for fund administration',\n",
        "        'Updated profile highlighting treasury transformation project, seeking technology solutions',\n",
        "        'Posted about digital transformation budget approval, fintech integration plans',\n",
        "        'Shared concerns about interest rate impacts, exploring refinancing options',\n",
        "        'Announced merger discussions with competitor, seeking M&A financing advice',\n",
        "        'Posted about rapid growth requiring additional banking facilities, scaling challenges',\n",
        "        'Updated with new property development approval, seeking construction finance',\n",
        "        'Shared article about supply chain financing challenges, working capital constraints',\n",
        "        'Posted about expanding into new markets, international banking requirements',\n",
        "        'Updated experience with ESG investment focus, seeking sustainable finance options',\n",
        "        'Announced successful exit, reinvesting proceeds, seeking private banking services',\n",
        "        'Posted about operational efficiency project, seeking technology financing',\n",
        "        'Shared concerns about client concentration risk, diversification financing needs',\n",
        "        'Updated profile with acquisition target identified, seeking acquisition finance',\n",
        "        'Posted about interest rate hedging strategies, derivatives requirements',\n",
        "        'Announced major export contract win, working capital for production ramp-up needed'\n",
        "    ]\n",
        "\n",
        "    # Generate recent activity with realistic timing\n",
        "    recent_activities = []\n",
        "    for _ in range(NUM_ROWS):\n",
        "        days_ago = random.randint(1, 14)\n",
        "        activity = random.choice(activities)\n",
        "        recent_activities.append(f\"{activity} - {days_ago} days ago\")\n",
        "\n",
        "    data['Last Activity'] = recent_activities\n",
        "\n",
        "    # Tags and notes (sales-specific)\n",
        "    tags_options = [\n",
        "        'Cash Flow Issues;Working Capital Need;High Priority',\n",
        "        'Manufacturing Growth;Equipment Finance;Expansion',\n",
        "        'M&A Activity;Acquisition Financing;Strategic Growth',\n",
        "        'Merchant Services;Banking Review;Cost Optimization',\n",
        "        'Fund Administration;Banking Partnership;Wealth Management',\n",
        "        'Treasury Management;Technology Upgrade;Cash Management',\n",
        "        'Digital Transformation;Fintech Integration;Technology Investment',\n",
        "        'Refinancing;Interest Rate Risk;Property Finance',\n",
        "        'Supply Chain Finance;Working Capital;Cash Flow',\n",
        "        'International Banking;Market Expansion;Foreign Exchange',\n",
        "        'ESG Finance;Sustainable Investing;Green Loans',\n",
        "        'Private Banking;Wealth Management;Investment Services'\n",
        "    ]\n",
        "\n",
        "    data['Tags'] = np.random.choice(tags_options, NUM_ROWS)\n",
        "\n",
        "    # Notes (contextual business intelligence)\n",
        "    notes_templates = [\n",
        "        'Recent posts indicate operational cash flow challenges, multiple consulting firm payments suggest restructuring costs',\n",
        "        'CEO actively discussing expansion plans, factory modernization, equipment needs for Q4',\n",
        "        'M&A specialist, recent acquisition completed, actively seeking next targets in Q3-Q4',\n",
        "        'CFO publicly discussing merchant service costs, comparing bank offerings, cost reduction focus',\n",
        "        'New fund launch requires banking services, fund administration, custody solutions',\n",
        "        'Treasury transformation underway, seeking modern cash management solutions, technology upgrade',\n",
        "        'CTO approved for digital transformation budget, fintech integration, API banking interest',\n",
        "        'Finance Director concerned about rate impacts, actively exploring refinancing options',\n",
        "        'CEO publicly announced merger discussions, actively seeking M&A financing and advisory services',\n",
        "        'Head of Finance indicating rapid growth, need for additional banking facilities, scaling support'\n",
        "    ]\n",
        "\n",
        "    data['Notes'] = np.random.choice(notes_templates, NUM_ROWS)\n",
        "\n",
        "    print(\"Generating sales tracking data...\")\n",
        "\n",
        "    # Sales tracking fields\n",
        "    date_added_list = []\n",
        "    last_contacted_list = []\n",
        "    for _ in range(NUM_ROWS):\n",
        "        # Date added (random over past 6 months)\n",
        "        days_back = random.randint(30, 180)\n",
        "        date_added = (datetime.now() - timedelta(days=days_back)).date()\n",
        "        date_added_list.append(date_added.strftime('%Y-%m-%d'))\n",
        "\n",
        "        # Last contacted (40% have been contacted)\n",
        "        if random.random() < 0.4:\n",
        "            contact_days_back = random.randint(1, days_back)\n",
        "            last_contact = (datetime.now() - timedelta(days=contact_days_back)).date()\n",
        "            last_contacted_list.append(last_contact.strftime('%Y-%m-%d'))\n",
        "        else:\n",
        "            last_contacted_list.append(\"\")\n",
        "\n",
        "    data['Date Added'] = date_added_list\n",
        "    data['Last Contacted'] = last_contacted_list\n",
        "\n",
        "    # Contact methods and responses\n",
        "    contact_methods = ['InMail', 'LinkedIn Message', 'Email', 'Phone Call', 'Connection Request']\n",
        "    response_statuses = [\n",
        "        'Responded - Interested', 'Responded - Not Interested', 'No Response',\n",
        "        'Accepted Connection', 'Declined Initial Contact', 'Scheduled Meeting',\n",
        "        'Follow-up Scheduled', 'Very Interested', 'Very Engaged', 'Opened - No Response'\n",
        "    ]\n",
        "\n",
        "    contact_method_list = []\n",
        "    response_status_list = []\n",
        "\n",
        "    for i in range(NUM_ROWS):\n",
        "        if data['Last Contacted'][i]:  # Only if they were contacted\n",
        "            contact_method_list.append(random.choice(contact_methods))\n",
        "            response_status_list.append(random.choice(response_statuses))\n",
        "        else:\n",
        "            contact_method_list.append(\"Not Contacted\")\n",
        "            response_status_list.append(\"\")\n",
        "\n",
        "    data['Contact Method'] = contact_method_list\n",
        "    data['Response Status'] = response_status_list\n",
        "\n",
        "    print(\"Generating financial and business intelligence...\")\n",
        "\n",
        "    # Financial and business intelligence\n",
        "    # Company revenue (correlated with company size)\n",
        "    revenues = []\n",
        "    for size in data['Company Size']:\n",
        "        if size in ['11-50']:\n",
        "            revenue = random.uniform(0.5, 5.0)\n",
        "        elif size in ['51-200', '101-250']:\n",
        "            revenue = random.uniform(3.0, 20.0)\n",
        "        elif size in ['201-500', '251-500']:\n",
        "            revenue = random.uniform(15.0, 80.0)\n",
        "        elif size in ['501-1000']:\n",
        "            revenue = random.uniform(50.0, 200.0)\n",
        "        else:\n",
        "            revenue = random.uniform(100.0, 1000.0)\n",
        "        revenues.append(f\"${revenue:.1f}M\")\n",
        "\n",
        "    data['Company Revenue'] = revenues\n",
        "\n",
        "    # Financial health score (60-95)\n",
        "    data['Financial Health Score'] = np.random.randint(60, 96, NUM_ROWS)\n",
        "\n",
        "    # Recent funding\n",
        "    funding_options = ['None', 'Series A funding', 'Series B funding', 'Equipment loan',\n",
        "                      'Private equity backing', 'Bank facilities', 'Export contract secured']\n",
        "    funding_weights = [0.6, 0.1, 0.05, 0.1, 0.05, 0.05, 0.05]\n",
        "    data['Recent Funding'] = np.random.choice(funding_options, NUM_ROWS, p=funding_weights)\n",
        "\n",
        "    # M&A activity\n",
        "    ma_options = ['None', 'Acquisition discussions', 'Merger talks active',\n",
        "                 'Target identified', 'Due diligence active']\n",
        "    ma_weights = [0.8, 0.08, 0.05, 0.04, 0.03]\n",
        "    data['Merger Activity'] = np.random.choice(ma_options, NUM_ROWS, p=ma_weights)\n",
        "\n",
        "    # Competitor banks used (2 banks per company typically)\n",
        "    competitor_banks = []\n",
        "    for _ in range(NUM_ROWS):\n",
        "        primary_bank = random.choice(australian_banks)\n",
        "        secondary_bank = random.choice([b for b in australian_banks if b != primary_bank])\n",
        "        competitor_banks.append(f\"{primary_bank};{secondary_bank}\")\n",
        "\n",
        "    data['Competitor Banks Used'] = competitor_banks\n",
        "\n",
        "    # Technology adoption\n",
        "    tech_levels = ['Basic Technology', 'Modern Tech Stack', 'Legacy Systems', 'Advanced Technology']\n",
        "    data['Technology Adoption'] = np.random.choice(tech_levels, NUM_ROWS, p=[0.4, 0.3, 0.2, 0.1])\n",
        "\n",
        "    # Industry trends\n",
        "    trends = [\n",
        "        'Professional Services Consolidation', 'Manufacturing Automation Growth',\n",
        "        'Fintech Integration Accelerating', 'ESG Investment Focus Growing',\n",
        "        'Supply Chain Digitization', 'Construction Technology Adoption',\n",
        "        'Real Estate Market Transformation', 'Venture Capital Activity',\n",
        "        'Export Market Opportunities', 'Interest Rate Risk Management'\n",
        "    ]\n",
        "    data['Industry Trends'] = np.random.choice(trends, NUM_ROWS)\n",
        "\n",
        "    # Key business signals (opportunities)\n",
        "    business_signals = [\n",
        "        'Cash flow stress from project delays', 'Large equipment purchases planned',\n",
        "        'Multiple acquisition discussions', 'Merchant service cost concerns',\n",
        "        'New fund administration needs', 'Treasury system modernization',\n",
        "        'Fintech partnership opportunities', 'Refinancing discussions active',\n",
        "        'Merger financing required', 'Rapid scaling requiring banking support',\n",
        "        'Major construction project financing', 'Working capital financing needs',\n",
        "        'International banking needs', 'Green finance requirements',\n",
        "        'High net worth banking needs', 'Technology upgrade financing'\n",
        "    ]\n",
        "    data['Key Business Signals'] = np.random.choice(business_signals, NUM_ROWS)\n",
        "\n",
        "    # Opportunity types (banking products)\n",
        "    opportunity_types = [\n",
        "        'Equipment Finance', 'Business Loan', 'M&A Financing', 'Merchant Services',\n",
        "        'Wealth Solutions', 'Transaction Banking', 'Digital Banking', 'Property Finance',\n",
        "        'Supply Chain Finance', 'International Banking', 'Sustainable Finance',\n",
        "        'Private Banking', 'Construction Finance', 'Export Finance', 'Working Capital',\n",
        "        'Treasury Solutions', 'Growth Capital', 'Acquisition Finance'\n",
        "    ]\n",
        "    data['Opportunity Type'] = np.random.choice(opportunity_types, NUM_ROWS)\n",
        "\n",
        "    # Risk indicators\n",
        "    risk_levels = ['Low', 'Medium', 'High']\n",
        "    risk_descriptions = [\n",
        "        'Strong financials', 'Cash flow constraints', 'Interest rate exposure',\n",
        "        'Solid business', 'Supply chain pressures', 'Technology transition',\n",
        "        'Project-dependent revenue', 'Market concentration', 'Expansion focused',\n",
        "        'Operational transformation', 'Regulatory compliance', 'Market volatility'\n",
        "    ]\n",
        "\n",
        "    risk_indicators = []\n",
        "    for _ in range(NUM_ROWS):\n",
        "        level = random.choice(risk_levels)\n",
        "        description = random.choice(risk_descriptions)\n",
        "        risk_indicators.append(f\"{level} - {description}\")\n",
        "\n",
        "    data['Risk Indicators'] = risk_indicators\n",
        "\n",
        "    print(\"Creating DataFrame...\")\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    print(f\"Dataset generation complete! Generated {len(df):,} rows with {len(df.columns)} columns.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to generate LinkedIn Sales Navigator dataset\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"LINKEDIN SALES NAVIGATOR DATA GENERATOR\")\n",
        "    print(\"Banking Customer Intelligence Export\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Generate the dataset\n",
        "    df = generate_linkedin_navigator_data()\n",
        "\n",
        "    # Display basic information\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"DATASET SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Total Rows: {len(df):,}\")\n",
        "    print(f\"Total Columns: {len(df.columns)}\")\n",
        "    print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "\n",
        "    print(\"\\nColumn Information:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, col in enumerate(df.columns, 1):\n",
        "        dtype = str(df[col].dtype)\n",
        "        null_count = df[col].isnull().sum()\n",
        "        print(f\"{i:2d}. {col:<35} {dtype:<15} ({null_count:,} nulls)\")\n",
        "\n",
        "    print(\"\\nSample Data (first 5 rows):\")\n",
        "    print(\"-\" * 50)\n",
        "    print(df.head())\n",
        "\n",
        "    print(\"\\nKey Metrics Summary:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Connection Degrees: {df['Connection Degree'].value_counts().to_dict()}\")\n",
        "    print(f\"Company Sizes: {df['Company Size'].value_counts().head().to_dict()}\")\n",
        "    print(f\"Industries: {df['Industry'].value_counts().head().to_dict()}\")\n",
        "    print(f\"Opportunity Types: {df['Opportunity Type'].value_counts().head().to_dict()}\")\n",
        "\n",
        "    # Save to CSV\n",
        "    csv_filename = 'linkedin_sales_navigator_banking_150k.csv'\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"\\n📁 Dataset saved as: {csv_filename}\")\n",
        "\n",
        "    # Download in Colab\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(csv_filename)\n",
        "        print(\"📥 File download initiated in Colab\")\n",
        "    except ImportError:\n",
        "        print(\"Note: Not running in Colab - file saved locally\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"LINKEDIN SALES NAVIGATOR DATA GENERATION COMPLETE\")\n",
        "    print(\"Ready for Banking Intelligence Analysis\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    df = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZvyxxXSzZ9oy",
        "outputId": "37be9d1f-b4a8-4654-bebc-8a40b2d4a784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "LINKEDIN SALES NAVIGATOR DATA GENERATOR\n",
            "Banking Customer Intelligence Export\n",
            "================================================================================\n",
            "Generating 150,000 rows of LinkedIn Sales Navigator data...\n",
            "Generating names and titles...\n",
            "Generating company information...\n",
            "Generating location and company details...\n",
            "Generating LinkedIn engagement metrics...\n",
            "Generating business intelligence and activity data...\n",
            "Generating sales tracking data...\n",
            "Generating financial and business intelligence...\n",
            "Creating DataFrame...\n",
            "Dataset generation complete! Generated 150,000 rows with 37 columns.\n",
            "\n",
            "================================================================================\n",
            "DATASET SUMMARY\n",
            "================================================================================\n",
            "Total Rows: 150,000\n",
            "Total Columns: 37\n",
            "Memory Usage: 363.2 MB\n",
            "\n",
            "Column Information:\n",
            "--------------------------------------------------\n",
            " 1. First Name                          object          (0 nulls)\n",
            " 2. Last Name                           object          (0 nulls)\n",
            " 3. Title                               object          (0 nulls)\n",
            " 4. Company                             object          (0 nulls)\n",
            " 5. Email                               object          (0 nulls)\n",
            " 6. Phone                               object          (0 nulls)\n",
            " 7. LinkedIn Profile URL                object          (0 nulls)\n",
            " 8. Current Company LinkedIn URL        object          (0 nulls)\n",
            " 9. Sales Navigator Lead URL            object          (0 nulls)\n",
            "10. Location                            object          (0 nulls)\n",
            "11. Industry                            object          (0 nulls)\n",
            "12. Company Size                        object          (0 nulls)\n",
            "13. Connection Degree                   object          (0 nulls)\n",
            "14. Shared Connections                  int64           (0 nulls)\n",
            "15. Profile Views                       int64           (0 nulls)\n",
            "16. Lead Score                          int64           (0 nulls)\n",
            "17. Premium Account                     object          (0 nulls)\n",
            "18. Years at Company                    float64         (0 nulls)\n",
            "19. Total Experience                    float64         (0 nulls)\n",
            "20. Education                           object          (0 nulls)\n",
            "21. Last Activity                       object          (0 nulls)\n",
            "22. Tags                                object          (0 nulls)\n",
            "23. Notes                               object          (0 nulls)\n",
            "24. Date Added                          object          (0 nulls)\n",
            "25. Last Contacted                      object          (0 nulls)\n",
            "26. Contact Method                      object          (0 nulls)\n",
            "27. Response Status                     object          (0 nulls)\n",
            "28. Company Revenue                     object          (0 nulls)\n",
            "29. Financial Health Score              int64           (0 nulls)\n",
            "30. Recent Funding                      object          (0 nulls)\n",
            "31. Merger Activity                     object          (0 nulls)\n",
            "32. Competitor Banks Used               object          (0 nulls)\n",
            "33. Technology Adoption                 object          (0 nulls)\n",
            "34. Industry Trends                     object          (0 nulls)\n",
            "35. Key Business Signals                object          (0 nulls)\n",
            "36. Opportunity Type                    object          (0 nulls)\n",
            "37. Risk Indicators                     object          (0 nulls)\n",
            "\n",
            "Sample Data (first 5 rows):\n",
            "--------------------------------------------------\n",
            "  First Name Last Name                    Title  \\\n",
            "0   Benjamin   Jackson  Chief Operating Officer   \n",
            "1       Adam  Gonzalez           Senior Manager   \n",
            "2       Ryan   Jackson             Area Manager   \n",
            "3      Simon     Green        Marketing Manager   \n",
            "4     Rachel     Moore        Head of Marketing   \n",
            "\n",
            "                             Company                         Email  \\\n",
            "0  Strategic Enterprises Corporation                                 \n",
            "1       Royal Industries Partnership                                 \n",
            "2              Modern Holdings Trust                                 \n",
            "3        Innovation Partners Pty Ltd                                 \n",
            "4               Modern Group Pty Ltd  rachel.moore@business.com.au   \n",
            "\n",
            "              Phone                              LinkedIn Profile URL  \\\n",
            "0                    https://www.linkedin.com/in/benjamin-jackson-745   \n",
            "1                       https://www.linkedin.com/in/adam-gonzalez-374   \n",
            "2  +61 03 5959 7501      https://www.linkedin.com/in/ryan-jackson-203   \n",
            "3                         https://www.linkedin.com/in/simon-green-763   \n",
            "4                        https://www.linkedin.com/in/rachel-moore-842   \n",
            "\n",
            "                        Current Company LinkedIn URL  \\\n",
            "0  https://www.linkedin.com/company/strategic-ent...   \n",
            "1  https://www.linkedin.com/company/royal-industr...   \n",
            "2  https://www.linkedin.com/company/modern-holdin...   \n",
            "3  https://www.linkedin.com/company/innovation-pa...   \n",
            "4  https://www.linkedin.com/company/modern-group-...   \n",
            "\n",
            "                         Sales Navigator Lead URL                  Location  \\\n",
            "0  https://www.linkedin.com/sales/lead/ACwAABU945  Melbourne, NT, Australia   \n",
            "1  https://www.linkedin.com/sales/lead/ACwAABL543  Brisbane, ACT, Australia   \n",
            "2  https://www.linkedin.com/sales/lead/ACwAABT608  Canberra, VIC, Australia   \n",
            "3  https://www.linkedin.com/sales/lead/ACwAABZ349    Sydney, ACT, Australia   \n",
            "4  https://www.linkedin.com/sales/lead/ACwAABC529    Sydney, VIC, Australia   \n",
            "\n",
            "   ... Company Revenue Financial Health Score           Recent Funding  \\\n",
            "0  ...           $4.8M                     60                     None   \n",
            "1  ...          $22.9M                     60                     None   \n",
            "2  ...           $4.6M                     86           Equipment loan   \n",
            "3  ...          $57.3M                     72                     None   \n",
            "4  ...           $4.0M                     72  Export contract secured   \n",
            "\n",
            "   Merger Activity                 Competitor Banks Used  Technology Adoption  \\\n",
            "0             None          Adelaide Bank;Macquarie Bank       Legacy Systems   \n",
            "1             None                     ANZ;Heritage Bank     Basic Technology   \n",
            "2             None                Bank of Queensland;NAB       Legacy Systems   \n",
            "3             None  Commonwealth Bank;Bank of Queensland    Modern Tech Stack   \n",
            "4             None                     ANZ;Heritage Bank       Legacy Systems   \n",
            "\n",
            "                     Industry Trends               Key Business Signals  \\\n",
            "0  Real Estate Market Transformation      Treasury system modernization   \n",
            "1   Construction Technology Adoption    Working capital financing needs   \n",
            "2        Export Market Opportunities          Merger financing required   \n",
            "3   Construction Technology Adoption   Multiple acquisition discussions   \n",
            "4    Manufacturing Automation Growth  Large equipment purchases planned   \n",
            "\n",
            "        Opportunity Type                    Risk Indicators  \n",
            "0  International Banking  High - Operational transformation  \n",
            "1         Growth Capital        High - Market concentration  \n",
            "2  International Banking    Medium - Interest rate exposure  \n",
            "3         Export Finance           High - Market volatility  \n",
            "4     Treasury Solutions  High - Operational transformation  \n",
            "\n",
            "[5 rows x 37 columns]\n",
            "\n",
            "Key Metrics Summary:\n",
            "--------------------------------------------------\n",
            "Connection Degrees: {'2nd': 74684, '3rd': 45391, '1st': 29925}\n",
            "Company Sizes: {'11-50': 37176, '51-200': 30199, '201-500': 22549, '101-250': 22477, '251-500': 14917}\n",
            "Industries: {'Venture Capital': 7684, 'Wholesale Trade': 7626, 'Professional Services': 7597, 'Construction': 7592, 'Management Consulting': 7546}\n",
            "Opportunity Types: {'Export Finance': 8458, 'Sustainable Finance': 8433, 'Merchant Services': 8421, 'Working Capital': 8408, 'Transaction Banking': 8406}\n",
            "\n",
            "📁 Dataset saved as: linkedin_sales_navigator_banking_150k.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_209deeeb-63de-44bf-ab4f-e3b1730b80f2\", \"linkedin_sales_navigator_banking_150k.csv\", 117555594)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 File download initiated in Colab\n",
            "\n",
            "================================================================================\n",
            "LINKEDIN SALES NAVIGATOR DATA GENERATION COMPLETE\n",
            "Ready for Banking Intelligence Analysis\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime, timedelta, date\n",
        "import string\n",
        "import warnings\n",
        "import os\n",
        "import json\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Configuration parameters - easily adjustable\n",
        "NUM_ROWS = 150000\n",
        "START_DATE = datetime(2023, 1, 1)\n",
        "END_DATE = datetime(2024, 6, 8)\n",
        "\n",
        "# BigQuery configuration\n",
        "PROJECT_ID = \"ehc-ldioneda-1d3210\"\n",
        "DATASET_ID = \"larry_demo_datasets\"\n",
        "TABLE_ID = \"linkedin_navigator_bb_sales_portfolio_demo\"\n",
        "FULL_TABLE_ID = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
        "\n",
        "def setup_colab_authentication():\n",
        "    \"\"\"\n",
        "    Setup authentication in Google Colab environment\n",
        "    \"\"\"\n",
        "    print(\"Setting up Google Colab authentication...\")\n",
        "\n",
        "    try:\n",
        "        # Method 1: Try user authentication first\n",
        "        from google.colab import auth\n",
        "        print(\"🔑 Attempting user authentication...\")\n",
        "        auth.authenticate_user()\n",
        "        print(\"✅ User authentication successful!\")\n",
        "        return \"user\"\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  User authentication failed: {e}\")\n",
        "\n",
        "    # Method 2: Service account upload\n",
        "    print(\"\\n📁 Please upload your service account JSON file using the method below:\")\n",
        "    print(\"1. Click the 'Choose Files' button that appears\")\n",
        "    print(\"2. Select your service account JSON file\")\n",
        "    print(\"3. Wait for upload to complete\")\n",
        "\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        if uploaded:\n",
        "            # Get the first uploaded file\n",
        "            filename = list(uploaded.keys())[0]\n",
        "            print(f\"📄 Uploaded file: {filename}\")\n",
        "\n",
        "            # Set environment variable\n",
        "            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = filename\n",
        "            print(\"✅ Service account credentials configured!\")\n",
        "            return \"service_account\"\n",
        "        else:\n",
        "            print(\"❌ No file uploaded\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ File upload failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def setup_bigquery_client():\n",
        "    \"\"\"\n",
        "    Setup BigQuery client for Google Colab\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from google.cloud import bigquery\n",
        "        from google.cloud.exceptions import NotFound\n",
        "        from google.auth.exceptions import DefaultCredentialsError\n",
        "\n",
        "        print(f\"Setting up BigQuery client for project: {PROJECT_ID}\")\n",
        "\n",
        "        # Try to create client\n",
        "        client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "        # Test the connection\n",
        "        try:\n",
        "            dataset = client.get_dataset(DATASET_ID)\n",
        "            print(f\"✅ Successfully connected to dataset: {DATASET_ID}\")\n",
        "            return client, bigquery, NotFound\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Dataset access test failed: {e}\")\n",
        "            print(\"Note: This might be normal if the dataset exists but has restricted access\")\n",
        "            return client, bigquery, NotFound\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"❌ google-cloud-bigquery not installed. Installing now...\")\n",
        "        !pip install google-cloud-bigquery\n",
        "\n",
        "        from google.cloud import bigquery\n",
        "        from google.cloud.exceptions import NotFound\n",
        "        client = bigquery.Client(project=PROJECT_ID)\n",
        "        return client, bigquery, NotFound\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error setting up BigQuery client: {e}\")\n",
        "        print(\"\\nTrying alternative authentication...\")\n",
        "        return None, None, None\n",
        "\n",
        "def create_bigquery_table_schema():\n",
        "    \"\"\"\n",
        "    Define BigQuery table schema for LinkedIn Sales Navigator data\n",
        "    \"\"\"\n",
        "    # Import here in case it's not available at module level\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"first_name\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"last_name\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"title\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"company\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"email\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"phone\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"linkedin_profile_url\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"location\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"industry\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"company_size\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"connection_degree\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"shared_connections\", \"INTEGER\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"last_activity\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"profile_views\", \"INTEGER\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"lead_score\", \"INTEGER\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"premium_account\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"years_at_company\", \"FLOAT64\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"total_experience\", \"FLOAT64\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"education\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"current_company_linkedin_url\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"sales_navigator_lead_url\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"tags\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"notes\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"date_added\", \"DATE\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"last_contacted\", \"DATE\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"contact_method\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"response_status\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"company_revenue\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"financial_health_score\", \"INTEGER\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"recent_funding\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"merger_activity\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"competitor_banks_used\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"technology_adoption\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"industry_trends\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"key_business_signals\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"opportunity_type\", \"STRING\", mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"risk_indicators\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    ]\n",
        "    return schema\n",
        "\n",
        "def create_or_update_bigquery_table(client, table_id, schema, bigquery, NotFound):\n",
        "    \"\"\"\n",
        "    Create BigQuery table if it doesn't exist, or update schema if it does\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if table exists\n",
        "        table = client.get_table(table_id)\n",
        "        print(f\"Table {table_id} already exists. Checking schema...\")\n",
        "\n",
        "        # Check if schema needs updating\n",
        "        current_schema = table.schema\n",
        "        if len(current_schema) != len(schema):\n",
        "            print(\"Schema differs. Updating table schema...\")\n",
        "            table.schema = schema\n",
        "            table = client.update_table(table, [\"schema\"])\n",
        "            print(\"Table schema updated successfully.\")\n",
        "        else:\n",
        "            print(\"Table schema is up to date.\")\n",
        "\n",
        "    except NotFound:\n",
        "        # Table doesn't exist, create it\n",
        "        print(f\"Creating table {table_id}...\")\n",
        "        table = bigquery.Table(table_id, schema=schema)\n",
        "        table = client.create_table(table)\n",
        "        print(f\"Table {table_id} created successfully.\")\n",
        "\n",
        "    return table\n",
        "\n",
        "def upload_to_bigquery(df, client, table_id, bigquery, write_disposition=\"WRITE_TRUNCATE\"):\n",
        "    \"\"\"\n",
        "    Upload DataFrame to BigQuery table\n",
        "    \"\"\"\n",
        "    # Configure the job\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        write_disposition=write_disposition,\n",
        "        autodetect=False,  # Use our explicit schema\n",
        "        schema=create_bigquery_table_schema()\n",
        "    )\n",
        "\n",
        "    print(f\"Starting upload to {table_id}...\")\n",
        "    print(f\"Upload mode: {write_disposition}\")\n",
        "    print(f\"Rows to upload: {len(df):,}\")\n",
        "\n",
        "    # Start the job\n",
        "    job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
        "\n",
        "    # Wait for the job to complete\n",
        "    try:\n",
        "        job.result()  # Wait for job to complete\n",
        "        print(f\"✅ Successfully uploaded {len(df):,} rows to {table_id}\")\n",
        "\n",
        "        # Get table info\n",
        "        table = client.get_table(table_id)\n",
        "        print(f\"📊 Table now contains {table.num_rows:,} total rows\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error uploading to BigQuery: {e}\")\n",
        "        if hasattr(job, 'errors') and job.errors:\n",
        "            for error in job.errors:\n",
        "                print(f\"Job error: {error}\")\n",
        "        return False\n",
        "\n",
        "def generate_linkedin_navigator_data():\n",
        "    \"\"\"\n",
        "    Generate comprehensive LinkedIn Sales Navigator dataset for banking customers\n",
        "    Returns: pandas DataFrame with 150,000 rows and 37 columns\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Generating {NUM_ROWS:,} rows of LinkedIn Sales Navigator data...\")\n",
        "\n",
        "    # Australian business context data\n",
        "    australian_states = ['VIC', 'NSW', 'QLD', 'WA', 'SA', 'TAS', 'NT', 'ACT']\n",
        "\n",
        "    australian_cities = [\n",
        "        'Melbourne', 'Sydney', 'Brisbane', 'Perth', 'Adelaide', 'Gold Coast',\n",
        "        'Newcastle', 'Canberra', 'Sunshine Coast', 'Wollongong', 'Hobart',\n",
        "        'Geelong', 'Townsville', 'Cairns', 'Darwin', 'Toowoomba', 'Ballarat',\n",
        "        'Bendigo', 'Albury', 'Launceston', 'Mackay', 'Rockhampton', 'Bunbury'\n",
        "    ]\n",
        "\n",
        "    # Business naming components\n",
        "    business_names = [\n",
        "        'Superior', 'Golden', 'Progressive', 'Innovation', 'Elite', 'Quality',\n",
        "        'Modern', 'Heritage', 'Excellence', 'Future', 'Advanced', 'Strategic',\n",
        "        'Dynamic', 'Professional', 'Premium', 'Global', 'National', 'Regional',\n",
        "        'Premier', 'Leading', 'Innovative', 'Specialist', 'Expert', 'Prime',\n",
        "        'Apex', 'Pinnacle', 'Summit', 'Crown', 'Royal', 'Imperial'\n",
        "    ]\n",
        "\n",
        "    business_types = [\n",
        "        'Consulting', 'Manufacturing', 'Services', 'Industries', 'Holdings',\n",
        "        'Group', 'Enterprises', 'Solutions', 'Capital', 'Ventures',\n",
        "        'Development', 'Associates', 'Partners', 'Corporation', 'Company'\n",
        "    ]\n",
        "\n",
        "    business_structures = ['Pty Ltd', 'Limited', 'Partnership', 'Trust', 'Corporation']\n",
        "\n",
        "    # Professional names (Australian context)\n",
        "    first_names = [\n",
        "        'Michael', 'Sarah', 'David', 'Emma', 'James', 'Lisa', 'Andrew', 'Rachel',\n",
        "        'Daniel', 'Jennifer', 'Matthew', 'Amanda', 'Christopher', 'Nicole', 'Ryan',\n",
        "        'Rebecca', 'Mark', 'Jessica', 'Peter', 'Michelle', 'Anthony', 'Stephanie',\n",
        "        'Paul', 'Laura', 'Steven', 'Catherine', 'John', 'Karen', 'Adam', 'Susan',\n",
        "        'Kevin', 'Helen', 'Robert', 'Anna', 'Timothy', 'Natalie', 'Jason', 'Claire',\n",
        "        'Benjamin', 'Samantha', 'Nathan', 'Louise', 'Simon', 'Victoria', 'Luke', 'Tracy'\n",
        "    ]\n",
        "\n",
        "    last_names = [\n",
        "        'Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Miller', 'Davis',\n",
        "        'Garcia', 'Rodriguez', 'Wilson', 'Martinez', 'Anderson', 'Taylor',\n",
        "        'Thomas', 'Hernandez', 'Moore', 'Martin', 'Jackson', 'Thompson', 'White',\n",
        "        'Lopez', 'Lee', 'Gonzalez', 'Harris', 'Clark', 'Lewis', 'Robinson',\n",
        "        'Walker', 'Perez', 'Hall', 'Young', 'Allen', 'Sanchez', 'Wright',\n",
        "        'King', 'Scott', 'Green', 'Baker', 'Adams', 'Nelson', 'Hill', 'Ramirez',\n",
        "        'Campbell', 'Mitchell', 'Roberts', 'Carter', 'Phillips', 'Evans', 'Turner'\n",
        "    ]\n",
        "\n",
        "    # Professional titles\n",
        "    executive_titles = [\n",
        "        'Chief Executive Officer', 'Chief Financial Officer', 'Chief Technology Officer',\n",
        "        'Chief Operating Officer', 'Managing Director', 'Executive Director',\n",
        "        'General Manager', 'Head of Finance', 'Head of Operations', 'Head of Sales',\n",
        "        'Director of Strategy', 'VP Technology', 'VP Business Development',\n",
        "        'Finance Director', 'Operations Director', 'Business Development Manager'\n",
        "    ]\n",
        "\n",
        "    senior_titles = [\n",
        "        'Senior Manager', 'Senior Business Analyst', 'Senior Project Manager',\n",
        "        'Head of Marketing', 'Head of HR', 'Head of Procurement', 'Head of Treasury',\n",
        "        'Senior Financial Analyst', 'Senior Operations Manager', 'Business Manager',\n",
        "        'Regional Manager', 'Area Manager', 'Branch Manager', 'Team Leader'\n",
        "    ]\n",
        "\n",
        "    mid_titles = [\n",
        "        'Business Analyst', 'Project Manager', 'Financial Analyst', 'Operations Manager',\n",
        "        'Marketing Manager', 'Sales Manager', 'Account Manager', 'Product Manager',\n",
        "        'Business Development Officer', 'Finance Manager', 'Operations Coordinator',\n",
        "        'Strategy Analyst', 'Investment Analyst', 'Risk Analyst', 'Compliance Officer'\n",
        "    ]\n",
        "\n",
        "    # Industries (business banking focus)\n",
        "    industries = [\n",
        "        'Management Consulting', 'Manufacturing', 'Professional Services',\n",
        "        'Investment Management', 'Construction', 'Technology', 'Real Estate',\n",
        "        'Real Estate Development', 'Venture Capital', 'Mining Services',\n",
        "        'Engineering Services', 'Financial Services', 'Healthcare Services',\n",
        "        'Transport & Logistics', 'Retail Trade', 'Wholesale Trade',\n",
        "        'Agriculture', 'Energy & Utilities', 'Telecommunications', 'Media'\n",
        "    ]\n",
        "\n",
        "    # Company sizes\n",
        "    company_sizes = ['11-50', '51-200', '101-250', '201-500', '251-500', '501-1000', '1001-5000', '5001-10000', '10001+']\n",
        "    company_size_weights = [0.25, 0.20, 0.15, 0.15, 0.10, 0.08, 0.05, 0.015, 0.005]\n",
        "\n",
        "    # Australian banks\n",
        "    australian_banks = [\n",
        "        'Commonwealth Bank', 'Westpac', 'ANZ', 'NAB', 'Bendigo Bank',\n",
        "        'Macquarie Bank', 'Suncorp', 'Bank of Queensland', 'ING',\n",
        "        'Heritage Bank', 'Adelaide Bank', 'Great Southern Bank'\n",
        "    ]\n",
        "\n",
        "    # Generate base data structure\n",
        "    data = {}\n",
        "\n",
        "    print(\"Generating names and titles...\")\n",
        "\n",
        "    # Names and titles\n",
        "    data['first_name'] = np.random.choice(first_names, NUM_ROWS)\n",
        "    data['last_name'] = np.random.choice(last_names, NUM_ROWS)\n",
        "\n",
        "    # Title generation based on seniority levels\n",
        "    title_categories = np.random.choice(['executive', 'senior', 'mid'], NUM_ROWS, p=[0.15, 0.35, 0.50])\n",
        "    titles = []\n",
        "    for category in title_categories:\n",
        "        if category == 'executive':\n",
        "            titles.append(random.choice(executive_titles))\n",
        "        elif category == 'senior':\n",
        "            titles.append(random.choice(senior_titles))\n",
        "        else:\n",
        "            titles.append(random.choice(mid_titles))\n",
        "    data['title'] = titles\n",
        "\n",
        "    print(\"Generating company information...\")\n",
        "\n",
        "    # Company names\n",
        "    company_names = []\n",
        "    for _ in range(NUM_ROWS):\n",
        "        name1 = random.choice(business_names)\n",
        "        name2 = random.choice(business_types)\n",
        "        structure = random.choice(business_structures)\n",
        "        company_names.append(f\"{name1} {name2} {structure}\")\n",
        "    data['company'] = company_names\n",
        "\n",
        "    # Contact information (limited availability as per LinkedIn reality)\n",
        "    emails = []\n",
        "    phones = []\n",
        "    for i in range(NUM_ROWS):\n",
        "        # Only 15% have email, 25% have phone (realistic LinkedIn limitation)\n",
        "        if random.random() < 0.15:\n",
        "            first = data['first_name'][i].lower()\n",
        "            last = data['last_name'][i].lower()\n",
        "            domain = random.choice(['gmail.com', 'company.com.au', 'business.com.au'])\n",
        "            emails.append(f\"{first}.{last}@{domain}\")\n",
        "        else:\n",
        "            emails.append(\"\")\n",
        "\n",
        "        if random.random() < 0.25:\n",
        "            area_code = random.choice(['02', '03', '07', '08'])\n",
        "            number = ''.join([str(random.randint(0, 9)) for _ in range(8)])\n",
        "            phones.append(f\"+61 {area_code} {number[:4]} {number[4:]}\")\n",
        "        else:\n",
        "            phones.append(\"\")\n",
        "\n",
        "    data['email'] = emails\n",
        "    data['phone'] = phones\n",
        "\n",
        "    # LinkedIn URLs\n",
        "    linkedin_profiles = []\n",
        "    company_urls = []\n",
        "    sales_navigator_urls = []\n",
        "\n",
        "    for i in range(NUM_ROWS):\n",
        "        first = data['first_name'][i].lower().replace(' ', '-')\n",
        "        last = data['last_name'][i].lower().replace(' ', '-')\n",
        "        company = data['company'][i].lower().replace(' ', '-').replace('&', 'and')\n",
        "\n",
        "        linkedin_profiles.append(f\"https://www.linkedin.com/in/{first}-{last}-{random.randint(100, 999)}\")\n",
        "        company_urls.append(f\"https://www.linkedin.com/company/{company}\")\n",
        "        sales_navigator_urls.append(f\"https://www.linkedin.com/sales/lead/ACwAAB{random.choice(string.ascii_uppercase)}{random.randint(100, 999)}\")\n",
        "\n",
        "    data['linkedin_profile_url'] = linkedin_profiles\n",
        "    data['current_company_linkedin_url'] = company_urls\n",
        "    data['sales_navigator_lead_url'] = sales_navigator_urls\n",
        "\n",
        "    print(\"Generating location and company details...\")\n",
        "\n",
        "    # Location data\n",
        "    num_remaining_cities = len(australian_cities) - 5\n",
        "    remaining_weight = 0.22 / num_remaining_cities\n",
        "    city_weights = [0.25, 0.20, 0.15, 0.10, 0.08] + [remaining_weight] * num_remaining_cities\n",
        "    data['location'] = [f\"{city}, {random.choice(australian_states)}, Australia\"\n",
        "                       for city in np.random.choice(australian_cities, NUM_ROWS, p=city_weights)]\n",
        "\n",
        "    # Industry and company size\n",
        "    data['industry'] = np.random.choice(industries, NUM_ROWS)\n",
        "    data['company_size'] = np.random.choice(company_sizes, NUM_ROWS, p=company_size_weights)\n",
        "\n",
        "    print(\"Generating LinkedIn engagement metrics...\")\n",
        "\n",
        "    # LinkedIn engagement metrics\n",
        "    data['connection_degree'] = np.random.choice(['1st', '2nd', '3rd'], NUM_ROWS, p=[0.20, 0.50, 0.30])\n",
        "    data['shared_connections'] = np.random.randint(0, 50, NUM_ROWS)\n",
        "    data['profile_views'] = np.random.randint(5, 100, NUM_ROWS)\n",
        "    data['lead_score'] = np.random.randint(60, 100, NUM_ROWS)  # LinkedIn scores typically 60-100\n",
        "    data['premium_account'] = np.random.choice(['Yes', 'No'], NUM_ROWS, p=[0.30, 0.70])\n",
        "\n",
        "    # Professional experience\n",
        "    data['years_at_company'] = np.round(np.random.exponential(2.5, NUM_ROWS), 1)\n",
        "    data['total_experience'] = data['years_at_company'] + np.round(np.random.exponential(8, NUM_ROWS), 1)\n",
        "\n",
        "    # Education (simplified)\n",
        "    universities = [\n",
        "        'University of Melbourne MBA', 'University of Sydney Business', 'UNSW Engineering',\n",
        "        'Monash University Commerce', 'University of Queensland Business', 'UTS Finance',\n",
        "        'Griffith University Business', 'Deakin University MBA', 'Curtin University Commerce',\n",
        "        'Macquarie University Finance', 'Adelaide University Business', 'ANU Economics'\n",
        "    ]\n",
        "    data['education'] = np.random.choice(universities, NUM_ROWS)\n",
        "\n",
        "    print(\"Generating business intelligence and activity data...\")\n",
        "\n",
        "    # Recent activity (key for banking opportunities)\n",
        "    activities = [\n",
        "        'Posted about cash flow challenges, seeking working capital solutions',\n",
        "        'Shared article about factory expansion plans, mentioned equipment upgrades',\n",
        "        'Updated experience with recent acquisition completion, hinting at next targets',\n",
        "        'Posted about merchant service fees hurting margins, comparing banking providers',\n",
        "        'Announced new fund launch, seeking banking partners for fund administration',\n",
        "        'Updated profile highlighting treasury transformation project, seeking technology solutions',\n",
        "        'Posted about digital transformation budget approval, fintech integration plans',\n",
        "        'Shared concerns about interest rate impacts, exploring refinancing options',\n",
        "        'Announced merger discussions with competitor, seeking M&A financing advice',\n",
        "        'Posted about rapid growth requiring additional banking facilities, scaling challenges',\n",
        "        'Updated with new property development approval, seeking construction finance',\n",
        "        'Shared article about supply chain financing challenges, working capital constraints',\n",
        "        'Posted about expanding into new markets, international banking requirements',\n",
        "        'Updated experience with ESG investment focus, seeking sustainable finance options',\n",
        "        'Announced successful exit, reinvesting proceeds, seeking private banking services',\n",
        "        'Posted about operational efficiency project, seeking technology financing',\n",
        "        'Shared concerns about client concentration risk, diversification financing needs',\n",
        "        'Updated profile with acquisition target identified, seeking acquisition finance',\n",
        "        'Posted about interest rate hedging strategies, derivatives requirements',\n",
        "        'Announced major export contract win, working capital for production ramp-up needed'\n",
        "    ]\n",
        "\n",
        "    # Generate recent activity with realistic timing\n",
        "    recent_activities = []\n",
        "    for _ in range(NUM_ROWS):\n",
        "        days_ago = random.randint(1, 14)\n",
        "        activity = random.choice(activities)\n",
        "        recent_activities.append(f\"{activity} - {days_ago} days ago\")\n",
        "\n",
        "    data['last_activity'] = recent_activities\n",
        "\n",
        "    # Tags and notes (sales-specific)\n",
        "    tags_options = [\n",
        "        'Cash Flow Issues;Working Capital Need;High Priority',\n",
        "        'Manufacturing Growth;Equipment Finance;Expansion',\n",
        "        'M&A Activity;Acquisition Financing;Strategic Growth',\n",
        "        'Merchant Services;Banking Review;Cost Optimization',\n",
        "        'Fund Administration;Banking Partnership;Wealth Management',\n",
        "        'Treasury Management;Technology Upgrade;Cash Management',\n",
        "        'Digital Transformation;Fintech Integration;Technology Investment',\n",
        "        'Refinancing;Interest Rate Risk;Property Finance',\n",
        "        'Supply Chain Finance;Working Capital;Cash Flow',\n",
        "        'International Banking;Market Expansion;Foreign Exchange',\n",
        "        'ESG Finance;Sustainable Investing;Green Loans',\n",
        "        'Private Banking;Wealth Management;Investment Services'\n",
        "    ]\n",
        "\n",
        "    data['tags'] = np.random.choice(tags_options, NUM_ROWS)\n",
        "\n",
        "    # Notes (contextual business intelligence)\n",
        "    notes_templates = [\n",
        "        'Recent posts indicate operational cash flow challenges, multiple consulting firm payments suggest restructuring costs',\n",
        "        'CEO actively discussing expansion plans, factory modernization, equipment needs for Q4',\n",
        "        'M&A specialist, recent acquisition completed, actively seeking next targets in Q3-Q4',\n",
        "        'CFO publicly discussing merchant service costs, comparing bank offerings, cost reduction focus',\n",
        "        'New fund launch requires banking services, fund administration, custody solutions',\n",
        "        'Treasury transformation underway, seeking modern cash management solutions, technology upgrade',\n",
        "        'CTO approved for digital transformation budget, fintech integration, API banking interest',\n",
        "        'Finance Director concerned about rate impacts, actively exploring refinancing options',\n",
        "        'CEO publicly announced merger discussions, actively seeking M&A financing and advisory services',\n",
        "        'Head of Finance indicating rapid growth, need for additional banking facilities, scaling support'\n",
        "    ]\n",
        "\n",
        "    data['notes'] = np.random.choice(notes_templates, NUM_ROWS)\n",
        "\n",
        "    print(\"Generating sales tracking data...\")\n",
        "\n",
        "    # Sales tracking fields\n",
        "    date_added_list = []\n",
        "    last_contacted_list = []\n",
        "    for _ in range(NUM_ROWS):\n",
        "        # Date added (random over past 6 months)\n",
        "        days_back = random.randint(30, 180)\n",
        "        date_added = (datetime.now() - timedelta(days=days_back)).date()\n",
        "        date_added_list.append(date_added)\n",
        "\n",
        "        # Last contacted (40% have been contacted)\n",
        "        if random.random() < 0.4:\n",
        "            contact_days_back = random.randint(1, days_back)\n",
        "            last_contact = (datetime.now() - timedelta(days=contact_days_back)).date()\n",
        "            last_contacted_list.append(last_contact)\n",
        "        else:\n",
        "            last_contacted_list.append(None)\n",
        "\n",
        "    data['date_added'] = date_added_list\n",
        "    data['last_contacted'] = last_contacted_list\n",
        "\n",
        "    # Contact methods and responses\n",
        "    contact_methods = ['InMail', 'LinkedIn Message', 'Email', 'Phone Call', 'Connection Request']\n",
        "    response_statuses = [\n",
        "        'Responded - Interested', 'Responded - Not Interested', 'No Response',\n",
        "        'Accepted Connection', 'Declined Initial Contact', 'Scheduled Meeting',\n",
        "        'Follow-up Scheduled', 'Very Interested', 'Very Engaged', 'Opened - No Response'\n",
        "    ]\n",
        "\n",
        "    contact_method_list = []\n",
        "    response_status_list = []\n",
        "\n",
        "    for i in range(NUM_ROWS):\n",
        "        if data['last_contacted'][i]:  # Only if they were contacted\n",
        "            contact_method_list.append(random.choice(contact_methods))\n",
        "            response_status_list.append(random.choice(response_statuses))\n",
        "        else:\n",
        "            contact_method_list.append(\"Not Contacted\")\n",
        "            response_status_list.append(\"\")\n",
        "\n",
        "    data['contact_method'] = contact_method_list\n",
        "    data['response_status'] = response_status_list\n",
        "\n",
        "    print(\"Generating financial and business intelligence...\")\n",
        "\n",
        "    # Financial and business intelligence\n",
        "    # Company revenue (correlated with company size)\n",
        "    revenues = []\n",
        "    for size in data['company_size']:\n",
        "        if size in ['11-50']:\n",
        "            revenue = random.uniform(0.5, 5.0)\n",
        "        elif size in ['51-200', '101-250']:\n",
        "            revenue = random.uniform(3.0, 20.0)\n",
        "        elif size in ['201-500', '251-500']:\n",
        "            revenue = random.uniform(15.0, 80.0)\n",
        "        elif size in ['501-1000']:\n",
        "            revenue = random.uniform(50.0, 200.0)\n",
        "        else:\n",
        "            revenue = random.uniform(100.0, 1000.0)\n",
        "        revenues.append(f\"${revenue:.1f}M\")\n",
        "\n",
        "    data['company_revenue'] = revenues\n",
        "\n",
        "    # Financial health score (60-95)\n",
        "    data['financial_health_score'] = np.random.randint(60, 96, NUM_ROWS)\n",
        "\n",
        "    # Recent funding\n",
        "    funding_options = ['None', 'Series A funding', 'Series B funding', 'Equipment loan',\n",
        "                      'Private equity backing', 'Bank facilities', 'Export contract secured']\n",
        "    funding_weights = [0.6, 0.1, 0.05, 0.1, 0.05, 0.05, 0.05]\n",
        "    data['recent_funding'] = np.random.choice(funding_options, NUM_ROWS, p=funding_weights)\n",
        "\n",
        "    # M&A activity\n",
        "    ma_options = ['None', 'Acquisition discussions', 'Merger talks active',\n",
        "                 'Target identified', 'Due diligence active']\n",
        "    ma_weights = [0.8, 0.08, 0.05, 0.04, 0.03]\n",
        "    data['merger_activity'] = np.random.choice(ma_options, NUM_ROWS, p=ma_weights)\n",
        "\n",
        "    # Competitor banks used (2 banks per company typically)\n",
        "    competitor_banks = []\n",
        "    for _ in range(NUM_ROWS):\n",
        "        primary_bank = random.choice(australian_banks)\n",
        "        secondary_bank = random.choice([b for b in australian_banks if b != primary_bank])\n",
        "        competitor_banks.append(f\"{primary_bank};{secondary_bank}\")\n",
        "\n",
        "    data['competitor_banks_used'] = competitor_banks\n",
        "\n",
        "    # Technology adoption\n",
        "    tech_levels = ['Basic Technology', 'Modern Tech Stack', 'Legacy Systems', 'Advanced Technology']\n",
        "    data['technology_adoption'] = np.random.choice(tech_levels, NUM_ROWS, p=[0.4, 0.3, 0.2, 0.1])\n",
        "\n",
        "    # Industry trends\n",
        "    trends = [\n",
        "        'Professional Services Consolidation', 'Manufacturing Automation Growth',\n",
        "        'Fintech Integration Accelerating', 'ESG Investment Focus Growing',\n",
        "        'Supply Chain Digitization', 'Construction Technology Adoption',\n",
        "        'Real Estate Market Transformation', 'Venture Capital Activity',\n",
        "        'Export Market Opportunities', 'Interest Rate Risk Management'\n",
        "    ]\n",
        "    data['industry_trends'] = np.random.choice(trends, NUM_ROWS)\n",
        "\n",
        "    # Key business signals (opportunities)\n",
        "    business_signals = [\n",
        "        'Cash flow stress from project delays', 'Large equipment purchases planned',\n",
        "        'Multiple acquisition discussions', 'Merchant service cost concerns',\n",
        "        'New fund administration needs', 'Treasury system modernization',\n",
        "        'Fintech partnership opportunities', 'Refinancing discussions active',\n",
        "        'Merger financing required', 'Rapid scaling requiring banking support',\n",
        "        'Major construction project financing', 'Working capital financing needs',\n",
        "        'International banking needs', 'Green finance requirements',\n",
        "        'High net worth banking needs', 'Technology upgrade financing'\n",
        "    ]\n",
        "    data['key_business_signals'] = np.random.choice(business_signals, NUM_ROWS)\n",
        "\n",
        "    # Opportunity types (banking products)\n",
        "    opportunity_types = [\n",
        "        'Equipment Finance', 'Business Loan', 'M&A Financing', 'Merchant Services',\n",
        "        'Wealth Solutions', 'Transaction Banking', 'Digital Banking', 'Property Finance',\n",
        "        'Supply Chain Finance', 'International Banking', 'Sustainable Finance',\n",
        "        'Private Banking', 'Construction Finance', 'Export Finance', 'Working Capital',\n",
        "        'Treasury Solutions', 'Growth Capital', 'Acquisition Finance'\n",
        "    ]\n",
        "    data['opportunity_type'] = np.random.choice(opportunity_types, NUM_ROWS)\n",
        "\n",
        "    # Risk indicators\n",
        "    risk_levels = ['Low', 'Medium', 'High']\n",
        "    risk_descriptions = [\n",
        "        'Strong financials', 'Cash flow constraints', 'Interest rate exposure',\n",
        "        'Solid business', 'Supply chain pressures', 'Technology transition',\n",
        "        'Project-dependent revenue', 'Market concentration', 'Expansion focused',\n",
        "        'Operational transformation', 'Regulatory compliance', 'Market volatility'\n",
        "    ]\n",
        "\n",
        "    risk_indicators = []\n",
        "    for _ in range(NUM_ROWS):\n",
        "        level = random.choice(risk_levels)\n",
        "        description = random.choice(risk_descriptions)\n",
        "        risk_indicators.append(f\"{level} - {description}\")\n",
        "\n",
        "    data['risk_indicators'] = risk_indicators\n",
        "\n",
        "    print(\"Creating DataFrame...\")\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    print(f\"Dataset generation complete! Generated {len(df):,} rows with {len(df.columns)} columns.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def save_csv_backup(df, filename='linkedin_sales_navigator_banking_150k.csv'):\n",
        "    \"\"\"\n",
        "    Save DataFrame as CSV\n",
        "    \"\"\"\n",
        "    print(f\"\\n📁 Saving CSV backup: {filename}\")\n",
        "\n",
        "    df_export = df.copy()\n",
        "\n",
        "    # Convert date columns to string format for CSV export\n",
        "    date_columns = ['date_added', 'last_contacted']\n",
        "\n",
        "    for col in date_columns:\n",
        "        if col in df_export.columns:\n",
        "            df_export[col] = df_export[col].apply(lambda x: x.strftime('%Y-%m-%d') if pd.notnull(x) else '')\n",
        "\n",
        "    df_export.to_csv(filename, index=False)\n",
        "    print(f\"✅ CSV saved successfully: {filename}\")\n",
        "\n",
        "    # Download file in Colab\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(filename)\n",
        "        print(\"📥 File download initiated in Colab\")\n",
        "    except:\n",
        "        print(\"Note: Not running in Colab, file saved locally\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function optimized for Google Colab\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"LINKEDIN SALES NAVIGATOR DATA GENERATOR\")\n",
        "    print(\"Banking Customer Intelligence Export\")\n",
        "    print(\"Google Colab BigQuery Version\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Setup authentication first\n",
        "    auth_method = setup_colab_authentication()\n",
        "\n",
        "    if not auth_method:\n",
        "        print(\"❌ Authentication failed. Generating dataset and saving CSV only...\")\n",
        "        df = generate_linkedin_navigator_data()\n",
        "        save_csv_backup(df)\n",
        "        return\n",
        "\n",
        "    # Generate the dataset\n",
        "    df = generate_linkedin_navigator_data()\n",
        "\n",
        "    # Display basic information about the dataset\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"DATASET SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Total Rows: {len(df):,}\")\n",
        "    print(f\"Total Columns: {len(df.columns)}\")\n",
        "    print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "\n",
        "    print(\"\\nColumn Information:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, col in enumerate(df.columns, 1):\n",
        "        dtype = str(df[col].dtype)\n",
        "        null_count = df[col].isnull().sum()\n",
        "        print(f\"{i:2d}. {col:<35} {dtype:<15} ({null_count:,} nulls)\")\n",
        "\n",
        "    print(\"\\nSample Data (first 5 rows):\")\n",
        "    print(\"-\" * 50)\n",
        "    print(df.head())\n",
        "\n",
        "    print(\"\\nKey Metrics Summary:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Connection Degrees: {df['connection_degree'].value_counts().to_dict()}\")\n",
        "    print(f\"Company Sizes: {df['company_size'].value_counts().head().to_dict()}\")\n",
        "    print(f\"Industries: {df['industry'].value_counts().head().to_dict()}\")\n",
        "    print(f\"Opportunity Types: {df['opportunity_type'].value_counts().head().to_dict()}\")\n",
        "\n",
        "    # Setup BigQuery and upload\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BIGQUERY UPLOAD\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Try BigQuery upload\n",
        "    client, bigquery, NotFound = setup_bigquery_client()\n",
        "\n",
        "    if client and bigquery and NotFound:\n",
        "        try:\n",
        "            # Create table schema\n",
        "            schema = create_bigquery_table_schema()\n",
        "\n",
        "            # Create or update table\n",
        "            create_or_update_bigquery_table(client, FULL_TABLE_ID, schema, bigquery, NotFound)\n",
        "\n",
        "            # Upload data to BigQuery\n",
        "            upload_success = upload_to_bigquery(df, client, FULL_TABLE_ID, bigquery, write_disposition=\"WRITE_TRUNCATE\")\n",
        "\n",
        "            if upload_success:\n",
        "                print(f\"\\n✅ SUCCESS! Data uploaded to BigQuery\")\n",
        "                print(f\"📊 Table: {FULL_TABLE_ID}\")\n",
        "                print(f\"📈 Rows: {len(df):,}\")\n",
        "                print(f\"📋 Columns: {len(df.columns)}\")\n",
        "\n",
        "                # Also save CSV for backup\n",
        "                print(\"\\n💾 Creating CSV backup...\")\n",
        "                save_csv_backup(df)\n",
        "            else:\n",
        "                print(\"\\n❌ BigQuery upload failed. Saving CSV instead...\")\n",
        "                save_csv_backup(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Unexpected error during BigQuery operations: {e}\")\n",
        "            print(\"Saving CSV backup...\")\n",
        "            save_csv_backup(df)\n",
        "    else:\n",
        "        print(\"\\n⚠️  BigQuery setup failed. Saving CSV instead...\")\n",
        "        save_csv_backup(df)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PROCESSING COMPLETE\")\n",
        "    print(\"Ready for Banking Intelligence Analysis\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cNhevo9tJ6TW",
        "outputId": "2fb4b270-9634-4e8a-bb12-203bdd770894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "LINKEDIN SALES NAVIGATOR DATA GENERATOR\n",
            "Banking Customer Intelligence Export\n",
            "Google Colab BigQuery Version\n",
            "================================================================================\n",
            "Setting up Google Colab authentication...\n",
            "🔑 Attempting user authentication...\n",
            "✅ User authentication successful!\n",
            "Generating 150,000 rows of LinkedIn Sales Navigator data...\n",
            "Generating names and titles...\n",
            "Generating company information...\n",
            "Generating location and company details...\n",
            "Generating LinkedIn engagement metrics...\n",
            "Generating business intelligence and activity data...\n",
            "Generating sales tracking data...\n",
            "Generating financial and business intelligence...\n",
            "Creating DataFrame...\n",
            "Dataset generation complete! Generated 150,000 rows with 37 columns.\n",
            "\n",
            "================================================================================\n",
            "DATASET SUMMARY\n",
            "================================================================================\n",
            "Total Rows: 150,000\n",
            "Total Columns: 37\n",
            "Memory Usage: 354.9 MB\n",
            "\n",
            "Column Information:\n",
            "--------------------------------------------------\n",
            " 1. first_name                          object          (0 nulls)\n",
            " 2. last_name                           object          (0 nulls)\n",
            " 3. title                               object          (0 nulls)\n",
            " 4. company                             object          (0 nulls)\n",
            " 5. email                               object          (0 nulls)\n",
            " 6. phone                               object          (0 nulls)\n",
            " 7. linkedin_profile_url                object          (0 nulls)\n",
            " 8. current_company_linkedin_url        object          (0 nulls)\n",
            " 9. sales_navigator_lead_url            object          (0 nulls)\n",
            "10. location                            object          (0 nulls)\n",
            "11. industry                            object          (0 nulls)\n",
            "12. company_size                        object          (0 nulls)\n",
            "13. connection_degree                   object          (0 nulls)\n",
            "14. shared_connections                  int64           (0 nulls)\n",
            "15. profile_views                       int64           (0 nulls)\n",
            "16. lead_score                          int64           (0 nulls)\n",
            "17. premium_account                     object          (0 nulls)\n",
            "18. years_at_company                    float64         (0 nulls)\n",
            "19. total_experience                    float64         (0 nulls)\n",
            "20. education                           object          (0 nulls)\n",
            "21. last_activity                       object          (0 nulls)\n",
            "22. tags                                object          (0 nulls)\n",
            "23. notes                               object          (0 nulls)\n",
            "24. date_added                          object          (0 nulls)\n",
            "25. last_contacted                      object          (90,054 nulls)\n",
            "26. contact_method                      object          (0 nulls)\n",
            "27. response_status                     object          (0 nulls)\n",
            "28. company_revenue                     object          (0 nulls)\n",
            "29. financial_health_score              int64           (0 nulls)\n",
            "30. recent_funding                      object          (0 nulls)\n",
            "31. merger_activity                     object          (0 nulls)\n",
            "32. competitor_banks_used               object          (0 nulls)\n",
            "33. technology_adoption                 object          (0 nulls)\n",
            "34. industry_trends                     object          (0 nulls)\n",
            "35. key_business_signals                object          (0 nulls)\n",
            "36. opportunity_type                    object          (0 nulls)\n",
            "37. risk_indicators                     object          (0 nulls)\n",
            "\n",
            "Sample Data (first 5 rows):\n",
            "--------------------------------------------------\n",
            "  first_name last_name                    title  \\\n",
            "0   Benjamin   Jackson  Chief Operating Officer   \n",
            "1       Adam  Gonzalez           Senior Manager   \n",
            "2       Ryan   Jackson             Area Manager   \n",
            "3      Simon     Green        Marketing Manager   \n",
            "4     Rachel     Moore        Head of Marketing   \n",
            "\n",
            "                             company                         email  \\\n",
            "0  Strategic Enterprises Corporation                                 \n",
            "1       Royal Industries Partnership                                 \n",
            "2              Modern Holdings Trust                                 \n",
            "3        Innovation Partners Pty Ltd                                 \n",
            "4               Modern Group Pty Ltd  rachel.moore@business.com.au   \n",
            "\n",
            "              phone                              linkedin_profile_url  \\\n",
            "0                    https://www.linkedin.com/in/benjamin-jackson-745   \n",
            "1                       https://www.linkedin.com/in/adam-gonzalez-374   \n",
            "2  +61 03 5959 7501      https://www.linkedin.com/in/ryan-jackson-203   \n",
            "3                         https://www.linkedin.com/in/simon-green-763   \n",
            "4                        https://www.linkedin.com/in/rachel-moore-842   \n",
            "\n",
            "                        current_company_linkedin_url  \\\n",
            "0  https://www.linkedin.com/company/strategic-ent...   \n",
            "1  https://www.linkedin.com/company/royal-industr...   \n",
            "2  https://www.linkedin.com/company/modern-holdin...   \n",
            "3  https://www.linkedin.com/company/innovation-pa...   \n",
            "4  https://www.linkedin.com/company/modern-group-...   \n",
            "\n",
            "                         sales_navigator_lead_url                  location  \\\n",
            "0  https://www.linkedin.com/sales/lead/ACwAABU945  Melbourne, NT, Australia   \n",
            "1  https://www.linkedin.com/sales/lead/ACwAABL543  Brisbane, ACT, Australia   \n",
            "2  https://www.linkedin.com/sales/lead/ACwAABT608  Canberra, VIC, Australia   \n",
            "3  https://www.linkedin.com/sales/lead/ACwAABZ349    Sydney, ACT, Australia   \n",
            "4  https://www.linkedin.com/sales/lead/ACwAABC529    Sydney, VIC, Australia   \n",
            "\n",
            "   ... company_revenue financial_health_score           recent_funding  \\\n",
            "0  ...           $4.8M                     60                     None   \n",
            "1  ...          $22.9M                     60                     None   \n",
            "2  ...           $4.6M                     86           Equipment loan   \n",
            "3  ...          $57.3M                     72                     None   \n",
            "4  ...           $4.0M                     72  Export contract secured   \n",
            "\n",
            "   merger_activity                 competitor_banks_used  technology_adoption  \\\n",
            "0             None          Adelaide Bank;Macquarie Bank       Legacy Systems   \n",
            "1             None                     ANZ;Heritage Bank     Basic Technology   \n",
            "2             None                Bank of Queensland;NAB       Legacy Systems   \n",
            "3             None  Commonwealth Bank;Bank of Queensland    Modern Tech Stack   \n",
            "4             None                     ANZ;Heritage Bank       Legacy Systems   \n",
            "\n",
            "                     industry_trends               key_business_signals  \\\n",
            "0  Real Estate Market Transformation      Treasury system modernization   \n",
            "1   Construction Technology Adoption    Working capital financing needs   \n",
            "2        Export Market Opportunities          Merger financing required   \n",
            "3   Construction Technology Adoption   Multiple acquisition discussions   \n",
            "4    Manufacturing Automation Growth  Large equipment purchases planned   \n",
            "\n",
            "        opportunity_type                    risk_indicators  \n",
            "0  International Banking  High - Operational transformation  \n",
            "1         Growth Capital        High - Market concentration  \n",
            "2  International Banking    Medium - Interest rate exposure  \n",
            "3         Export Finance           High - Market volatility  \n",
            "4     Treasury Solutions  High - Operational transformation  \n",
            "\n",
            "[5 rows x 37 columns]\n",
            "\n",
            "Key Metrics Summary:\n",
            "--------------------------------------------------\n",
            "Connection Degrees: {'2nd': 74684, '3rd': 45391, '1st': 29925}\n",
            "Company Sizes: {'11-50': 37176, '51-200': 30199, '201-500': 22549, '101-250': 22477, '251-500': 14917}\n",
            "Industries: {'Venture Capital': 7684, 'Wholesale Trade': 7626, 'Professional Services': 7597, 'Construction': 7592, 'Management Consulting': 7546}\n",
            "Opportunity Types: {'Export Finance': 8458, 'Sustainable Finance': 8433, 'Merchant Services': 8421, 'Working Capital': 8408, 'Transaction Banking': 8406}\n",
            "\n",
            "================================================================================\n",
            "BIGQUERY UPLOAD\n",
            "================================================================================\n",
            "Setting up BigQuery client for project: ehc-ldioneda-1d3210\n",
            "✅ Successfully connected to dataset: larry_demo_datasets\n",
            "Creating table ehc-ldioneda-1d3210.larry_demo_datasets.linkedin_navigator_bb_sales_portfolio_demo...\n",
            "Table ehc-ldioneda-1d3210.larry_demo_datasets.linkedin_navigator_bb_sales_portfolio_demo created successfully.\n",
            "Starting upload to ehc-ldioneda-1d3210.larry_demo_datasets.linkedin_navigator_bb_sales_portfolio_demo...\n",
            "Upload mode: WRITE_TRUNCATE\n",
            "Rows to upload: 150,000\n",
            "✅ Successfully uploaded 150,000 rows to ehc-ldioneda-1d3210.larry_demo_datasets.linkedin_navigator_bb_sales_portfolio_demo\n",
            "📊 Table now contains 150,000 total rows\n",
            "\n",
            "✅ SUCCESS! Data uploaded to BigQuery\n",
            "📊 Table: ehc-ldioneda-1d3210.larry_demo_datasets.linkedin_navigator_bb_sales_portfolio_demo\n",
            "📈 Rows: 150,000\n",
            "📋 Columns: 37\n",
            "\n",
            "💾 Creating CSV backup...\n",
            "\n",
            "📁 Saving CSV backup: linkedin_sales_navigator_banking_150k.csv\n",
            "✅ CSV saved successfully: linkedin_sales_navigator_banking_150k.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cbbded88-0732-4d22-9d3f-06fe1403ecc8\", \"linkedin_sales_navigator_banking_150k.csv\", 117555594)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 File download initiated in Colab\n",
            "\n",
            "================================================================================\n",
            "PROCESSING COMPLETE\n",
            "Ready for Banking Intelligence Analysis\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}