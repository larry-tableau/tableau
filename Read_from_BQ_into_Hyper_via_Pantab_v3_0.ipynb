{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMG7piraak1chM6CDj6jc7u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larry-tableau/tableau/blob/main/Read_from_BQ_into_Hyper_via_Pantab_v3_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BigQuery Data Extraction Tool\n",
        "\n",
        "A Python-based tool for extracting large datasets from Google BigQuery with support for parallel processing and multiple output formats (Hyper, Parquet, CSV).\n",
        "\n",
        "---\n",
        "\n",
        "## Features\n",
        "\n",
        "- Dynamic chunk sizing for optimal memory usage.\n",
        "- Parallel processing using ThreadPoolExecutor.\n",
        "- Support for Hyper, Parquet, and CSV file outputs.\n",
        "- Detailed logging and error handling.\n",
        "- Schema-based type conversion for BigQuery data.\n",
        "\n",
        "---\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- Python 3.8+\n",
        "- Google Cloud SDK with authenticated credentials."
      ],
      "metadata": {
        "id": "8NPS-2yVuL8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-auth google-auth-oauthlib google-auth-httplib2 google-cloud-bigquery pandas pantab==4.1.0\n"
      ],
      "metadata": {
        "id": "o1FUBfLRuR6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Will need to configure the relevant sections below\n",
        "```\n",
        "        # Configuration setup\n",
        "        config = BigQueryConfig(\n",
        "            project_id='pre-sales-demo',\n",
        "            source_project='bigquery-public-data',\n",
        "            dataset_id='google_trends',\n",
        "            table_id='top_terms',\n",
        "            output_format='hyper',\n",
        "            output_path='./data',\n",
        "            max_bytes_billed=1000 * 1024 * 1024 * 1024,  # 1TB\n",
        "            initial_chunk_size=500_000,  # Start with smaller chunks\n",
        "            max_workers=4,\n",
        "            clean_up_temp_files=True\n",
        "        )\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "PMnFmwzuuWE5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzjQpHE7uIl-"
      },
      "outputs": [],
      "source": [
        "# Imports and Configuration\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import logging\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime, timezone\n",
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "import pandas_gbq\n",
        "import numpy as np\n",
        "from typing import List, Optional, Tuple, Dict, Any\n",
        "from dataclasses import dataclass\n",
        "from IPython.display import clear_output, display, HTML\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import psutil\n",
        "from pathlib import Path\n",
        "\n",
        "def install_requirements():\n",
        "    \"\"\"Install required packages silently.\"\"\"\n",
        "    requirements = [\n",
        "        'pandas-gbq',\n",
        "        'google-cloud-bigquery',\n",
        "        'pyarrow',\n",
        "        'pantab==4.1.0',\n",
        "        'tableauHyperapi',\n",
        "        'psutil'\n",
        "    ]\n",
        "    for package in requirements:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
        "\n",
        "install_requirements()\n",
        "\n",
        "# Configure logging with more detailed format\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class BigQueryConfig:\n",
        "    \"\"\"Enhanced configuration for BigQuery extraction.\"\"\"\n",
        "    project_id: str\n",
        "    source_project: str\n",
        "    dataset_id: str\n",
        "    table_id: str\n",
        "    output_format: str = 'hyper'  # 'hyper', 'parquet', or 'csv'\n",
        "    output_path: str = './data'\n",
        "    max_bytes_billed: int = 100 * 1024 * 1024 * 1024  # 100 GB\n",
        "    initial_chunk_size: int = 1_000_000\n",
        "    max_workers: int = 4\n",
        "    where_clause: Optional[str] = None\n",
        "    columns: Optional[List[str]] = None\n",
        "    clean_up_temp_files: bool = True\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validate configuration parameters.\"\"\"\n",
        "        if self.output_format not in ['hyper', 'parquet', 'csv']:\n",
        "            raise ValueError(\"output_format must be one of: hyper, parquet, csv\")\n",
        "        if self.initial_chunk_size <= 0:\n",
        "            raise ValueError(\"chunk_size must be positive\")\n",
        "        if self.max_workers <= 0:\n",
        "            raise ValueError(\"max_workers must be positive\")\n",
        "        self.output_path = str(Path(self.output_path).resolve())\n",
        "\n",
        "class BigQueryExtractor:\n",
        "    \"\"\"Enhanced BigQuery data extraction utility with parallel processing and memory management.\"\"\"\n",
        "\n",
        "    def __init__(self, config: BigQueryConfig):\n",
        "        \"\"\"Initialize the extractor with configuration.\"\"\"\n",
        "        self.config = config\n",
        "        self._ensure_output_directory()\n",
        "        self.client = self._initialize_client()\n",
        "        self.schema = self._get_schema()\n",
        "        self.total_rows = 0\n",
        "        self.start_time = time.time()\n",
        "        self.chunk_size = self.config.initial_chunk_size\n",
        "        self.type_mapping = self._get_bq_type_mapping()\n",
        "        self.processed_chunks = 0\n",
        "        self.failed_chunks = []\n",
        "        self._setup_progress_display()\n",
        "\n",
        "    def _setup_progress_display(self):\n",
        "        \"\"\"Initialize progress display styling.\"\"\"\n",
        "        display(HTML(\"\"\"\n",
        "        <style>\n",
        "            .bq-progress {\n",
        "                font-family: monospace;\n",
        "                padding: 10px;\n",
        "                border: 1px solid #ccc;\n",
        "                border-radius: 4px;\n",
        "                margin: 10px 0;\n",
        "                background-color: #f8f9fa;\n",
        "            }\n",
        "            .progress-bar {\n",
        "                color: #fff;\n",
        "                background-color: #28a745;\n",
        "                height: 20px;\n",
        "                border-radius: 3px;\n",
        "                transition: width 0.3s ease;\n",
        "            }\n",
        "        </style>\n",
        "        \"\"\"))\n",
        "\n",
        "    def _get_bq_type_mapping(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get mapping of BigQuery data types to Python/Pandas types.\"\"\"\n",
        "        return {\n",
        "            'STRING': str,\n",
        "            'BYTES': str,\n",
        "            'INTEGER': 'Int64',\n",
        "            'INT64': 'Int64',\n",
        "            'FLOAT': 'float64',\n",
        "            'FLOAT64': 'float64',\n",
        "            'NUMERIC': 'float64',\n",
        "            'BIGNUMERIC': 'float64',\n",
        "            'BOOLEAN': 'boolean',\n",
        "            'BOOL': 'boolean',\n",
        "            'DATE': 'datetime64[ns]',\n",
        "            'DATETIME': 'datetime64[ns]',\n",
        "            'TIME': str,\n",
        "            'TIMESTAMP': 'datetime64[ns]',\n",
        "            'RECORD': str,\n",
        "            'STRUCT': str,\n",
        "            'ARRAY': str,\n",
        "            'GEOGRAPHY': str\n",
        "        }\n",
        "\n",
        "    def _ensure_output_directory(self):\n",
        "        \"\"\"Create output directory if it doesn't exist.\"\"\"\n",
        "        os.makedirs(self.config.output_path, exist_ok=True)\n",
        "\n",
        "    def _initialize_client(self) -> bigquery.Client:\n",
        "        \"\"\"Initialize BigQuery client with authentication.\"\"\"\n",
        "        try:\n",
        "            auth.authenticate_user()\n",
        "            return bigquery.Client(project=self.config.project_id)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize BigQuery client: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _get_schema(self) -> List[bigquery.SchemaField]:\n",
        "        \"\"\"Get table schema information.\"\"\"\n",
        "        try:\n",
        "            dataset_ref = self.client.dataset(self.config.dataset_id,\n",
        "                                            project=self.config.source_project)\n",
        "            table_ref = dataset_ref.table(self.config.table_id)\n",
        "            return self.client.get_table(table_ref).schema\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to get schema: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def adjust_chunk_size(self):\n",
        "        \"\"\"Dynamically adjust chunk size based on available system memory.\"\"\"\n",
        "        try:\n",
        "            available_memory = psutil.virtual_memory().available\n",
        "            ideal_chunk_size = available_memory // (200 * len(self.schema))  # Adjust based on column count\n",
        "            self.chunk_size = max(100_000, min(self.config.initial_chunk_size, ideal_chunk_size))\n",
        "            logger.info(f\"Adjusted chunk size to {self.chunk_size:,} rows based on available memory\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to adjust chunk size: {str(e)}. Using default size.\")\n",
        "            self.chunk_size = self.config.initial_chunk_size\n",
        "\n",
        "    def _count_records(self) -> int:\n",
        "        \"\"\"Count total records in table.\"\"\"\n",
        "        query = f\"\"\"\n",
        "        SELECT COUNT(*) as total\n",
        "        FROM `{self.config.source_project}.{self.config.dataset_id}.{self.config.table_id}`\n",
        "        \"\"\"\n",
        "        if self.config.where_clause:\n",
        "            query += f\" WHERE {self.config.where_clause}\"\n",
        "\n",
        "        df = pandas_gbq.read_gbq(query, project_id=self.config.project_id)\n",
        "        total = int(df['total'].iloc[0])\n",
        "        logger.info(f\"Total records to process: {total:,}\")\n",
        "        return total\n",
        "    def _build_query(self, offset: int) -> str:\n",
        "        \"\"\"Build optimized BigQuery query.\"\"\"\n",
        "        columns = self.config.columns or [field.name for field in self.schema]\n",
        "\n",
        "        query = f\"\"\"\n",
        "        -- Add optimization hints\n",
        "        /*\n",
        "        @param partition_pruning=true\n",
        "        */\n",
        "        SELECT {', '.join(columns)}\n",
        "        FROM `{self.config.source_project}.{self.config.dataset_id}.{self.config.table_id}`\n",
        "        \"\"\"\n",
        "\n",
        "        if self.config.where_clause:\n",
        "            query += f\" WHERE {self.config.where_clause}\"\n",
        "\n",
        "        query += f\"\"\"\n",
        "        ORDER BY {self.schema[0].name}\n",
        "        LIMIT {self.chunk_size}\n",
        "        OFFSET {offset}\n",
        "        \"\"\"\n",
        "\n",
        "        return query\n",
        "\n",
        "    def _process_chunk(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process and convert data types in chunk.\"\"\"\n",
        "        if df.empty:\n",
        "            return df\n",
        "\n",
        "        column_types = {field.name: field.field_type for field in self.schema}\n",
        "\n",
        "        for column in df.columns:\n",
        "            try:\n",
        "                bq_type = column_types.get(column, 'STRING')\n",
        "\n",
        "                # Handle different types\n",
        "                if bq_type in ['RECORD', 'STRUCT']:\n",
        "                    df[column] = df[column].apply(lambda x: json.dumps(x) if x is not None else None)\n",
        "                elif bq_type == 'ARRAY':\n",
        "                    df[column] = df[column].apply(\n",
        "                        lambda x: json.dumps(list(x)) if isinstance(x, (list, np.ndarray)) else\n",
        "                                (json.dumps([x]) if x is not None else None)\n",
        "                    )\n",
        "                elif bq_type in ['DATE', 'DATETIME', 'TIMESTAMP']:\n",
        "                    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
        "                elif bq_type in ['NUMERIC', 'BIGNUMERIC', 'FLOAT', 'FLOAT64']:\n",
        "                    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "                elif bq_type in ['INTEGER', 'INT64']:\n",
        "                    df[column] = df[column].astype('Int64', errors='ignore')\n",
        "                elif bq_type in ['BOOLEAN', 'BOOL']:\n",
        "                    df[column] = df[column].astype('boolean', errors='ignore')\n",
        "                elif bq_type == 'STRING' and df[column].nunique() / len(df) < 0.5:\n",
        "                    df[column] = df[column].astype('category')\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error processing column {column}: {str(e)}\")\n",
        "                df[column] = df[column].astype(str)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _save_chunk(self, df: pd.DataFrame, chunk_num: int) -> str:\n",
        "        \"\"\"Save data chunk in specified format.\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        base_filename = f\"{self.config.table_id}_chunk_{chunk_num}_{timestamp}\"\n",
        "\n",
        "        try:\n",
        "            if self.config.output_format == 'parquet':\n",
        "                filename = f\"{base_filename}.parquet\"\n",
        "                full_path = f\"{self.config.output_path}/{filename}\"\n",
        "                df.to_parquet(full_path, index=False)\n",
        "            else:  # Save as parquet temporarily even if final format is different\n",
        "                filename = f\"{base_filename}.parquet\"\n",
        "                full_path = f\"{self.config.output_path}/{filename}\"\n",
        "                df.to_parquet(full_path, index=False)\n",
        "\n",
        "            logger.debug(f\"Saved chunk {chunk_num} to {full_path}\")\n",
        "            return filename\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save chunk {chunk_num}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _fetch_and_save_chunk(self, offset: int, chunk_num: int) -> Optional[str]:\n",
        "        \"\"\"Fetch and save a single chunk of data.\"\"\"\n",
        "        try:\n",
        "            query = self._build_query(offset)\n",
        "            df_chunk = pandas_gbq.read_gbq(\n",
        "                query,\n",
        "                project_id=self.config.project_id,\n",
        "                configuration={\n",
        "                    'query': {\n",
        "                        'useQueryCache': True,\n",
        "                        'maximumBytesBilled': self.config.max_bytes_billed\n",
        "                    }\n",
        "                }\n",
        "            )\n",
        "\n",
        "            if not df_chunk.empty:\n",
        "                df_chunk = self._process_chunk(df_chunk)\n",
        "                saved_file = self._save_chunk(df_chunk, chunk_num)\n",
        "                self.total_rows += len(df_chunk)\n",
        "                self.processed_chunks += 1\n",
        "                return saved_file\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing chunk {chunk_num}: {str(e)}\")\n",
        "            self.failed_chunks.append(chunk_num)\n",
        "            return None\n",
        "\n",
        "    def _update_progress(self, total_chunks: int):\n",
        "        \"\"\"Update progress with a static, cleaner display.\"\"\"\n",
        "        clear_output(wait=True)  # Clear previous output\n",
        "\n",
        "        elapsed_time = time.time() - self.start_time\n",
        "        progress = (self.processed_chunks / total_chunks) * 100\n",
        "\n",
        "        # Calculate processing statistics\n",
        "        rows_per_second = self.total_rows / elapsed_time if elapsed_time > 0 else 0\n",
        "        remaining_chunks = total_chunks - self.processed_chunks\n",
        "        estimated_remaining = (remaining_chunks * elapsed_time) / max(1, self.processed_chunks)\n",
        "\n",
        "        # Get memory information\n",
        "        memory_info = psutil.Process(os.getpid()).memory_info()\n",
        "        memory_usage_mb = memory_info.rss / 1024 / 1024\n",
        "\n",
        "        # Create progress display\n",
        "        progress_html = f\"\"\"\n",
        "        <div class=\"bq-progress\">\n",
        "            <div>Processing BigQuery Table: {self.config.source_project}.{self.config.dataset_id}.{self.config.table_id}</div>\n",
        "            <div style=\"margin: 10px 0;\">\n",
        "                <div style=\"width: 100%; background-color: #eee; border-radius: 3px;\">\n",
        "                    <div class=\"progress-bar\" style=\"width: {min(100, progress)}%;\">\n",
        "                        {progress:.1f}%\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "            <div style=\"display: grid; grid-template-columns: repeat(2, 1fr); gap: 10px;\">\n",
        "                <div>Chunks: {self.processed_chunks}/{total_chunks}</div>\n",
        "                <div>Rows: {self.total_rows:,}</div>\n",
        "                <div>Failed Chunks: {len(self.failed_chunks)}</div>\n",
        "                <div>Memory: {memory_usage_mb:.1f} MB</div>\n",
        "                <div>Rate: {rows_per_second:.1f} rows/sec</div>\n",
        "                <div>Time Left: {estimated_remaining:.1f}s</div>\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(progress_html))\n",
        "\n",
        "\n",
        "    def _merge_to_final_format(self, saved_files: List[str]) -> str:\n",
        "        \"\"\"Merge chunks into final format.\"\"\"\n",
        "        final_filename = f\"{self.config.table_id}_complete_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "        try:\n",
        "            if self.config.output_format == 'hyper':\n",
        "                final_path = f\"{self.config.output_path}/{final_filename}.hyper\"\n",
        "                import pantab\n",
        "\n",
        "                # Read and combine chunks\n",
        "                dfs = []\n",
        "                for file in saved_files:\n",
        "                    if file:\n",
        "                        df = pd.read_parquet(f\"{self.config.output_path}/{file}\")\n",
        "                        dfs.append(df)\n",
        "\n",
        "                combined_df = pd.concat(dfs, ignore_index=True)\n",
        "                pantab.frame_to_hyper(combined_df, final_path, table=self.config.table_id)\n",
        "\n",
        "            elif self.config.output_format == 'csv':\n",
        "                final_path = f\"{self.config.output_path}/{final_filename}.csv\"\n",
        "                pd.concat(\n",
        "                    [pd.read_parquet(f\"{self.config.output_path}/{f}\") for f in saved_files if f],\n",
        "                    ignore_index=True\n",
        "                ).to_csv(final_path, index=False)\n",
        "\n",
        "            # Clean up temporary files if requested\n",
        "            if self.config.clean_up_temp_files:\n",
        "                for file in saved_files:\n",
        "                    if file:\n",
        "                        try:\n",
        "                            os.remove(f\"{self.config.output_path}/{file}\")\n",
        "                        except Exception as e:\n",
        "                            logger.warning(f\"Failed to remove temporary file {file}: {str(e)}\")\n",
        "\n",
        "            return f\"{final_filename}.{self.config.output_format}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error merging files: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def extract_data(self) -> Tuple[int, str]:\n",
        "        \"\"\"Main method to extract data with parallel processing.\"\"\"\n",
        "        try:\n",
        "            # Adjust chunk size based on memory\n",
        "            self.adjust_chunk_size()\n",
        "\n",
        "            # Get total record count\n",
        "            total_records = self._count_records()\n",
        "            if total_records == 0:\n",
        "                logger.warning(\"No records found to extract\")\n",
        "                return 0, None\n",
        "\n",
        "            total_chunks = (total_records + self.chunk_size - 1) // self.chunk_size\n",
        "            saved_files = []\n",
        "\n",
        "            # Extract data in parallel\n",
        "            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:\n",
        "                future_to_chunk = {\n",
        "                    executor.submit(\n",
        "                        self._fetch_and_save_chunk,\n",
        "                        chunk_num * self.chunk_size,\n",
        "                        chunk_num\n",
        "                    ): chunk_num\n",
        "                    for chunk_num in range(total_chunks)\n",
        "                }\n",
        "\n",
        "                for future in as_completed(future_to_chunk):\n",
        "                    chunk_num = future_to_chunk[future]\n",
        "                    try:\n",
        "                        saved_file = future.result()\n",
        "                        if saved_file:\n",
        "                            saved_files.append(saved_file)\n",
        "                            self._update_progress(total_chunks)\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"Error processing chunk {chunk_num}: {str(e)}\")\n",
        "                        self.failed_chunks.append(chunk_num)\n",
        "                        self._update_progress(total_chunks)  # Update even on failure\n",
        "\n",
        "            # Handle failed chunks retry\n",
        "            if self.failed_chunks:\n",
        "                logger.info(f\"Retrying {len(self.failed_chunks)} failed chunks...\")\n",
        "                for chunk_num in self.failed_chunks[:]:  # Copy list to iterate\n",
        "                    try:\n",
        "                        saved_file = self._fetch_and_save_chunk(chunk_num * self.chunk_size, chunk_num)\n",
        "                        if saved_file:\n",
        "                            saved_files.append(saved_file)\n",
        "                            self.failed_chunks.remove(chunk_num)\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"Retry failed for chunk {chunk_num}: {str(e)}\")\n",
        "\n",
        "            # Merge chunks to final format if any data was extracted\n",
        "            if saved_files:\n",
        "                try:\n",
        "                    final_file = self._merge_to_final_format(saved_files)\n",
        "                    return self.total_rows, final_file\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Failed to create final file: {str(e)}\")\n",
        "                    raise\n",
        "            else:\n",
        "                logger.warning(\"No data was extracted successfully\")\n",
        "                return 0, None\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during extraction: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def get_memory_usage(self) -> str:\n",
        "        \"\"\"Get current memory usage information.\"\"\"\n",
        "        process = psutil.Process(os.getpid())\n",
        "        memory_info = process.memory_info()\n",
        "        return f\"\"\"Memory Usage:\n",
        "        - RSS: {memory_info.rss / 1024 / 1024:.2f} MB\n",
        "        - VMS: {memory_info.vms / 1024 / 1024:.2f} MB\n",
        "        - Available System Memory: {psutil.virtual_memory().available / 1024 / 1024:.2f} MB\"\"\"\n",
        "\n",
        "def extract_bigquery_data(config: BigQueryConfig) -> Tuple[int, Optional[str]]:\n",
        "    \"\"\"Main function to extract data from BigQuery.\"\"\"\n",
        "    extractor = BigQueryExtractor(config)\n",
        "    return extractor.extract_data()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Example usage with enhanced error handling and logging\n",
        "        logger.info(\"Starting BigQuery data extraction process\")\n",
        "\n",
        "        # Configuration setup\n",
        "        config = BigQueryConfig(\n",
        "            project_id='pre-sales-demo',\n",
        "            source_project='bigquery-public-data',\n",
        "            dataset_id='google_trends',\n",
        "            table_id='top_terms',\n",
        "            output_format='hyper',\n",
        "            output_path='./data',\n",
        "            max_bytes_billed=1000 * 1024 * 1024 * 1024,  # 1TB\n",
        "            initial_chunk_size=500_000,  # Start with smaller chunks\n",
        "            max_workers=4,\n",
        "            clean_up_temp_files=True\n",
        "        )\n",
        "\n",
        "        logger.info(\"Configuration:\")\n",
        "        logger.info(f\"- Source: {config.source_project}.{config.dataset_id}.{config.table_id}\")\n",
        "        logger.info(f\"- Output Format: {config.output_format}\")\n",
        "        logger.info(f\"- Initial Chunk Size: {config.initial_chunk_size:,} rows\")\n",
        "        logger.info(f\"- Max Workers: {config.max_workers}\")\n",
        "\n",
        "        # Extract data\n",
        "        total_rows, final_file = extract_bigquery_data(config)\n",
        "\n",
        "        # Log results\n",
        "        logger.info(\"Extraction completed successfully\")\n",
        "        logger.info(f\"Total rows processed: {total_rows:,}\")\n",
        "        logger.info(f\"Final output file: {final_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Extraction failed: {str(e)}\", exc_info=True)\n",
        "        raise\n",
        "    finally:\n",
        "        # Clean up and final status\n",
        "        logger.info(\"Process completed\")"
      ]
    }
  ]
}