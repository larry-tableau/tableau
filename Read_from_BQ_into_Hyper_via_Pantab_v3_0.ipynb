{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwkXWr/+gHMG4iM7y5LeEi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larry-tableau/tableau/blob/main/Read_from_BQ_into_Hyper_via_Pantab_v3_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BigQuery Data Extraction Tool\n",
        "\n",
        "A Python-based tool for extracting large datasets from Google BigQuery with support for parallel processing and multiple output formats (Hyper, Parquet, CSV).\n",
        "\n",
        "---\n",
        "\n",
        "## Features\n",
        "\n",
        "- Dynamic chunk sizing for optimal memory usage.\n",
        "- Parallel processing using ThreadPoolExecutor.\n",
        "- Support for Hyper, Parquet, and CSV file outputs.\n",
        "- Detailed logging and error handling.\n",
        "- Schema-based type conversion for BigQuery data.\n",
        "\n",
        "---\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- Python 3.8+\n",
        "- Google Cloud SDK with authenticated credentials."
      ],
      "metadata": {
        "id": "8NPS-2yVuL8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-auth google-auth-oauthlib google-auth-httplib2 google-cloud-bigquery pandas pantab==4.1.0\n"
      ],
      "metadata": {
        "id": "o1FUBfLRuR6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Will need to configure the relevant sections below\n",
        "```\n",
        "        # Configuration setup\n",
        "        config = BigQueryConfig(\n",
        "            project_id='pre-sales-demo',\n",
        "            source_project='bigquery-public-data',\n",
        "            dataset_id='google_trends',\n",
        "            table_id='top_terms',\n",
        "            output_format='hyper',\n",
        "            output_path='./data',\n",
        "            max_bytes_billed=1000 * 1024 * 1024 * 1024,  # 1TB\n",
        "            initial_chunk_size=500_000,  # Start with smaller chunks\n",
        "            max_workers=4,\n",
        "            clean_up_temp_files=True\n",
        "        )\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "PMnFmwzuuWE5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzjQpHE7uIl-"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import logging\n",
        "import time\n",
        "from datetime import datetime, timezone\n",
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "import pandas_gbq\n",
        "import numpy as np\n",
        "from typing import List, Optional, Tuple, Dict, Any\n",
        "from dataclasses import dataclass\n",
        "from IPython.display import clear_output, display, HTML\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import psutil\n",
        "import json\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "\n",
        "\n",
        "# Install required packages\n",
        "def install_requirements():\n",
        "    \"\"\"Install required packages silently.\"\"\"\n",
        "    requirements = [\n",
        "        'pandas-gbq',\n",
        "        'google-cloud-bigquery',\n",
        "        'pyarrow',\n",
        "        'pantab==4.1.0',\n",
        "        'tableauHyperapi',\n",
        "        'psutil'\n",
        "    ]\n",
        "    for package in requirements:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
        "\n",
        "install_requirements()\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CoLabLogHandler(logging.Handler):\n",
        "    \"\"\"Custom log handler to store logs in memory for display.\"\"\"\n",
        "    def __init__(self, max_lines=1000):\n",
        "        super().__init__()\n",
        "        self.log_buffer = []\n",
        "        self.max_lines = max_lines\n",
        "\n",
        "    def emit(self, record):\n",
        "        \"\"\"Emit a log record.\"\"\"\n",
        "        log_entry = self.format(record)\n",
        "        self.log_buffer.append(log_entry)\n",
        "        if len(self.log_buffer) > self.max_lines:\n",
        "            self.log_buffer = self.log_buffer[-self.max_lines:]\n",
        "\n",
        "    def get_logs(self):\n",
        "        \"\"\"Get all stored logs.\"\"\"\n",
        "        return '\\n'.join(self.log_buffer)\n",
        "\n",
        "@dataclass\n",
        "class BigQueryConfig:\n",
        "    \"\"\"Configuration for BigQuery extraction.\"\"\"\n",
        "    project_id: str\n",
        "    source_project: str\n",
        "    dataset_id: str\n",
        "    table_id: str\n",
        "    output_format: str = 'hyper'\n",
        "    output_path: str = './data'\n",
        "    max_bytes_billed: int = 100 * 1024 * 1024 * 1024  # 100GB\n",
        "    initial_chunk_size: int = 500_000\n",
        "    max_workers: int = 4\n",
        "    columns: Optional[List[str]] = None\n",
        "    where_clause: Optional[str] = None\n",
        "    clean_up_temp_files: bool = True\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validate configuration parameters.\"\"\"\n",
        "        if self.output_format not in ['hyper', 'parquet', 'csv']:\n",
        "            raise ValueError(\"output_format must be one of: hyper, parquet, csv\")\n",
        "        if self.initial_chunk_size <= 0:\n",
        "            raise ValueError(\"chunk_size must be positive\")\n",
        "        if self.max_workers <= 0:\n",
        "            raise ValueError(\"max_workers must be positive\")\n",
        "        self.output_path = str(Path(self.output_path).resolve())\n",
        "\n",
        "class BigQueryExtractor:\n",
        "    \"\"\"Enhanced BigQuery data extraction utility.\"\"\"\n",
        "\n",
        "    def __init__(self, config: BigQueryConfig):\n",
        "        \"\"\"Initialize extractor with configuration.\"\"\"\n",
        "        self.config = config\n",
        "        self._ensure_output_directory()\n",
        "\n",
        "        # Setup logging\n",
        "        self.log_handler = CoLabLogHandler()\n",
        "        self.log_handler.setFormatter(\n",
        "            logging.Formatter('%(asctime)s - %(levelname)s - %(message)s',\n",
        "                            datefmt='%Y-%m-%d %H:%M:%S')\n",
        "        )\n",
        "        logger.addHandler(self.log_handler)\n",
        "\n",
        "        self.client = self._initialize_client()\n",
        "        self.schema = self._get_schema()\n",
        "        self.total_rows = 0\n",
        "        self.processed_chunks = 0\n",
        "        self.failed_chunks = []\n",
        "        self.start_time = time.time()\n",
        "        self.chunk_size = self.config.initial_chunk_size\n",
        "        self._setup_progress_display()\n",
        "\n",
        "    def _ensure_output_directory(self):\n",
        "        \"\"\"Create output directory if it doesn't exist.\"\"\"\n",
        "        os.makedirs(self.config.output_path, exist_ok=True)\n",
        "\n",
        "    def _initialize_client(self) -> bigquery.Client:\n",
        "        \"\"\"Initialize BigQuery client with authentication.\"\"\"\n",
        "        try:\n",
        "            auth.authenticate_user()\n",
        "            return bigquery.Client(project=self.config.project_id)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize BigQuery client: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _get_schema(self) -> List[bigquery.SchemaField]:\n",
        "        \"\"\"Get table schema information.\"\"\"\n",
        "        try:\n",
        "            dataset_ref = self.client.dataset(self.config.dataset_id,\n",
        "                                            project=self.config.source_project)\n",
        "            table_ref = dataset_ref.table(self.config.table_id)\n",
        "            return self.client.get_table(table_ref).schema\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to get schema: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _get_bq_type_mapping(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get mapping of BigQuery data types to Python/Pandas types.\"\"\"\n",
        "        return {\n",
        "            'STRING': str,\n",
        "            'BYTES': str,\n",
        "            'INTEGER': 'Int64',\n",
        "            'INT64': 'Int64',\n",
        "            'FLOAT': 'float64',\n",
        "            'FLOAT64': 'float64',\n",
        "            'NUMERIC': 'float64',\n",
        "            'BIGNUMERIC': 'float64',\n",
        "            'BOOLEAN': 'boolean',\n",
        "            'BOOL': 'boolean',\n",
        "            'DATE': 'datetime64[ns]',\n",
        "            'DATETIME': 'datetime64[ns]',\n",
        "            'TIME': str,\n",
        "            'TIMESTAMP': 'datetime64[ns]',\n",
        "            'RECORD': str,\n",
        "            'STRUCT': str,\n",
        "            'ARRAY': str,\n",
        "            'GEOGRAPHY': str\n",
        "        }\n",
        "\n",
        "    def _setup_progress_display(self):\n",
        "        \"\"\"Initialize progress display styling.\"\"\"\n",
        "        display(HTML(\"\"\"\n",
        "        <style>\n",
        "            .bq-progress {\n",
        "                font-family: monospace;\n",
        "                padding: 10px;\n",
        "                border: 1px solid #ccc;\n",
        "                border-radius: 4px;\n",
        "                margin: 10px 0;\n",
        "                background-color: #f8f9fa;\n",
        "            }\n",
        "            .progress-bar {\n",
        "                color: #fff;\n",
        "                background-color: #28a745;\n",
        "                height: 20px;\n",
        "                border-radius: 3px;\n",
        "                transition: width 0.3s ease;\n",
        "                text-align: center;\n",
        "                line-height: 20px;\n",
        "            }\n",
        "            .log-container {\n",
        "                font-family: monospace;\n",
        "                padding: 10px;\n",
        "                border: 1px solid #ddd;\n",
        "                border-radius: 4px;\n",
        "                margin: 10px 0;\n",
        "                background-color: #f8f9fa;\n",
        "                max-height: 200px;\n",
        "                overflow-y: auto;\n",
        "                white-space: pre-wrap;\n",
        "                font-size: 12px;\n",
        "            }\n",
        "            .log-entry {\n",
        "                margin: 2px 0;\n",
        "            }\n",
        "            .log-error { color: #dc3545; }\n",
        "            .log-warning { color: #ffc107; }\n",
        "            .log-info { color: #17a2b8; }\n",
        "        </style>\n",
        "        \"\"\"))\n",
        "\n",
        "    @staticmethod\n",
        "    def adjust_chunk_size(schema: List[bigquery.SchemaField], available_memory: int) -> int:\n",
        "        \"\"\"\n",
        "        Dynamically adjust chunk size based on memory and schema details.\n",
        "        Adds a cap to avoid overly large chunk sizes.\n",
        "        \"\"\"\n",
        "        # Estimated memory usage per data type in bytes\n",
        "        type_memory_footprint = {\n",
        "            'STRING': 100,        # Average string size\n",
        "            'BYTES': 50,          # Binary data\n",
        "            'INTEGER': 8,         # 64-bit integers\n",
        "            'INT64': 8,           # 64-bit integers\n",
        "            'FLOAT': 8,           # 64-bit floats\n",
        "            'FLOAT64': 8,         # 64-bit floats\n",
        "            'NUMERIC': 16,        # Numeric with higher precision\n",
        "            'BIGNUMERIC': 32,     # Big numeric\n",
        "            'BOOLEAN': 1,         # Boolean\n",
        "            'DATE': 4,            # Dates\n",
        "            'DATETIME': 8,        # Datetime objects\n",
        "            'TIMESTAMP': 8,       # Timestamp\n",
        "            'RECORD': 200,        # Nested structure (average)\n",
        "            'STRUCT': 200,        # Nested structure (average)\n",
        "            'ARRAY': 150,         # Array (average per element)\n",
        "            'GEOGRAPHY': 1000     # Geography data\n",
        "        }\n",
        "\n",
        "        # Calculate total memory usage per row based on schema\n",
        "        total_memory_per_row = sum(\n",
        "            type_memory_footprint.get(field.field_type, 50) for field in schema\n",
        "        )\n",
        "\n",
        "        # Account for overhead and concurrency\n",
        "        row_memory_with_overhead = total_memory_per_row * 1.1  # Add 10% buffer\n",
        "\n",
        "        # Calculate max rows based on available memory\n",
        "        max_rows = available_memory // row_memory_with_overhead\n",
        "\n",
        "        # Cap chunk size to avoid excessive memory usage\n",
        "        capped_chunk_size = min(max_rows, 100_000)  # Cap at 100,000 rows\n",
        "\n",
        "        # Log intermediate values for debugging\n",
        "        logger.info(f\"Available memory: {available_memory} bytes\")\n",
        "        logger.info(f\"Estimated memory per row: {row_memory_with_overhead:.2f} bytes\")\n",
        "        logger.info(f\"Calculated chunk size: {max_rows} rows\")\n",
        "        logger.info(f\"Capped chunk size: {capped_chunk_size} rows\")\n",
        "\n",
        "        # Ensure a minimum chunk size to avoid inefficient queries\n",
        "        return max(10_000, int(capped_chunk_size))\n",
        "\n",
        "    def _count_records(self) -> int:\n",
        "        \"\"\"Count total records in table.\"\"\"\n",
        "        query = f\"\"\"\n",
        "        SELECT COUNT(*) as total\n",
        "        FROM `{self.config.source_project}.{self.config.dataset_id}.{self.config.table_id}`\n",
        "        \"\"\"\n",
        "        if self.config.where_clause:\n",
        "            query += f\" WHERE {self.config.where_clause}\"\n",
        "\n",
        "        df = pandas_gbq.read_gbq(query, project_id=self.config.project_id)\n",
        "        total = int(df['total'].iloc[0])\n",
        "        logger.info(f\"Total records to process: {total:,}\")\n",
        "        return total\n",
        "\n",
        "    def _build_query(self, offset: int) -> str:\n",
        "        \"\"\"Build optimized BigQuery query.\"\"\"\n",
        "        columns = self.config.columns or [field.name for field in self.schema]\n",
        "\n",
        "        query = f\"\"\"\n",
        "        SELECT {', '.join(columns)}\n",
        "        FROM `{self.config.source_project}.{self.config.dataset_id}.{self.config.table_id}`\n",
        "        \"\"\"\n",
        "\n",
        "        if self.config.where_clause:\n",
        "            query += f\" WHERE {self.config.where_clause}\"\n",
        "\n",
        "        # Simple LIMIT/OFFSET without ORDER BY to avoid memory issues\n",
        "        query += f\"\"\"\n",
        "        LIMIT {self.chunk_size}\n",
        "        OFFSET {offset}\n",
        "        \"\"\"\n",
        "\n",
        "        return query\n",
        "\n",
        "    def _process_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process and convert column data types.\"\"\"\n",
        "        if df.empty:\n",
        "            return df\n",
        "\n",
        "        column_types = {field.name: field.field_type for field in self.schema}\n",
        "        type_mapping = self._get_bq_type_mapping()\n",
        "\n",
        "        for column in df.columns:\n",
        "            try:\n",
        "                bq_type = column_types.get(column, 'STRING')\n",
        "\n",
        "                # Handle different types\n",
        "                if bq_type in ['RECORD', 'STRUCT']:\n",
        "                    df[column] = df[column].apply(lambda x: json.dumps(x) if x is not None else None)\n",
        "\n",
        "                elif bq_type == 'ARRAY':\n",
        "                    df[column] = df[column].apply(\n",
        "                        lambda x: json.dumps(list(x)) if isinstance(x, (list, np.ndarray)) else\n",
        "                                (json.dumps([x]) if x is not None else None)\n",
        "                    )\n",
        "\n",
        "                elif bq_type in ['DATE', 'DATETIME', 'TIMESTAMP']:\n",
        "                    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
        "\n",
        "                elif bq_type in ['NUMERIC', 'BIGNUMERIC', 'FLOAT', 'FLOAT64']:\n",
        "                    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "\n",
        "                elif bq_type in ['INTEGER', 'INT64']:\n",
        "                    df[column] = df[column].astype('Int64', errors='ignore')\n",
        "\n",
        "                elif bq_type in ['BOOLEAN', 'BOOL']:\n",
        "                    df[column] = df[column].astype('boolean', errors='ignore')\n",
        "\n",
        "                elif bq_type == 'STRING' and df[column].nunique() / len(df) < 0.5:\n",
        "                    df[column] = df[column].astype('category')\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error processing column {column}: {str(e)}\")\n",
        "                try:\n",
        "                    df[column] = df[column].astype(str)\n",
        "                except:\n",
        "                    df[column] = df[column].apply(lambda x: str(x) if x is not None else None)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _save_chunk(self, df: pd.DataFrame, chunk_num: int) -> str:\n",
        "        \"\"\"Save data chunk with enhanced type handling.\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        base_filename = f\"{self.config.table_id}_chunk_{chunk_num}_{timestamp}\"\n",
        "\n",
        "        try:\n",
        "            filename = f\"{base_filename}.parquet\"\n",
        "            full_path = f\"{self.config.output_path}/{filename}\"\n",
        "\n",
        "            # Convert to Arrow Table and write with compression\n",
        "            table = pa.Table.from_pandas(df)\n",
        "            pq.write_table(table, full_path, compression='snappy')  # Snappy compression\n",
        "\n",
        "            logger.debug(f\"Saved chunk {chunk_num} to {full_path}\")\n",
        "            return filename\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save chunk: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def _update_progress(self, total_chunks: int):\n",
        "        \"\"\"Update progress display with logs.\"\"\"\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        elapsed_time = time.time() - self.start_time\n",
        "        progress = (self.processed_chunks / total_chunks) * 100\n",
        "\n",
        "        # Calculate statistics\n",
        "        rows_per_second = self.total_rows / elapsed_time if elapsed_time > 0 else 0\n",
        "        remaining_chunks = total_chunks - self.processed_chunks\n",
        "        estimated_remaining = (remaining_chunks * elapsed_time) / max(1, self.processed_chunks)\n",
        "        memory_info = psutil.Process(os.getpid()).memory_info()\n",
        "        memory_usage_mb = memory_info.rss / 1024 / 1024\n",
        "\n",
        "        # Create progress display\n",
        "        progress_html = f\"\"\"\n",
        "        <div class=\"bq-progress\">\n",
        "            <div style=\"font-weight: bold;\">Processing: {self.config.source_project}.{self.config.dataset_id}.{self.config.table_id}</div>\n",
        "            <div style=\"margin: 10px 0;\">\n",
        "                <div style=\"width: 100%; background-color: #eee; border-radius: 3px;\">\n",
        "                    <div class=\"progress-bar\" style=\"width: {min(100, progress)}%;\">\n",
        "                        {progress:.1f}%\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "            <div style=\"display: grid; grid-template-columns: repeat(2, 1fr); gap: 10px;\">\n",
        "                <div>Chunks: {self.processed_chunks}/{total_chunks}</div>\n",
        "                <div>Rows: {self.total_rows:,}</div>\n",
        "                <div>Failed Chunks: {len(self.failed_chunks)}</div>\n",
        "                <div>Memory: {memory_usage_mb:.1f} MB</div>\n",
        "                <div>Rate: {rows_per_second:.1f} rows/sec</div>\n",
        "                <div>Time Left: {estimated_remaining:.1f}s</div>\n",
        "            </div>\n",
        "        </div>\n",
        "        <div class=\"log-container\">\n",
        "        \"\"\"\n",
        "\n",
        "        # Add log entries with color coding\n",
        "        logs = self.log_handler.get_logs()\n",
        "        for log_line in logs.split('\\n'):\n",
        "            if 'ERROR' in log_line:\n",
        "                log_class = 'log-error'\n",
        "            elif 'WARNING' in log_line:\n",
        "                log_class = 'log-warning'\n",
        "            elif 'INFO' in log_line:\n",
        "                log_class = 'log-info'\n",
        "            else:\n",
        "                log_class = ''\n",
        "            progress_html += f'<div class=\"log-entry {log_class}\">{log_line}</div>'\n",
        "\n",
        "        progress_html += \"</div>\"\n",
        "        display(HTML(progress_html))\n",
        "\n",
        "    def _fetch_and_save_chunk(self, offset: int, chunk_num: int) -> Optional[str]:\n",
        "        \"\"\"Fetch and save a single chunk with retry logic.\"\"\"\n",
        "        max_retries = 3\n",
        "        original_chunk_size = self.chunk_size\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                if attempt > 0:\n",
        "                    logger.info(f\"Retry attempt {attempt + 1} for chunk {chunk_num}\")\n",
        "                    self.chunk_size = max(10000, original_chunk_size // (2 ** attempt))\n",
        "\n",
        "                query = self._build_query(offset)\n",
        "                df_chunk = pandas_gbq.read_gbq(\n",
        "                    query,\n",
        "                    project_id=self.config.project_id,\n",
        "                    configuration={\n",
        "                        'query': {\n",
        "                            'useQueryCache': True,\n",
        "                            'maximumBytesBilled': self.config.max_bytes_billed,\n",
        "                            'priority': 'BATCH'\n",
        "                        }\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                if not df_chunk.empty:\n",
        "                    # Changed from _process_special_columns to _process_columns\n",
        "                    df_chunk = self._process_columns(df_chunk)\n",
        "                    saved_file = self._save_chunk(df_chunk, chunk_num)\n",
        "                    self.total_rows += len(df_chunk)\n",
        "                    self.processed_chunks += 1\n",
        "                    return saved_file\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error in attempt {attempt + 1} for chunk {chunk_num}: {str(e)}\")\n",
        "                if attempt == max_retries - 1:\n",
        "                    self.failed_chunks.append(chunk_num)\n",
        "                    return None\n",
        "                time.sleep(2 ** attempt)  # Exponential backoff\n",
        "            finally:\n",
        "                self.chunk_size = original_chunk_size\n",
        "\n",
        "        return None\n",
        "\n",
        "    def parallel_process_chunks(chunk_count: int, process_chunk_fn, *args):\n",
        "        \"\"\"\n",
        "        Process chunks in parallel using ProcessPoolExecutor.\n",
        "        :param chunk_count: Total number of chunks to process.\n",
        "        :param process_chunk_fn: Function to process each chunk.\n",
        "        :param args: Additional arguments for process_chunk_fn.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        with ProcessPoolExecutor() as executor:\n",
        "            futures = {\n",
        "                executor.submit(process_chunk_fn, chunk_num, *args): chunk_num\n",
        "                for chunk_num in range(chunk_count)\n",
        "            }\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                chunk_num = futures[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    results.append(result)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Chunk {chunk_num} failed with error: {e}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _merge_to_final_format(self, saved_files: List[str]) -> str:\n",
        "        \"\"\"Merge chunks into final output format.\"\"\"\n",
        "        final_filename = f\"{self.config.table_id}_complete_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "        try:\n",
        "            if self.config.output_format == 'hyper':\n",
        "                final_path = f\"{self.config.output_path}/{final_filename}.hyper\"\n",
        "                import pantab\n",
        "\n",
        "                logger.info(\"Starting merge to Hyper format...\")\n",
        "                # Read and combine chunks with memory-efficient approach\n",
        "                dfs = []\n",
        "                total_size = 0\n",
        "                max_batch_size = 1000000  # Maximum rows per batch\n",
        "\n",
        "                for file in saved_files:\n",
        "                    if file:\n",
        "                        df = pd.read_parquet(f\"{self.config.output_path}/{file}\")\n",
        "                        dfs.append(df)\n",
        "                        total_size += len(df)\n",
        "\n",
        "                        # Process batch if size threshold reached\n",
        "                        if total_size >= max_batch_size:\n",
        "                            combined_df = pd.concat(dfs, ignore_index=True)\n",
        "                            pantab.frame_to_hyper(combined_df, final_path, table=self.config.table_id)\n",
        "                            dfs = []  # Clear processed dataframes\n",
        "                            total_size = 0\n",
        "\n",
        "                # Process remaining dataframes\n",
        "                if dfs:\n",
        "                    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "                    pantab.frame_to_hyper(combined_df, final_path, table=self.config.table_id)\n",
        "\n",
        "            elif self.config.output_format == 'csv':\n",
        "                final_path = f\"{self.config.output_path}/{final_filename}.csv\"\n",
        "                # Process chunks one at a time for CSV\n",
        "                first_chunk = True\n",
        "                for file in saved_files:\n",
        "                    if file:\n",
        "                        df = pd.read_parquet(f\"{self.config.output_path}/{file}\")\n",
        "                        df.to_csv(final_path, mode='w' if first_chunk else 'a',\n",
        "                                header=first_chunk, index=False)\n",
        "                        first_chunk = False\n",
        "\n",
        "            elif self.config.output_format == 'parquet':\n",
        "                final_path = f\"{self.config.output_path}/{final_filename}.parquet\"\n",
        "                # For parquet, we'll use a similar batching approach as hyper\n",
        "                dfs = []\n",
        "                total_size = 0\n",
        "                max_batch_size = 1000000\n",
        "\n",
        "                for file in saved_files:\n",
        "                    if file:\n",
        "                        df = pd.read_parquet(f\"{self.config.output_path}/{file}\")\n",
        "                        dfs.append(df)\n",
        "                        total_size += len(df)\n",
        "\n",
        "                        if total_size >= max_batch_size:\n",
        "                            combined_df = pd.concat(dfs, ignore_index=True)\n",
        "                            combined_df.to_parquet(final_path)\n",
        "                            dfs = []\n",
        "                            total_size = 0\n",
        "\n",
        "                if dfs:\n",
        "                    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "                    combined_df.to_parquet(final_path)\n",
        "\n",
        "            # Clean up temporary files if requested\n",
        "            if self.config.clean_up_temp_files:\n",
        "                for file in saved_files:\n",
        "                    if file:\n",
        "                        try:\n",
        "                            os.remove(f\"{self.config.output_path}/{file}\")\n",
        "                            logger.debug(f\"Removed temporary file: {file}\")\n",
        "                        except Exception as e:\n",
        "                            logger.warning(f\"Failed to remove temporary file {file}: {str(e)}\")\n",
        "\n",
        "            return f\"{final_filename}.{self.config.output_format}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error merging files: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def extract_data(self) -> Tuple[int, str]:\n",
        "        \"\"\"Main extraction method with parallel processing.\"\"\"\n",
        "        try:\n",
        "            # Initialize and validate\n",
        "            memory_info = psutil.virtual_memory()\n",
        "            available_memory = memory_info.available\n",
        "            self.chunk_size = self.adjust_chunk_size(self.schema, available_memory)\n",
        "\n",
        "            total_records = self._count_records()\n",
        "            if total_records == 0:\n",
        "                logger.warning(\"No records found to extract\")\n",
        "                return 0, None\n",
        "\n",
        "            total_chunks = (total_records + self.chunk_size - 1) // self.chunk_size\n",
        "            saved_files = []\n",
        "\n",
        "            logger.info(f\"Starting extraction with {self.config.max_workers} workers\")\n",
        "            logger.info(f\"Initial chunk size: {self.chunk_size:,} rows\")\n",
        "            logger.info(f\"Total chunks: {total_chunks}\")\n",
        "\n",
        "            # Extract data in parallel\n",
        "            with ThreadPoolExecutor(max_workers=min(self.config.max_workers, os.cpu_count() * 2)) as executor:\n",
        "                futures = []\n",
        "                for chunk_num in range(total_chunks):\n",
        "                    offset = chunk_num * self.chunk_size\n",
        "                    future = executor.submit(self._fetch_and_save_chunk, offset, chunk_num)\n",
        "                    futures.append(future)\n",
        "\n",
        "                    offset = chunk_num * self.chunk_size\n",
        "                    future = executor.submit(self._fetch_and_save_chunk, offset, chunk_num)\n",
        "                    futures.append(future)\n",
        "\n",
        "                for future in as_completed(futures):\n",
        "                    try:\n",
        "                        saved_file = future.result()\n",
        "                        if saved_file:\n",
        "                            saved_files.append(saved_file)\n",
        "                        self._update_progress(total_chunks)\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"Error processing chunk: {str(e)}\")\n",
        "\n",
        "            # Handle final merging\n",
        "            if saved_files:\n",
        "                try:\n",
        "                    final_file = self._merge_to_final_format(saved_files)\n",
        "                    logger.info(f\"Successfully created final file: {final_file}\")\n",
        "                    return self.total_rows, final_file\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Failed to create final file: {str(e)}\")\n",
        "                    raise\n",
        "            else:\n",
        "                logger.warning(\"No data was extracted successfully\")\n",
        "                return 0, None\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during extraction: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "def extract_bigquery_data(config: BigQueryConfig) -> Tuple[int, Optional[str]]:\n",
        "    \"\"\"Main function to extract data from BigQuery.\"\"\"\n",
        "    extractor = BigQueryExtractor(config)\n",
        "    return extractor.extract_data()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Configure extraction\n",
        "        config = BigQueryConfig(\n",
        "            project_id='pre-sales-demo',\n",
        "            source_project='bigquery-public-data',\n",
        "            dataset_id='samples',\n",
        "            table_id='github_timeline',\n",
        "            output_format='hyper',\n",
        "            output_path='./data',\n",
        "            initial_chunk_size=500_000,\n",
        "            max_workers=4,\n",
        "            max_bytes_billed=1000 * 1024 * 1024 * 1024,  # 1TB\n",
        "            clean_up_temp_files=True\n",
        "        )\n",
        "\n",
        "        logger.info(\"Starting BigQuery data extraction\")\n",
        "        logger.info(f\"Source: {config.source_project}.{config.dataset_id}.{config.table_id}\")\n",
        "        logger.info(f\"Output format: {config.output_format}\")\n",
        "\n",
        "        # Extract data\n",
        "        total_rows, final_file = extract_bigquery_data(config)\n",
        "\n",
        "        # Log results\n",
        "        logger.info(\"Extraction completed successfully\")\n",
        "        logger.info(f\"Total rows processed: {total_rows:,}\")\n",
        "        logger.info(f\"Final output file: {final_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Extraction failed: {str(e)}\", exc_info=True)\n",
        "        raise\n",
        "    finally:\n",
        "        logger.info(\"Process completed\")\n"
      ]
    }
  ]
}