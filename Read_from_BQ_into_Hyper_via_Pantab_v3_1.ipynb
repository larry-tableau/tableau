{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMspUIfSLNdlx6E9dvaj1F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larry-tableau/tableau/blob/main/Read_from_BQ_into_Hyper_via_Pantab_v3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script provides a highly configurable and robust way to extract data from Google BigQuery and save it in a specified output format (`hyper`, `parquet`, or `csv`). Below is the detailed documentation for running this script, its parameters, and configuration:\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Setup and Prerequisites**\n",
        "\n",
        "Before running the script:\n",
        "1. **Python Environment**: Ensure you have Python 3.8+ installed with all required libraries.\n",
        "2. **Dependencies**: Install the required Python libraries:\n",
        "   ```bash\n",
        "   pip install google-cloud-bigquery google-cloud-bigquery-storage pandas pandas-gbq pyarrow pantab psutil\n",
        "   ```\n",
        "3. **Authentication**: Authenticate with Google Cloud in Colab or your local environment:\n",
        "   - In Colab:\n",
        "     ```python\n",
        "     from google.colab import auth\n",
        "     auth.authenticate_user()\n",
        "     ```\n",
        "   - Locally:\n",
        "     Set up authentication by downloading a service account key from your Google Cloud project and exporting the `GOOGLE_APPLICATION_CREDENTIALS` environment variable:\n",
        "     ```bash\n",
        "     export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your-key.json\"\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Configuration Parameters**\n",
        "\n",
        "The script uses a `BigQueryConfig` class to define configuration parameters. Here's a detailed breakdown:\n",
        "\n",
        "| **Parameter**          | **Type**           | **Description**                                                                                          | **Default**               |\n",
        "|-------------------------|--------------------|----------------------------------------------------------------------------------------------------------|---------------------------|\n",
        "| `project_id`           | `str`             | Your Google Cloud project ID where BigQuery is configured.                                               | N/A                       |\n",
        "| `source_project`       | `str`             | Source project ID containing the dataset.                                                                | N/A                       |\n",
        "| `dataset_id`           | `str`             | The dataset ID containing the table to extract data from.                                                | N/A                       |\n",
        "| `table_id`             | `str`             | The table ID from which to extract data.                                                                 | N/A                       |\n",
        "| `output_format`        | `str`             | Output format: `hyper`, `parquet`, or `csv`.                                                             | `'hyper'`                 |\n",
        "| `output_path`          | `str`             | Path to save the output files.                                                                           | `'./data'`                |\n",
        "| `max_bytes_billed`     | `int`             | Maximum bytes allowed for billing (e.g., 100GB).                                                         | `100GB`                   |\n",
        "| `initial_chunk_size`   | `int`             | Initial chunk size for data extraction in rows.                                                          | `500,000`                 |\n",
        "| `max_workers`          | `int`             | Number of workers for parallel data processing.                                                          | `4`                       |\n",
        "| `chunk_size`           | `int` (Optional)  | Override for dynamically adjusted chunk size.                                                            | `None`                    |\n",
        "| `columns`              | `List[str]`       | List of columns to extract. If `None`, all columns are fetched.                                           | `None`                    |\n",
        "| `where_clause`         | `str` (Optional)  | SQL WHERE clause to filter data.                                                                         | `None`                    |\n",
        "| `max_rows`             | `int` (Optional)  | Maximum number of rows to extract.                                                                       | `None`                    |\n",
        "| `hyper_batch_size`     | `int`             | Batch size for writing to Tableau Hyper files.                                                           | `100,000`                 |\n",
        "| `max_memory_gb`        | `float`           | Maximum memory in GB to allocate for extraction.                                                         | `0.8`                     |\n",
        "| `clean_up_temp_files`  | `bool`            | Whether to delete temporary files after merging.                                                         | `True`                    |\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Execution Steps**\n",
        "\n",
        "1. **Define Configuration**: Modify the `BigQueryConfig` object with your specific parameters:\n",
        "   ```python\n",
        "   config = BigQueryConfig(\n",
        "       project_id='your-project-id',\n",
        "       source_project='source-project-id',\n",
        "       dataset_id='dataset-id',\n",
        "       table_id='table-id',\n",
        "       output_format='parquet',\n",
        "       output_path='./data',\n",
        "       max_bytes_billed=100 * 1024 * 1024 * 1024,  # 100GB\n",
        "       initial_chunk_size=200_000,\n",
        "       max_workers=4,\n",
        "       clean_up_temp_files=True\n",
        "   )\n",
        "   ```\n",
        "\n",
        "2. **Run the Script**: Execute the extraction using:\n",
        "   ```python\n",
        "   total_rows, final_file = extract_bigquery_data(config)\n",
        "   print(f\"Total Rows: {total_rows}\")\n",
        "   print(f\"Output File: {final_file}\")\n",
        "   ```\n",
        "\n",
        "3. **Logging**: Check the logs for progress and errors. Logs are dynamically displayed in the notebook environment.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Key Functions**\n",
        "\n",
        "| **Function**                         | **Description**                                                                                         |\n",
        "|--------------------------------------|---------------------------------------------------------------------------------------------------------|\n",
        "| `extract_bigquery_data(config)`      | Main entry point. Extracts data from BigQuery and saves it in the desired format.                       |\n",
        "| `_build_query(offset)`               | Builds paginated SQL queries for chunked data extraction.                                              |\n",
        "| `_fetch_and_save_chunk(offset, ...)` | Fetches a data chunk and saves it to disk in the configured format.                                     |\n",
        "| `_merge_to_final_format()`           | Merges all chunks into a single file based on the specified output format.                              |\n",
        "| `_get_optimal_stream_config()`       | Dynamically calculates streaming and memory configurations based on available resources.                |\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Parameters Available via GitHub Artefacts**\n",
        "\n",
        "If your environment supports GitHub integration:\n",
        "1. Clone the repository:\n",
        "   ```bash\n",
        "   git clone <repository-url>\n",
        "   cd <repository-directory>\n",
        "   ```\n",
        "2. Run the script locally or modify it as needed.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Notes and Best Practices**\n",
        "\n",
        "- **Chunk Size**: Adjust `chunk_size` for low-memory environments to prevent memory overruns.\n",
        "- **Output Format**: Use `hyper` format for Tableau, `parquet` for efficient storage, or `csv` for compatibility.\n",
        "- **Error Handling**: The script includes robust retry mechanisms with exponential backoff.\n",
        "- **Parallel Processing**: Increase `max_workers` for faster extraction, but ensure sufficient CPU and memory availability.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Example Use Case**\n",
        "\n",
        "Extracting GitHub timeline data into a Tableau `.hyper` file:\n",
        "```python\n",
        "config = BigQueryConfig(\n",
        "    project_id='my-project-id',\n",
        "    source_project='bigquery-public-data',\n",
        "    dataset_id='samples',\n",
        "    table_id='github_timeline',\n",
        "    output_format='hyper',\n",
        "    output_path='/content/data',\n",
        "    max_bytes_billed=10 * 1024 * 1024 * 1024,  # 10GB\n",
        "    chunk_size=200_000,\n",
        "    max_workers=4\n",
        ")\n",
        "total_rows, final_file = extract_bigquery_data(config)\n",
        "```\n",
        "\n",
        "Output:\n",
        "```\n",
        "Total Rows: 500,000\n",
        "Output File: /content/data/github_timeline_complete_<timestamp>.hyper\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "By following this documentation, you can confidently configure and execute the script for efficient data extraction from Google BigQuery to your desired format. Let me know if you need further assistance!"
      ],
      "metadata": {
        "id": "zSR-OM5txf6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install google-auth google-auth-oauthlib google-auth-httplib2 google-cloud-bigquery \\\n",
        "    pandas pantab psutil pyarrow pandas-gbq google-cloud-core google-cloud-storage \\\n",
        "    google-api-core google-auth-httplib2 google-api-python-client tableauhyperapi"
      ],
      "metadata": {
        "id": "pVi7V3Sbxleu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaVuCyPXxevA"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import gc\n",
        "import logging\n",
        "import time\n",
        "from datetime import datetime\n",
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "import pandas_gbq\n",
        "import numpy as np\n",
        "from typing import List, Optional, Tuple, Dict, Any\n",
        "from dataclasses import dataclass\n",
        "from IPython.display import clear_output, display, HTML\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import psutil\n",
        "import pantab\n",
        "import json\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from pathlib import Path\n",
        "from google.cloud import bigquery_storage\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CoLabLogHandler(logging.Handler):\n",
        "    \"\"\"Custom log handler to store logs in memory for display.\"\"\"\n",
        "    def __init__(self, max_lines=1000):\n",
        "        super().__init__()\n",
        "        self.log_buffer = []\n",
        "        self.max_lines = max_lines\n",
        "\n",
        "    def emit(self, record):\n",
        "        \"\"\"Emit a log record.\"\"\"\n",
        "        log_entry = self.format(record)\n",
        "        self.log_buffer.append(log_entry)\n",
        "        if len(self.log_buffer) > self.max_lines:\n",
        "            self.log_buffer = self.log_buffer[-self.max_lines:]\n",
        "\n",
        "        # Immediately update log display\n",
        "        self._update_display()\n",
        "\n",
        "    def get_logs(self):\n",
        "        \"\"\"Get all stored logs.\"\"\"\n",
        "        return '\\n'.join(self.log_buffer)\n",
        "\n",
        "    def _update_display(self):\n",
        "        \"\"\"Update log display in the notebook.\"\"\"\n",
        "        clear_output(wait=True)\n",
        "        logs_html = \"<br>\".join(self.log_buffer[-20:])  # Display only the last 20 logs for readability\n",
        "        display(HTML(f\"\"\"\n",
        "        <div style=\"font-family:monospace; font-size:12px; background-color:#f8f9fa;\n",
        "                    border:1px solid #ddd; padding:10px; max-height:200px; overflow-y:auto;\">\n",
        "            {logs_html}\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BigQueryConfig:\n",
        "    \"\"\"Configuration for BigQuery extraction.\"\"\"\n",
        "    project_id: str\n",
        "    source_project: str\n",
        "    dataset_id: str\n",
        "    table_id: str\n",
        "    output_format: str = 'hyper'\n",
        "    output_path: str = './data'\n",
        "    max_bytes_billed: int = 100 * 1024 * 1024 * 1024  # 100GB\n",
        "    initial_chunk_size: int = 500_000\n",
        "    max_workers: int = 4\n",
        "    chunk_size: Optional[int] = None\n",
        "    columns: Optional[List[str]] = None\n",
        "    where_clause: Optional[str] = None\n",
        "    clean_up_temp_files: bool = True\n",
        "    max_rows: Optional[int] = None\n",
        "    hyper_batch_size: int = 100000  # Add this here with other defaults\n",
        "    max_memory_gb: float = 0.8  # And this here\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validate configuration parameters.\"\"\"\n",
        "        if self.output_format not in ['hyper', 'parquet', 'csv']:\n",
        "            raise ValueError(\"output_format must be one of: hyper, parquet, csv\")\n",
        "        if self.initial_chunk_size <= 0:\n",
        "            raise ValueError(\"chunk_size must be positive\")\n",
        "        if self.max_workers <= 0:\n",
        "            raise ValueError(\"max_workers must be positive\")\n",
        "        if self.max_rows is not None and self.max_rows <= 0:\n",
        "            raise ValueError(\"max_rows must be positive if specified\")\n",
        "        self.output_path = str(Path(self.output_path).resolve())\n",
        "\n",
        "class BigQueryExtractor:\n",
        "    \"\"\"Enhanced BigQuery data extraction utility.\"\"\"\n",
        "\n",
        "    def __init__(self, config: BigQueryConfig):\n",
        "        \"\"\"Initialize extractor with configuration.\"\"\"\n",
        "        self.config = config\n",
        "\n",
        "        # Initialize chunk_size early to avoid access issues\n",
        "        self.chunk_size = config.initial_chunk_size\n",
        "\n",
        "        self._ensure_output_directory()\n",
        "\n",
        "        # Initialize BigQuery client first\n",
        "        self.client = self._initialize_client()\n",
        "\n",
        "        # Fetch schema information\n",
        "        self.schema = self._get_schema()\n",
        "\n",
        "        # Initialize BigQuery Storage client\n",
        "        self.bq_storage_client = bigquery_storage.BigQueryReadClient()\n",
        "\n",
        "        # Setup logging\n",
        "        self.log_handler = CoLabLogHandler(max_lines=100)\n",
        "        self.log_handler.setFormatter(\n",
        "            logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
        "        )\n",
        "        logger.addHandler(self.log_handler)\n",
        "\n",
        "        logger.info(f\"Adjusted chunk size: {self.chunk_size} rows\")\n",
        "        logger.info(\"BigQuery connection initialized. Fetching schema information completed.\")\n",
        "\n",
        "        # Initialize other attributes\n",
        "        self.total_rows = 0\n",
        "        self.processed_chunks = 0\n",
        "        self.failed_chunks = []\n",
        "        self.start_time = time.time()\n",
        "        self._setup_progress_display()\n",
        "\n",
        "    def _preprocess_dataframe(self, df: pd.DataFrame, column_types: Dict[str, str]) -> pd.DataFrame:\n",
        "        \"\"\"Preprocess the DataFrame to handle types and NaNs efficiently.\"\"\"\n",
        "        if df.empty:\n",
        "            return df\n",
        "\n",
        "        for column in df.columns:\n",
        "            if df[column].isna().all():  # If column is entirely NaN\n",
        "                if column_types.get(column, '').upper() in ['INTEGER', 'FLOAT']:\n",
        "                    df[column] = 0  # Default to 0 for numeric types\n",
        "                else:\n",
        "                    df[column] = ''  # Default to empty string for non-numeric types\n",
        "            elif pd.api.types.is_object_dtype(df[column]):  # Object type columns\n",
        "                df[column] = df[column].astype(str).fillna(\"\")  # Convert to string and fill NaNs\n",
        "            elif pd.api.types.is_float_dtype(df[column]):  # Float columns with NaNs\n",
        "                df[column] = df[column].fillna(0.0)\n",
        "            elif pd.api.types.is_integer_dtype(df[column]):  # Integer columns with NaNs\n",
        "                df[column] = df[column].fillna(0)\n",
        "            elif column_types.get(column, '').upper() in ['DATE', 'TIMESTAMP']:  # Ensure datetime consistency\n",
        "                df[column] = pd.to_datetime(df[column], errors='coerce')\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    def preprocess_dataframe(df: pd.DataFrame, schema: List[bigquery.SchemaField]) -> pd.DataFrame:\n",
        "        \"\"\"Preprocess the DataFrame to handle types and NaNs efficiently.\"\"\"\n",
        "        type_mapping = {\n",
        "            'STRING': str,\n",
        "            'BYTES': str,\n",
        "            'INTEGER': 'Int',\n",
        "            'INT64': 'Int64',\n",
        "            'FLOAT': 'float',\n",
        "            'FLOAT64': 'float64',\n",
        "            'NUMERIC': 'float64',\n",
        "            'BIGNUMERIC': 'float64',\n",
        "            'BOOLEAN': 'boolean',\n",
        "            'BOOL': 'boolean',\n",
        "            'DATE': 'datetime64[ns]',\n",
        "            'DATETIME': 'datetime64[ns]',\n",
        "            'TIME': str,\n",
        "            'TIMESTAMP': 'datetime64[ns]',\n",
        "            'RECORD': str,\n",
        "            'STRUCT': str,\n",
        "            'ARRAY': str,\n",
        "            'GEOGRAPHY': str\n",
        "        }\n",
        "\n",
        "        for column in df.columns:\n",
        "            if df[column].isna().all():  # If the column is entirely NaN\n",
        "                if schema[column] in ['INTEGER', 'FLOAT']:\n",
        "                    df[column] = 0  # Default to 0 for numeric types\n",
        "                else:\n",
        "                    df[column] = ''  # Default to empty string for non-numeric types\n",
        "            elif pd.api.types.is_object_dtype(df[column]):  # Object type columns\n",
        "                df[column] = df[column].astype(str).fillna(\"\")  # Convert to string and fill\n",
        "            elif pd.api.types.is_float_dtype(df[column]):  # Float columns with NaNs\n",
        "                df[column] = df[column].fillna(0)\n",
        "            elif pd.api.types.is_integer_dtype(df[column]):  # Integer columns with NaNs\n",
        "                df[column] = df[column].fillna(0)\n",
        "            elif schema[column] in ['DATE', 'TIMESTAMP']:  # Ensure datetime consistency\n",
        "                df[column] = pd.to_datetime(df[column], errors='coerce')\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _ensure_output_directory(self):\n",
        "        \"\"\"Create output directory if it doesn't exist.\"\"\"\n",
        "        os.makedirs(self.config.output_path, exist_ok=True)\n",
        "\n",
        "    def _initialize_client(self) -> bigquery.Client:\n",
        "        \"\"\"Initialize BigQuery client with authentication.\"\"\"\n",
        "        try:\n",
        "            auth.authenticate_user()\n",
        "            return bigquery.Client(project=self.config.project_id)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize BigQuery client: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _get_schema(self) -> List[bigquery.SchemaField]:\n",
        "        \"\"\"Get table schema information.\"\"\"\n",
        "        try:\n",
        "            dataset_ref = self.client.dataset(self.config.dataset_id,\n",
        "                                            project=self.config.source_project)\n",
        "            table_ref = dataset_ref.table(self.config.table_id)\n",
        "            return self.client.get_table(table_ref).schema\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to get schema: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _get_bq_type_mapping(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get mapping of BigQuery data types to Python/Pandas types.\"\"\"\n",
        "        return {\n",
        "            'STRING': str,\n",
        "            'BYTES': str,\n",
        "            'INTEGER': 'Int64',\n",
        "            'INT64': 'Int64',\n",
        "            'FLOAT': 'float64',\n",
        "            'FLOAT64': 'float64',\n",
        "            'NUMERIC': 'float64',\n",
        "            'BIGNUMERIC': 'float64',\n",
        "            'BOOLEAN': 'boolean',\n",
        "            'BOOL': 'boolean',\n",
        "            'DATE': 'datetime64[ns]',\n",
        "            'DATETIME': 'datetime64[ns]',\n",
        "            'TIME': str,\n",
        "            'TIMESTAMP': 'datetime64[ns]',\n",
        "            'RECORD': str,\n",
        "            'STRUCT': str,\n",
        "            'ARRAY': str,\n",
        "            'GEOGRAPHY': str\n",
        "        }\n",
        "\n",
        "    def _setup_progress_display(self):\n",
        "        \"\"\"Initialize progress display styling.\"\"\"\n",
        "        display(HTML(\"\"\"\n",
        "        <style>\n",
        "            .bq-progress {\n",
        "                font-family: monospace;\n",
        "                padding: 10px;\n",
        "                border: 1px solid #ccc;\n",
        "                border-radius: 4px;\n",
        "                margin: 10px 0;\n",
        "                background-color: #f8f9fa;\n",
        "            }\n",
        "            .progress-bar {\n",
        "                color: #fff;\n",
        "                background-color: #28a745;\n",
        "                height: 20px;\n",
        "                border-radius: 3px;\n",
        "                transition: width 0.3s ease;\n",
        "                text-align: center;\n",
        "                line-height: 20px;\n",
        "            }\n",
        "            .log-container {\n",
        "                font-family: monospace;\n",
        "                padding: 10px;\n",
        "                border: 1px solid #ddd;\n",
        "                border-radius: 4px;\n",
        "                margin: 10px 0;\n",
        "                background-color: #f8f9fa;\n",
        "                max-height: 200px;\n",
        "                overflow-y: auto;\n",
        "                white-space: pre-wrap;\n",
        "                font-size: 12px;\n",
        "            }\n",
        "            .log-entry {\n",
        "                margin: 2px 0;\n",
        "            }\n",
        "            .log-error { color: #dc3545; }\n",
        "            .log-warning { color: #ffc107; }\n",
        "            .log-info { color: #17a2b8; }\n",
        "        </style>\n",
        "        \"\"\"))\n",
        "\n",
        "    @staticmethod\n",
        "    def adjust_chunk_size(schema: List[bigquery.SchemaField], available_memory: int) -> int:\n",
        "        \"\"\"\n",
        "        Dynamically adjust chunk size based on memory and schema details.\n",
        "        Adds a cap to avoid overly large chunk sizes.\n",
        "        \"\"\"\n",
        "        # Estimated memory usage per data type in bytes\n",
        "        type_memory_footprint = {\n",
        "            'STRING': 100,        # Average string size\n",
        "            'BYTES': 50,          # Binary data\n",
        "            'INTEGER': 8,         # 64-bit integers\n",
        "            'INT64': 8,           # 64-bit integers\n",
        "            'FLOAT': 8,           # 64-bit floats\n",
        "            'FLOAT64': 8,         # 64-bit floats\n",
        "            'NUMERIC': 16,        # Numeric with higher precision\n",
        "            'BIGNUMERIC': 32,     # Big numeric\n",
        "            'BOOLEAN': 1,         # Boolean\n",
        "            'DATE': 4,            # Dates\n",
        "            'DATETIME': 8,        # Datetime objects\n",
        "            'TIMESTAMP': 8,       # Timestamp\n",
        "            'RECORD': 200,        # Nested structure (average)\n",
        "            'STRUCT': 200,        # Nested structure (average)\n",
        "            'ARRAY': 150,         # Array (average per element)\n",
        "            'GEOGRAPHY': 1000     # Geography data\n",
        "        }\n",
        "\n",
        "        # Calculate total memory usage per row based on schema\n",
        "        total_memory_per_row = sum(\n",
        "            type_memory_footprint.get(field.field_type, 50) for field in schema\n",
        "        )\n",
        "\n",
        "        # Account for overhead and concurrency\n",
        "        row_memory_with_overhead = total_memory_per_row * 1.1  # Add 10% buffer\n",
        "\n",
        "        # Calculate max rows based on available memory\n",
        "        max_rows = available_memory // row_memory_with_overhead\n",
        "\n",
        "        # Cap chunk size to avoid excessive memory usage\n",
        "        capped_chunk_size = min(max_rows, 1_000_000)  # Cap at 100,000 rows\n",
        "\n",
        "        # Log intermediate values for debugging\n",
        "        logger.info(f\"Available memory: {available_memory} bytes\")\n",
        "        logger.info(f\"Estimated memory per row: {row_memory_with_overhead:.2f} bytes\")\n",
        "        logger.info(f\"Calculated chunk size: {max_rows} rows\")\n",
        "        logger.info(f\"Capped chunk size: {capped_chunk_size} rows\")\n",
        "\n",
        "        # Ensure a minimum chunk size to avoid inefficient queries\n",
        "        # return max(100_000, int(capped_chunk_size)) #10_000\n",
        "        return max(50_000, min(500_000, max_rows))\n",
        "\n",
        "    def _count_records(self) -> int:\n",
        "        \"\"\"Count total records in table.\"\"\"\n",
        "        query = f\"\"\"\n",
        "        SELECT COUNT(*) as total\n",
        "        FROM `{self.config.source_project}.{self.config.dataset_id}.{self.config.table_id}`\n",
        "        \"\"\"\n",
        "        if self.config.where_clause:\n",
        "            query += f\" WHERE {self.config.where_clause}\"\n",
        "\n",
        "        #df = pandas_gbq.read_gbq(query, project_id=self.config.project_id, use_bqstorage_api=True)\n",
        "        #total = int(df['total'].iloc[0])\n",
        "        query_job = self.client.query(query)\n",
        "        result = query_job.result()  # Wait for the query to complete\n",
        "        total = next(iter(result)).get('total', 0)\n",
        "\n",
        "        logger.info(f\"Total records to process: {total:,}\")\n",
        "        return total\n",
        "\n",
        "    def _build_query(self, offset: int) -> str:\n",
        "        \"\"\"Build optimized BigQuery query.\"\"\"\n",
        "        columns = self.config.columns or [field.name for field in self.schema]\n",
        "\n",
        "        query = f\"\"\"\n",
        "        SELECT {', '.join(columns)}\n",
        "        FROM `{self.config.source_project}.{self.config.dataset_id}.{self.config.table_id}`\n",
        "        \"\"\"\n",
        "\n",
        "        if self.config.where_clause:\n",
        "            query += f\" WHERE {self.config.where_clause}\"\n",
        "\n",
        "        # Simple LIMIT/OFFSET without ORDER BY to avoid memory issues\n",
        "        query += f\"\"\"\n",
        "        LIMIT {self.chunk_size}\n",
        "        OFFSET {offset}\n",
        "        \"\"\"\n",
        "\n",
        "        return query\n",
        "\n",
        "    def _process_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process the DataFrame to handle types and NaNs efficiently.\"\"\"\n",
        "        if df.empty:\n",
        "            return df\n",
        "\n",
        "        # Get schema type mappings for BigQuery table\n",
        "        column_types = {field.name: field.field_type.upper() for field in self.schema}\n",
        "\n",
        "        # Preprocess DataFrame using class method\n",
        "        df = self._preprocess_dataframe(df, column_types)\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "    def _save_chunk(self, df: pd.DataFrame, chunk_num: int) -> str:\n",
        "        \"\"\"Save data chunk with enhanced type handling.\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        base_filename = f\"{self.config.table_id}_chunk_{chunk_num}_{timestamp}\"\n",
        "\n",
        "        try:\n",
        "            filename = f\"{base_filename}.parquet\"\n",
        "            full_path = f\"{self.config.output_path}/{filename}\"\n",
        "\n",
        "            # Convert to Arrow Table and write with compression\n",
        "            table = pa.Table.from_pandas(df)\n",
        "            pq.write_table(table, full_path, compression='snappy')  # Snappy compression\n",
        "\n",
        "            logger.debug(f\"Saved chunk {chunk_num} to {full_path}\")\n",
        "            return filename\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to save chunk: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def _update_progress(self, total_chunks: int, throttle: int = 1):\n",
        "        \"\"\"Throttle progress updates to reduce I/O overhead.\"\"\"\n",
        "        if self.processed_chunks % throttle != 0:\n",
        "            return\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        elapsed_time = time.time() - self.start_time\n",
        "        progress = (self.processed_chunks / total_chunks) * 100\n",
        "\n",
        "        rows_per_second = self.total_rows / elapsed_time if elapsed_time > 0 else 0\n",
        "        memory_info = psutil.Process(os.getpid()).memory_info()\n",
        "        memory_usage_mb = memory_info.rss / 1024 / 1024\n",
        "\n",
        "        progress_html = f\"\"\"\n",
        "        <div style=\"font-family:monospace; font-size:12px; padding:10px;\">\n",
        "            <div><strong>Progress:</strong> {progress:.1f}%</div>\n",
        "            <div><strong>Processed Chunks:</strong> {self.processed_chunks}/{total_chunks}</div>\n",
        "            <div><strong>Rows Processed:</strong> {self.total_rows:,}</div>\n",
        "            <div><strong>Processing Speed:</strong> {rows_per_second:.1f} rows/sec</div>\n",
        "            <div><strong>Memory Usage:</strong> {memory_usage_mb:.1f} MB</div>\n",
        "            <div><strong>Elapsed Time:</strong> {elapsed_time:.1f} seconds</div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(progress_html))\n",
        "\n",
        "\n",
        "    def _fetch_and_save_chunk(self, offset: int, chunk_num: int) -> Optional[str]:\n",
        "        \"\"\"Fetch and save a single chunk with optimized retry logic and memory management.\"\"\"\n",
        "        max_retries = 3\n",
        "        original_chunk_size = self.chunk_size\n",
        "        min_chunk_size = 50_000  # Minimum chunk size to maintain efficiency\n",
        "\n",
        "        def get_memory_usage_mb():\n",
        "            \"\"\"Get current memory usage in MB.\"\"\"\n",
        "            return psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
        "\n",
        "        def _adjust_chunk_size(error_type: str, attempt: int) -> int:\n",
        "            \"\"\"Adjust chunk size based on error type and retry attempt.\"\"\"\n",
        "            reduction_factor = {\n",
        "                'memory': 2,      # Reduce chunk size more for memory errors\n",
        "                'timeout': 1.5,   # Moderate reduction for timeout errors\n",
        "                'quota': 3        # Aggressive reduction for quota issues\n",
        "            }.get(error_type.lower(), 1.5)  # Default reduction factor\n",
        "\n",
        "            new_size = max(\n",
        "                original_chunk_size // (reduction_factor * (attempt + 1)),\n",
        "                min_chunk_size\n",
        "            )\n",
        "            logger.info(f\"Adjusted chunk size to {new_size:,} rows due to {error_type} error.\")\n",
        "            return new_size\n",
        "\n",
        "        def preprocess_dataframe(df: pd.DataFrame, schema: Dict[str, str]) -> pd.DataFrame:\n",
        "            \"\"\"Preprocess the DataFrame to handle types and NaNs efficiently.\"\"\"\n",
        "            for column in df.columns:\n",
        "                if pd.api.types.is_object_dtype(df[column]):  # Object type columns\n",
        "                    df[column] = df[column].astype(str).fillna(\"\")  # Convert to string and fill NaNs\n",
        "                elif pd.api.types.is_float_dtype(df[column]):  # Float columns with NaNs\n",
        "                    df[column] = df[column].fillna(0.0)\n",
        "                elif pd.api.types.is_integer_dtype(df[column]):  # Integer columns with NaNs\n",
        "                    df[column] = df[column].fillna(0)\n",
        "                elif schema.get(column, '').upper() in ['DATE', 'TIMESTAMP']:  # Ensure datetime consistency\n",
        "                    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
        "            return df\n",
        "\n",
        "        initial_memory = get_memory_usage_mb()\n",
        "        last_error = None\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                if attempt > 0:\n",
        "                    # Retry logic: Adjust chunk size for subsequent attempts\n",
        "                    logger.info(f\"Retrying chunk {chunk_num} (Attempt {attempt + 1}/{max_retries})\")\n",
        "                    self.chunk_size = _adjust_chunk_size(str(last_error), attempt)\n",
        "\n",
        "                # Build and execute query\n",
        "                query = self._build_query(offset)\n",
        "                df_chunk = self.client.query(query).to_dataframe()  # Fetch data as a DataFrame\n",
        "                if df_chunk.empty:\n",
        "                    logger.warning(f\"Chunk {chunk_num} is empty. Skipping.\")\n",
        "                    return None  # Skip saving empty chunks\n",
        "\n",
        "                # Update total rows processed\n",
        "                self.total_rows += len(df_chunk)\n",
        "\n",
        "                # Preprocess and process columns\n",
        "                column_schema = {field.name: field.field_type for field in self.schema}\n",
        "                df_chunk = preprocess_dataframe(df_chunk, column_schema)\n",
        "\n",
        "                # Save the chunk\n",
        "                saved_file = self._save_chunk(df_chunk, chunk_num)\n",
        "                logger.info(f\"Successfully processed and saved chunk {chunk_num}\")\n",
        "\n",
        "                # **Release memory after saving**\n",
        "                del df_chunk  # Delete the chunk DataFrame\n",
        "                gc.collect()  # Trigger garbage collection\n",
        "\n",
        "                return saved_file\n",
        "\n",
        "            except Exception as e:\n",
        "                # Handle errors and determine backoff strategy\n",
        "                last_error = e\n",
        "                logger.error(f\"Error processing chunk {chunk_num}: {str(e)}\")\n",
        "\n",
        "                if attempt == max_retries - 1:\n",
        "                    logger.error(f\"Max retries reached for chunk {chunk_num}. Marking as failed.\")\n",
        "                    self.failed_chunks.append(chunk_num)\n",
        "                    return None  # Return failure after max retries\n",
        "\n",
        "                # Exponential backoff for retries\n",
        "                backoff_time = min(2 ** attempt, 30)  # Cap backoff at 30 seconds\n",
        "                logger.info(f\"Backing off for {backoff_time} seconds before retrying...\")\n",
        "                time.sleep(backoff_time)\n",
        "\n",
        "            finally:\n",
        "                # Clean up memory if needed\n",
        "                current_memory = get_memory_usage_mb()\n",
        "                if current_memory > initial_memory * 1.7:  # Memory exceeds 70% of the initial usage\n",
        "                    logger.warning(\"High memory usage detected. Triggering garbage collection.\")\n",
        "                    gc.collect()\n",
        "\n",
        "        # Reset chunk size before exiting\n",
        "        self.chunk_size = original_chunk_size\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _get_optimal_stream_config(self) -> Dict[str, Any]:\n",
        "        \"\"\"Calculate optimal streaming configuration based on available resources.\"\"\"\n",
        "        try:\n",
        "            available_memory = psutil.virtual_memory()\n",
        "            total_memory_gb = available_memory.total / (1024**3)\n",
        "            available_memory_gb = available_memory.available / (1024**3)\n",
        "\n",
        "            # Reserve 20% memory for system and overhead\n",
        "            usable_memory_gb = available_memory_gb * 0.8\n",
        "\n",
        "            # Calculate optimal stream count and batch size\n",
        "            optimal_streams = max(1, min(\n",
        "                int(usable_memory_gb / 1.5),  # 1.5GB per stream\n",
        "                os.cpu_count() * 2,  # Double CPU count\n",
        "                8  # Hard cap at 8 streams\n",
        "            ))\n",
        "\n",
        "            # For 12GB Colab machine, adjust batch size accordingly\n",
        "            rows_per_gb = 250000  # Estimated rows per GB\n",
        "            optimal_batch_size = int((usable_memory_gb / optimal_streams) * rows_per_gb)\n",
        "\n",
        "            # Cap batch size based on total memory\n",
        "            max_safe_batch = min(\n",
        "                optimal_batch_size,\n",
        "                500000 if total_memory_gb <= 13 else 1000000  # Lower batch size for Colab\n",
        "            )\n",
        "\n",
        "            logger.info(f\"Memory Configuration:\")\n",
        "            logger.info(f\"- Total RAM: {total_memory_gb:.1f}GB\")\n",
        "            logger.info(f\"- Available: {available_memory_gb:.1f}GB\")\n",
        "            logger.info(f\"- Usable: {usable_memory_gb:.1f}GB\")\n",
        "            logger.info(f\"- Streams: {optimal_streams}\")\n",
        "            logger.info(f\"- Batch Size: {max_safe_batch:,} rows\")\n",
        "\n",
        "            return {\n",
        "                'stream_count': optimal_streams,\n",
        "                'batch_size': max_safe_batch,\n",
        "                'memory_limit': int(usable_memory_gb * 1024 * 1024 * 1024),  # in bytes\n",
        "                'total_memory_gb': total_memory_gb,\n",
        "                'available_memory_gb': available_memory_gb\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calculating stream configuration: {str(e)}\")\n",
        "            # Provide safe default values\n",
        "            return {\n",
        "                'stream_count': 2,\n",
        "                'batch_size': 250000,\n",
        "                'memory_limit': 8 * 1024 * 1024 * 1024,  # 8GB\n",
        "                'total_memory_gb': 12,\n",
        "                'available_memory_gb': 8\n",
        "            }\n",
        "\n",
        "    def _merge_to_final_format(self, saved_files: List[str]) -> str:\n",
        "        \"\"\"Merge chunks into final output format with improved memory handling.\"\"\"\n",
        "        final_filename = f\"{self.config.table_id}_complete_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "        try:\n",
        "            if self.config.output_format == 'hyper':\n",
        "                final_path = f\"{self.config.output_path}/{final_filename}.hyper\"\n",
        "                import pantab\n",
        "\n",
        "                logger.info(\"Starting merge to Hyper format...\")\n",
        "                first_write = True\n",
        "\n",
        "                for file in saved_files:\n",
        "                    if file:\n",
        "                        try:\n",
        "                            # Read parquet file in chunks\n",
        "                            pq_file = pq.ParquetFile(f\"{self.config.output_path}/{file}\")\n",
        "                            num_row_groups = pq_file.num_row_groups\n",
        "\n",
        "                            for i in range(num_row_groups):\n",
        "                                df = pq_file.read_row_group(i).to_pandas()\n",
        "\n",
        "                                if not df.empty and not df.isna().all().all():\n",
        "                                    # Process columns\n",
        "                                    df = self._process_columns(df)\n",
        "\n",
        "                                    # Convert all object columns to string\n",
        "                                    for col in df.select_dtypes(include=['object']).columns:\n",
        "                                        df[col] = df[col].astype(str)\n",
        "\n",
        "                                    # Write to hyper file\n",
        "                                    if first_write:\n",
        "                                        # First write creates the file\n",
        "                                        pantab.frame_to_hyper(\n",
        "                                            df,\n",
        "                                            final_path,\n",
        "                                            table=self.config.table_id,\n",
        "                                            table_mode='w'  # Write mode for first chunk\n",
        "                                        )\n",
        "                                        first_write = False\n",
        "                                    else:\n",
        "                                        # Subsequent writes append to the file\n",
        "                                        pantab.frame_to_hyper(\n",
        "                                            df,\n",
        "                                            final_path,\n",
        "                                            table=self.config.table_id,\n",
        "                                            table_mode='a'  # Append mode for subsequent chunks\n",
        "                                        )\n",
        "\n",
        "                                    # Force cleanup\n",
        "                                    del df\n",
        "                                    gc.collect()\n",
        "\n",
        "                        except Exception as e:\n",
        "                            logger.error(f\"Error processing file {file}: {str(e)}\")\n",
        "                            continue\n",
        "\n",
        "                logger.info(\"Completed Hyper file creation\")\n",
        "\n",
        "            elif self.config.output_format == 'csv':\n",
        "                final_path = f\"{self.config.output_path}/{final_filename}.csv\"\n",
        "                first_chunk = True\n",
        "                for file in saved_files:\n",
        "                    if file:\n",
        "                        df = pd.read_parquet(f\"{self.config.output_path}/{file}\")\n",
        "                        if not df.empty and not df.isna().all().all():  # Exclude empty or all-NA DataFrames\n",
        "                            df.to_csv(final_path, mode='w' if first_chunk else 'a',\n",
        "                                      header=first_chunk, index=False)\n",
        "                            first_chunk = False\n",
        "\n",
        "            elif self.config.output_format == 'parquet':\n",
        "                final_path = f\"{self.config.output_path}/{final_filename}.parquet\"\n",
        "                dfs = []\n",
        "                total_size = 0\n",
        "                max_batch_size = 1000000\n",
        "\n",
        "                for file in saved_files:\n",
        "                    if file:\n",
        "                        df = pd.read_parquet(f\"{self.config.output_path}/{file}\")\n",
        "                        if not df.empty and not df.isna().all().all():  # Exclude empty or all-NA DataFrames\n",
        "                            dfs.append(df)\n",
        "                            total_size += len(df)\n",
        "\n",
        "                            if total_size >= max_batch_size:\n",
        "                                combined_df = pd.concat(dfs, ignore_index=True)\n",
        "                                combined_df.to_parquet(final_path)\n",
        "                                dfs = []\n",
        "                                total_size = 0\n",
        "\n",
        "                if dfs:\n",
        "                    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "                    combined_df.to_parquet(final_path)\n",
        "\n",
        "            # Clean up temporary files if requested\n",
        "            if self.config.clean_up_temp_files:\n",
        "                for file in saved_files:\n",
        "                    if file:\n",
        "                        try:\n",
        "                            os.remove(f\"{self.config.output_path}/{file}\")\n",
        "                            logger.debug(f\"Removed temporary file: {file}\")\n",
        "                        except Exception as e:\n",
        "                            logger.warning(f\"Failed to remove temporary file {file}: {str(e)}\")\n",
        "\n",
        "            return f\"{final_filename}.{self.config.output_format}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error merging files: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def extract_data(self) -> Tuple[int, str]:\n",
        "        \"\"\"Main extraction method using parallelized chunk-based approach.\"\"\"\n",
        "        try:\n",
        "            total_records = self._count_records()\n",
        "            logger.info(f\"Total records to process: {total_records:,}\")\n",
        "            if total_records == 0:\n",
        "                logger.warning(\"No records found to extract\")\n",
        "                return 0, None\n",
        "\n",
        "            offsets = range(0, total_records, self.chunk_size)\n",
        "            results = []\n",
        "            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:\n",
        "                futures = [\n",
        "                    executor.submit(self._fetch_and_save_chunk, offset, chunk_num)\n",
        "                    for chunk_num, offset in enumerate(offsets)\n",
        "                ]\n",
        "                for future in as_completed(futures):\n",
        "                    try:\n",
        "                        result = future.result()\n",
        "                        if result:\n",
        "                            results.append(result)\n",
        "                        self.processed_chunks += 1\n",
        "                        self._update_progress(len(offsets))  # Update progress dynamically\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"Error in chunk processing: {e}\")\n",
        "            gc.collect()\n",
        "\n",
        "            if not results:\n",
        "                logger.warning(\"No data was successfully extracted.\")\n",
        "                return 0, None\n",
        "\n",
        "            final_path = self._merge_to_final_format(results)\n",
        "            logger.info(f\"Successfully created final file: {final_path}\")\n",
        "            return len(results), final_path\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during extraction: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def _stream_to_parquet(self, output_path: str) -> int:\n",
        "        \"\"\"Stream directly to parquet with memory optimization.\"\"\"\n",
        "        config = self._get_optimal_stream_config()\n",
        "        writer = None\n",
        "        total_rows = 0\n",
        "\n",
        "        try:\n",
        "            session = self._create_read_session(config['stream_count'])\n",
        "\n",
        "            for stream in session.streams:\n",
        "                reader = self.bq_storage_client.read_rows(stream.name)\n",
        "\n",
        "                for batch in reader.rows().pages:\n",
        "                    # Convert RecordBatch to Table\n",
        "                    record_batch = batch.to_arrow()\n",
        "                    arrow_table = pa.Table.from_batches([record_batch])  # Convert to Table\n",
        "\n",
        "                    if writer is None:\n",
        "                        # Initialize writer with schema from the first Arrow table\n",
        "                        writer = pq.ParquetWriter(\n",
        "                            output_path,\n",
        "                            arrow_table.schema,  # Use schema from the first Arrow table\n",
        "                            compression='snappy',\n",
        "                            use_dictionary=True,\n",
        "                            write_statistics=True\n",
        "                        )\n",
        "\n",
        "                    # Write the Arrow Table to Parquet\n",
        "                    writer.write_table(arrow_table)\n",
        "                    total_rows += arrow_table.num_rows\n",
        "\n",
        "                    self._update_progress_streaming(total_rows)\n",
        "\n",
        "            if writer:\n",
        "                writer.close()\n",
        "\n",
        "            return total_rows\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in parquet streaming: {str(e)}\")\n",
        "            if writer:\n",
        "                writer.close()\n",
        "            raise\n",
        "\n",
        "    def _stream_to_hyper(self, output_path: str) -> int:\n",
        "        \"\"\"Stream to hyper format with memory optimization and row limit.\"\"\"\n",
        "        config = self._get_optimal_stream_config()\n",
        "        total_rows = 0\n",
        "        batch_rows = []\n",
        "        current_size = 0\n",
        "\n",
        "        try:\n",
        "            session = self._create_read_session(config['stream_count'])\n",
        "\n",
        "            for stream in session.streams:\n",
        "                if self.config.max_rows and total_rows >= self.config.max_rows:\n",
        "                    break\n",
        "\n",
        "                reader = self.bq_storage_client.read_rows(stream.name)\n",
        "\n",
        "                for batch in reader.rows().pages:\n",
        "                    # Convert ReadRowsPage to Pandas DataFrame\n",
        "                    df = batch.to_arrow().to_pandas()\n",
        "\n",
        "                    # Apply row limit if needed\n",
        "                    if self.config.max_rows:\n",
        "                        rows_remaining = self.config.max_rows - total_rows\n",
        "                        if rows_remaining <= 0:\n",
        "                            break\n",
        "                        if len(df) > rows_remaining:\n",
        "                            df = df.iloc[:rows_remaining]\n",
        "\n",
        "                    df = self._process_columns(df)\n",
        "                    batch_rows.append(df)\n",
        "                    current_size += len(df)\n",
        "\n",
        "                    if current_size >= config['batch_size']:\n",
        "                        try:\n",
        "                            # Filter out empty DataFrames before concatenation\n",
        "                            combined_df = pd.concat([df for df in batch_rows if not df.empty], ignore_index=True)\n",
        "                            pantab.frame_to_hyper(\n",
        "                                combined_df,\n",
        "                                output_path,\n",
        "                                table=self.config.table_id,\n",
        "                                exists_ok=True  # Append rows if table exists\n",
        "                            )\n",
        "                            total_rows += current_size\n",
        "                            batch_rows = []\n",
        "                            current_size = 0\n",
        "                            self._update_progress_streaming(total_rows)\n",
        "\n",
        "                            combined_df = None\n",
        "                            gc.collect()\n",
        "\n",
        "                        except Exception as e:\n",
        "                            logger.error(f\"Error writing batch: {str(e)}\")\n",
        "                            raise\n",
        "\n",
        "            # Process remaining rows\n",
        "            if batch_rows:\n",
        "                try:\n",
        "                    combined_df = pd.concat([df for df in batch_rows if not df.empty], ignore_index=True)\n",
        "                    pantab.frame_to_hyper(\n",
        "                        combined_df,\n",
        "                        output_path,\n",
        "                        table=self.config.table_id,\n",
        "                        exists_ok=True\n",
        "                    )\n",
        "                    total_rows += current_size\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error writing final batch: {str(e)}\")\n",
        "                    raise\n",
        "\n",
        "            logger.info(f\"Successfully processed {total_rows:,} rows\")\n",
        "            return total_rows\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in hyper streaming: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "\n",
        "    def _stream_to_csv(self, output_path: str) -> int:\n",
        "        \"\"\"Stream to CSV format with memory optimization.\"\"\"\n",
        "        config = self._get_optimal_stream_config()\n",
        "        total_rows = 0\n",
        "        first_batch = True\n",
        "\n",
        "        try:\n",
        "            session = self._create_read_session(config['stream_count'])\n",
        "\n",
        "            for stream in session.streams:\n",
        "                reader = self.bq_storage_client.read_rows(stream.name)\n",
        "\n",
        "                for batch in reader.rows().pages:\n",
        "                    df = batch.to_pandas()\n",
        "                    df = self._process_columns(df)\n",
        "\n",
        "                    df.to_csv(\n",
        "                        output_path,\n",
        "                        mode='w' if first_batch else 'a',\n",
        "                        header=first_batch,\n",
        "                        index=False\n",
        "                    )\n",
        "\n",
        "                    first_batch = False\n",
        "                    total_rows += len(df)\n",
        "                    self._update_progress_streaming(total_rows)\n",
        "\n",
        "                    # Force garbage collection\n",
        "                    df = None\n",
        "                    gc.collect()\n",
        "\n",
        "            return total_rows\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in CSV streaming: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _adjust_chunk_size(self, error_type: str, attempt: int) -> int:\n",
        "        \"\"\"Adjust chunk size dynamically based on error type and retry attempt.\"\"\"\n",
        "        reduction_factor = {\n",
        "            'memory': 2,  # Divide chunk size by 2 for memory issues\n",
        "            'timeout': 1.5,  # Divide by 1.5 for timeouts\n",
        "            'quota': 3  # Divide by 3 for quota errors\n",
        "        }.get(error_type.lower(), 1.5)  # Default reduction factor\n",
        "\n",
        "        # Exponential backoff: smaller chunks for repeated retries\n",
        "        adjusted_chunk_size = max(\n",
        "            self.chunk_size // (reduction_factor * (attempt + 1)),\n",
        "            50_000  # Minimum chunk size to avoid inefficiency\n",
        "        )\n",
        "\n",
        "        logger.info(\n",
        "            f\"Adjusting chunk size to {adjusted_chunk_size:,} rows (attempt {attempt + 1}) \"\n",
        "            f\"due to {error_type} error.\"\n",
        "        )\n",
        "        return adjusted_chunk_size\n",
        "\n",
        "    def _create_read_session(self, stream_count: int):\n",
        "        \"\"\"Create optimized read session.\"\"\"\n",
        "        try:\n",
        "            read_session = bigquery_storage.types.ReadSession()\n",
        "            table_path = f\"projects/{self.config.source_project}/datasets/{self.config.dataset_id}/tables/{self.config.table_id}\"\n",
        "            read_session.table = table_path\n",
        "            read_session.data_format = bigquery_storage.types.DataFormat.ARROW\n",
        "\n",
        "            # Only apply where clause if specified\n",
        "            if self.config.where_clause:\n",
        "                read_session.read_options.row_restriction = self.config.where_clause\n",
        "\n",
        "            # Create session with snapshot\n",
        "            session = self.bq_storage_client.create_read_session(\n",
        "                parent=f\"projects/{self.config.project_id}\",\n",
        "                read_session=read_session,\n",
        "                max_stream_count=stream_count #max_stream_count=stream_count\n",
        "            )\n",
        "\n",
        "            logger.info(f\"Created read session with {len(session.streams)} streams\")\n",
        "            return session\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating read session: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _update_progress_streaming(self, total_rows: int):\n",
        "        \"\"\"Update progress display for streaming mode.\"\"\"\n",
        "        try:\n",
        "            # Use clear_output with wait=True to prevent flickering\n",
        "            clear_output(wait=True)\n",
        "\n",
        "            elapsed_time = time.time() - self.start_time\n",
        "            total_records = self._count_records()\n",
        "            progress = (total_rows / total_records) * 100 if total_records > 0 else 0\n",
        "            rows_per_second = total_rows / elapsed_time if elapsed_time > 0 else 0\n",
        "            memory_usage_mb = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024\n",
        "\n",
        "            # Create simple progress display\n",
        "            progress_html = f\"\"\"\n",
        "            <div class=\"bq-progress\">\n",
        "                <div style=\"font-weight: bold;\">Processing: {self.config.source_project}.{self.config.dataset_id}.{self.config.table_id}</div>\n",
        "                <div style=\"margin: 10px 0;\">\n",
        "                    <div style=\"width: 100%; background-color: #eee; border-radius: 3px;\">\n",
        "                        <div class=\"progress-bar\" style=\"width: {min(100, progress)}%;\">\n",
        "                            {progress:.1f}%\n",
        "                        </div>\n",
        "                    </div>\n",
        "                </div>\n",
        "                <div style=\"margin: 10px 0;\">\n",
        "                    <div>Processed Rows: {total_rows:,} of {total_records:,}</div>\n",
        "                    <div>Processing Rate: {rows_per_second:.1f} rows/sec</div>\n",
        "                    <div>Memory Usage: {memory_usage_mb:.1f} MB</div>\n",
        "                    <div>Elapsed Time: {elapsed_time:.1f}s</div>\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "            # Display progress first\n",
        "            display(HTML(progress_html))\n",
        "\n",
        "            # Force display to flush\n",
        "            import sys\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error updating progress: {str(e)}\")\n",
        "\n",
        "def extract_bigquery_data(config: BigQueryConfig) -> Tuple[int, Optional[str]]:\n",
        "    \"\"\"Main function to extract data from BigQuery.\"\"\"\n",
        "    extractor = BigQueryExtractor(config)\n",
        "    return extractor.extract_data()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Calculate optimal number of workers based on CPU cores\n",
        "        cpu_count = os.cpu_count()\n",
        "        recommended_workers = max(1, min(cpu_count * 2, 8))  # 2 workers per CPU, max 8\n",
        "\n",
        "        # Calculate reasonable max bytes billed (10TB default limit)\n",
        "        MAX_BYTES_BILLED = 10 * 1024 * 1024 * 1024 * 1024  # 10TB\n",
        "\n",
        "        # Configure extraction with reasonable defaults\n",
        "        config = BigQueryConfig(\n",
        "            project_id='pre-sales-demo',\n",
        "            source_project='bigquery-public-data',\n",
        "            dataset_id='samples',\n",
        "            table_id='github_timeline',\n",
        "            output_format='hyper',\n",
        "            output_path='/content/data',\n",
        "            hyper_batch_size=100000,  # Added for Hyper file creation\n",
        "            max_memory_gb=0.8,  # Maximum memory usage (80% of available)\n",
        "            chunk_size=200_000,\n",
        "            initial_chunk_size=200_000,  # More conservative initial chunk size\n",
        "            max_workers=recommended_workers,  # Dynamic based on CPU cores\n",
        "            max_bytes_billed=MAX_BYTES_BILLED,  # 10TB limit\n",
        "            clean_up_temp_files=True,\n",
        "            where_clause=None,  # Optional filtering\n",
        "            columns=None,  # All columns by default\n",
        "            max_rows=None\n",
        "        )\n",
        "\n",
        "        # Log configuration details\n",
        "        logger.info(\"Starting extraction with configuration:\")\n",
        "        logger.info(f\"- Project ID: {config.project_id}\")\n",
        "        logger.info(f\"- Source: {config.source_project}.{config.dataset_id}.{config.table_id}\")\n",
        "        logger.info(f\"- Workers: {config.max_workers} (based on {cpu_count} CPU cores)\")\n",
        "        logger.info(f\"- Initial chunk size: {config.initial_chunk_size:,} rows\")\n",
        "        logger.info(f\"- Max bytes billed: {config.max_bytes_billed / (1024**4):.1f}TB\")\n",
        "        logger.info(f\"- Output format: {config.output_format}\")\n",
        "        logger.info(f\"- Output path: {config.output_path}\")\n",
        "\n",
        "        # Validate output directory exists\n",
        "        Path(config.output_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Check available memory\n",
        "        memory_info = psutil.virtual_memory()\n",
        "        available_memory_gb = memory_info.available / (1024**3)\n",
        "        logger.info(f\"Available memory: {available_memory_gb:.1f}GB\")\n",
        "\n",
        "        if available_memory_gb < 2:  # Less than 2GB available\n",
        "            logger.warning(\"Low memory detected - reducing chunk size\")\n",
        "            config.initial_chunk_size = 100_000  # Reduce chunk size for low memory\n",
        "\n",
        "        # Extract data\n",
        "        total_rows, final_file = extract_bigquery_data(config)\n",
        "\n",
        "        # Log final results\n",
        "        if final_file:\n",
        "            logger.info(\"Extraction completed successfully\")\n",
        "            logger.info(f\"Total rows processed: {total_rows:,}\")\n",
        "            logger.info(f\"Final output file: {final_file}\")\n",
        "            logger.info(f\"Output location: {os.path.join(config.output_path, final_file)}\")\n",
        "        else:\n",
        "            logger.error(\"Extraction failed - no output file generated\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Extraction failed with error: {str(e)}\", exc_info=True)\n",
        "        raise\n",
        "    finally:\n",
        "        logger.info(\"Process completed\")\n",
        "        # Optional: Display memory usage at end\n",
        "        final_memory = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
        "        logger.info(f\"Final memory usage: {final_memory:.1f}MB\")"
      ]
    }
  ]
}